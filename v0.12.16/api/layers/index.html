<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Layers · ReservoirComputing.jl</title><meta name="title" content="Layers · ReservoirComputing.jl"/><meta property="og:title" content="Layers · ReservoirComputing.jl"/><meta property="twitter:title" content="Layers · ReservoirComputing.jl"/><meta name="description" content="Documentation for ReservoirComputing.jl."/><meta property="og:description" content="Documentation for ReservoirComputing.jl."/><meta property="twitter:description" content="Documentation for ReservoirComputing.jl."/><meta property="og:url" content="https://docs.sciml.ai/ReservoirComputing/stable/api/layers/"/><meta property="twitter:url" content="https://docs.sciml.ai/ReservoirComputing/stable/api/layers/"/><link rel="canonical" href="https://docs.sciml.ai/ReservoirComputing/stable/api/layers/"/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/favicon.ico" rel="icon" type="image/x-icon"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.png" alt="ReservoirComputing.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">ReservoirComputing.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">ReservoirComputing.jl</a></li><li><a class="tocitem" href="../../tutorials/getting_started/">Getting Started with ReservoirComputing.jl</a></li><li><span class="tocitem">Tutorials</span><ul><li><a class="tocitem" href="../../tutorials/scratch/">Building a model from scratch</a></li><li><a class="tocitem" href="../../tutorials/lorenz_basic/">Chaos forecasting with an ESN</a></li><li><a class="tocitem" href="../../tutorials/ngrc/">Fitting a Next Generation Reservoir Computer</a></li><li><a class="tocitem" href="../../tutorials/deep_esn/">Deep Echo State Networks</a></li><li><a class="tocitem" href="../../tutorials/train/">Training Reservoir Computing Models</a></li><li><a class="tocitem" href="../../tutorials/reca/">Reservoir Computing with Cellular Automata</a></li><li><a class="tocitem" href="../../tutorials/saveload/">Saving and loading models</a></li></ul></li><li><span class="tocitem">Examples</span><ul><li><a class="tocitem" href="../../examples/model_es2n/">Building a model to add to ReservoirComputing.jl</a></li></ul></li><li><span class="tocitem">API Documentation</span><ul><li class="is-active"><a class="tocitem" href>Layers</a><ul class="internal"><li><a class="tocitem" href="#Base-Layers"><span>Base Layers</span></a></li><li><a class="tocitem" href="#Readout-Layers"><span>Readout Layers</span></a></li><li><a class="tocitem" href="#Echo-State-Networks"><span>Echo State Networks</span></a></li><li><a class="tocitem" href="#Reservoir-computing-with-cellular-automata"><span>Reservoir computing with cellular automata</span></a></li></ul></li><li><a class="tocitem" href="../models/">Models</a></li><li><a class="tocitem" href="../utils/">Utilities</a></li><li><a class="tocitem" href="../train/">Train</a></li><li><a class="tocitem" href="../predict/">Predict</a></li><li><a class="tocitem" href="../states/">States</a></li><li><a class="tocitem" href="../inits/">Initializers</a></li></ul></li><li><a class="tocitem" href="../../references/">References</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">API Documentation</a></li><li class="is-active"><a href>Layers</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Layers</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/SciML/ReservoirComputing.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/SciML/ReservoirComputing.jl/blob/master/docs/src/api/layers.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Layers"><a class="docs-heading-anchor" href="#Layers">Layers</a><a id="Layers-1"></a><a class="docs-heading-anchor-permalink" href="#Layers" title="Permalink"></a></h1><h2 id="Base-Layers"><a class="docs-heading-anchor" href="#Base-Layers">Base Layers</a><a id="Base-Layers-1"></a><a class="docs-heading-anchor-permalink" href="#Base-Layers" title="Permalink"></a></h2><article><details class="docstring" open="true"><summary id="ReservoirComputing.ReservoirComputer"><a class="docstring-binding" href="#ReservoirComputing.ReservoirComputer"><code>ReservoirComputing.ReservoirComputer</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">ReservoirComputer(reservoir, states_modifiers, readout)
ReservoirComputer(reservoir, readout)</code></pre><p>Generic reservoir computing container that wires together:</p><ol><li>a <code>reservoir</code> (any Lux-compatible layer producing features),</li><li>(Optional) zero or more <code>states_modifiers</code> applied sequentially to the reservoir features,</li><li>a <code>readout</code> layer (typically <a href="#ReservoirComputing.LinearReadout"><code>LinearReadout</code></a>).</li></ol><p>The container exposes a standard <code>(x, ps, st) -&gt; (y, st′)</code> interface and utility functions to initialize parameters/states, stream sequences to collect features, and install trained readout weights.</p><p><strong>Arguments</strong></p><ul><li><code>reservoir</code>: a layer that consumes inputs and produces feature vectors.</li><li><code>states_modifiers</code>: a tuple (or vector converted to <code>Tuple</code>) of layers applied after the reservoir (optional. May be empty).</li><li><code>readout</code>: the final trainable layer mapping features to outputs.</li></ul><p><strong>Inputs</strong></p><ul><li><code>x</code>: input to the reservoir (shape determined by the reservoir).</li><li><code>ps</code>: reservoir computing parameters.</li><li><code>st</code>: reservoir computing states.</li></ul><p><strong>Returns</strong></p><ul><li><code>(y, st′)</code> where <code>y</code> is the readout output and <code>st′</code> contains the updated states of the reservoir, modifiers, and readout.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/SciML/ReservoirComputing.jl/blob/f5ea20f44ef287a45c84eece8dd0af54556e4ffe/src/reservoircomputer.jl#L1-L31">source</a></section></details></article><article><details class="docstring" open="true"><summary id="ReservoirComputing.ReservoirChain"><a class="docstring-binding" href="#ReservoirComputing.ReservoirChain"><code>ReservoirComputing.ReservoirChain</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">ReservoirChain(layers...; name=nothing)
ReservoirChain(xs::AbstractVector; name=nothing)
ReservoirChain(nt::NamedTuple; name=nothing)
ReservoirChain(; name=nothing, kwargs...)</code></pre><p>A lightweight, Lux-compatible container that composes a sequence of layers and executes them in order. The implementation of <code>ReservoirChain</code> is equivalent to Lux&#39;s own <code>Chain</code>.</p><p><strong>Construction</strong></p><p>You can build a chain from:</p><ul><li><strong>Positional layers:</strong> <code>ReservoirChain(l1, l2, ...)</code></li><li><strong>A vector of layers:</strong> <code>ReservoirChain([l1, l2, ...])</code></li><li><strong>A named tuple of layers:</strong> <code>ReservoirChain((; layer_a=l1, layer_b=l2))</code></li><li><strong>Keywords (sugar for a named tuple):</strong> <code>ReservoirChain(; layer_a=l1, layer_b=l2)</code></li></ul><p>In all cases, function objects are automatically wrapped via <code>WrappedFunction</code> so they can participate like regular layers. If a <a href="#ReservoirComputing.LinearReadout"><code>LinearReadout</code></a> with <code>include_collect=true</code> is present, the chain automatically inserts a <a href="#ReservoirComputing.Collect"><code>Collect</code></a> layer immediately before that readout.</p><p>Use <code>name</code> to optionally tag the chain instance.</p><p><strong>Inputs</strong></p><p><code>(x, ps, st)</code> where:</p><ul><li><code>x</code>: input to the first layer.</li><li><code>ps</code>: parameters as a named tuple with the same fields and order as the chain&#39;s layers.</li><li><code>st</code>: states as a named tuple with the same fields and order as the chain&#39;s layers.</li></ul><p>The call <code>(c::ReservoirChain)(x, ps, st)</code> forwards <code>x</code> through each layer: <code>(x, ps_i, st_i) -&gt; (x_next, st_i′)</code> and returns the final output and the updated states for every layer.</p><p><strong>Returns</strong></p><ul><li><code>(y, st′)</code> where <code>y</code> is the output of the last layer and <code>st′</code> is a named tuple collecting the updated states for each layer.</li></ul><p><strong>Parameters</strong></p><ul><li>A <code>NamedTuple</code> whose fields correspond 1:1 with the layers. Each field holds the parameters for that layer.</li><li>Field names are generated as <code>:layer_1, :layer_2, ...</code> when constructed positionally, or preserved when you pass a <code>NamedTuple</code>/keyword constructor.</li></ul><p><strong>States</strong></p><ul><li>A <code>NamedTuple</code> whose fields correspond 1:1 with the layers. Each field holds the state for that layer.</li></ul><p><strong>Layer access &amp; indexing</strong></p><ul><li><code>c[i]</code>: get the <em>i</em>-th layer (1-based).</li><li><code>c[indices]</code>: return a new <code>ReservoirChain</code> formed by selecting a subset of layers.</li><li><code>getproperty(c, :layer_k)</code>: access layer <code>k</code> by its generated/explicit name.</li><li><code>length(c)</code>, <code>firstindex(c)</code>, <code>lastindex(c)</code>: standard collection interfaces.</li></ul><p><strong>Notes</strong></p><ul><li><strong>Function wrapping:</strong> Any plain <code>Function</code> in the constructor is wrapped as <code>WrappedFunction(f)</code>. Non-layer, non-function objects will error.</li><li><strong>Auto-collect for readouts:</strong> When a <a href="#ReservoirComputing.LinearReadout"><code>LinearReadout</code></a> has <code>include_collect=true</code>, the constructor expands it to <code>(Collect(), readout)</code> so that downstream tooling can capture features consistently.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/SciML/ReservoirComputing.jl/blob/f5ea20f44ef287a45c84eece8dd0af54556e4ffe/src/layers/lux_layers.jl#L54-L124">source</a></section></details></article><article><details class="docstring" open="true"><summary id="ReservoirComputing.Collect"><a class="docstring-binding" href="#ReservoirComputing.Collect"><code>ReservoirComputing.Collect</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">Collect()</code></pre><p>Marker layer that passes data through unchanged but marks a feature checkpoint for <a href="../utils/#ReservoirComputing.collectstates"><code>collectstates</code></a>. At each time step, whenever a <code>Collect</code> is encountered in the chain, the current vector is recorded as part of the feature vector used to train the readout. If multiple <code>Collect</code> layers exist, their vectors are concatenated with <code>vcat</code> in order of appearance.</p><p><strong>Arguments</strong></p><ul><li>None.</li></ul><p><strong>Keyword arguments</strong></p><ul><li>None.</li></ul><p><strong>Inputs</strong></p><ul><li><code>x :: AbstractArray (d, batch)</code> — the current tensor flowing through the chain.</li></ul><p><strong>Returns</strong></p><ul><li><code>(x, st)</code> — the same tensor <code>x</code> and the <strong>unchanged</strong> state <code>st</code>.</li></ul><p><strong>Parameters</strong></p><ul><li>None.</li></ul><p><strong>States</strong></p><ul><li>None.</li></ul><p><strong>Notes</strong></p><ul><li>When used with a single <code>Collect</code> before a <a href="#ReservoirComputing.LinearReadout"><code>LinearReadout</code></a>, training uses exactly the tensor right before the readout (e.g., the reservoir state).</li><li>With <strong>multiple</strong> <code>Collect</code> layers (e.g., after different submodules), the per-step features are <code>vcat</code>-ed in chain order to form one feature vector.</li><li>If the readout is constructed with <code>include_collect=true</code>, an <em>implicit</em> collection point is assumed immediately before the readout. Use an explicit <code>Collect</code> only when you want to control where/what is collected (or to stack multiple features).</li></ul><pre><code class="language-julia hljs">rc = ReservoirChain(
    StatefulLayer(ESNCell(3 =&gt; 300)),
    NLAT2(),
    Collect(), # &lt;-- collect the 300-dim reservoir after NLAT2
    LinearReadout(300 =&gt; 3; include_collect=false) # &lt;-- toggle off the default Collect()
)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/SciML/ReservoirComputing.jl/blob/f5ea20f44ef287a45c84eece8dd0af54556e4ffe/src/layers/basic.jl#L113-L165">source</a></section></details></article><article><details class="docstring" open="true"><summary id="ReservoirComputing.StatefulLayer"><a class="docstring-binding" href="#ReservoirComputing.StatefulLayer"><code>ReservoirComputing.StatefulLayer</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">StatefulLayer(cell::AbstractReservoirRecurrentCell)</code></pre><p>A lightweight wrapper that makes a recurrent cell carry its input state to the next step.</p><p><strong>Arguments</strong></p><ul><li><code>cell</code>: <code>AbstractReservoirRecurrentCell</code> (e.g. <a href="#ReservoirComputing.ESNCell"><code>ESNCell</code></a>).</li></ul><p><strong>States</strong></p><ul><li><code>cell</code>: internal states for the wrapped <code>cell</code> (e.g., RNG replicas, etc.).</li><li><code>carry</code>: the per-sequence hidden state; initialized to <code>nothing</code>.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/SciML/ReservoirComputing.jl/blob/f5ea20f44ef287a45c84eece8dd0af54556e4ffe/src/layers/lux_layers.jl#L11-L26">source</a></section></details></article><article><details class="docstring" open="true"><summary id="ReservoirComputing.DelayLayer"><a class="docstring-binding" href="#ReservoirComputing.DelayLayer"><code>ReservoirComputing.DelayLayer</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">DelayLayer(input_dim; num_delays=2, stride=1)</code></pre><p>Stateful delay layer that augments the current vector with a fixed number of time-delayed copies of itself. Intended to be used as a <code>state_modifier</code> in a <code>ReservoirComputer</code>, for example to build NVAR-style feature vectors.</p><p>At each call, the layer:</p><ol><li>Takes the current input vector <code>h(t)</code> of length <code>input_dim</code>.</li><li>Produces an output vector that concatenates:</li></ol><ul><li>the current input <code>h(t)</code>, and</li><li><code>num_delays</code> previous inputs stored in an internal buffer.</li></ul><ol><li>Updates its internal delay buffer with <code>h(t)</code> every <code>stride</code> calls.</li></ol><p>Newly initialized buffers are filled with zeros, so at the beginning of a sequence, delayed entries correspond to zero padding.</p><p><strong>Arguments</strong></p><ul><li><code>input_dim</code>: Dimension of the input/state vector at each time step.</li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>num_delays</code>: Number of delayed copies to keep. The output will have <code>(num_delays + 1) * input_dim</code> entries: the current vector plus <code>num_delays</code> past vectors. Default is 2.</li><li><code>stride</code>: Delay stride in layer calls. The internal buffer is updated only when <code>clock % stride == 0</code>. Default is 1.</li><li><code>init_delay</code>: Initializer(s) for the delays. Must be either a single function (e.g. <code>zeros32</code>, <code>randn32</code>) or an <code>NTuple</code> of <code>num_delays</code> functions, e.g. <code>(zeros32, randn32)</code>. If a single function <code>fn</code> is provided, it is automatically expanded into a <code>num_delays</code>-element tuple <code>(fn, fn, ..., fn)</code>. Default is <code>zeros32</code>.</li></ul><p><strong>Inputs</strong></p><ul><li><code>h(t) :: AbstractVector (input_dim,)</code></li></ul><p>Typically the current reservoir state at time <code>t</code>.</p><p><strong>Returns</strong></p><ul><li><code>z :: AbstractVector ((num_delays + 1) * input_dim,)</code></li></ul><p>Concatenation of the current input and its delayed copies.</p><p><strong>Parameters</strong></p><p>None</p><p><strong>States</strong></p><ul><li><code>history</code>: a matrix whose column j holds the j-th most recent stored input. On a stride update, the columns are shifted and the current input is placed in column 1.</li><li><code>clock</code>: A counter that updates each call of the layer. The delay buffer is updated when <code>clock % stride == 0</code>.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/SciML/ReservoirComputing.jl/blob/f5ea20f44ef287a45c84eece8dd0af54556e4ffe/src/layers/basic.jl#L234-L291">source</a></section></details></article><article><details class="docstring" open="true"><summary id="ReservoirComputing.NonlinearFeaturesLayer"><a class="docstring-binding" href="#ReservoirComputing.NonlinearFeaturesLayer"><code>ReservoirComputing.NonlinearFeaturesLayer</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">NonlinearFeaturesLayer(features...; include_input=true)</code></pre><p>Layer that builds a feature vector by applying one or more user-defined functions to a single input vector and concatenating the results. Intended to be used as a <code>state_modifier</code> (for example, after a <code>DelayLayer</code>) to construct NGRC/NVAR-style feature maps.</p><p>At each call, for an input vector <code>x</code>, the layer:</p><ol><li>Optionally includes <code>x</code> itself (if <code>include_input=true</code>).</li><li>Applies each function in <code>features</code> to <code>x</code>.</li><li>Returns the vertical concatenation of all results.</li></ol><p><strong>Arguments</strong></p><ul><li><code>features...</code>: One or more functions <code>f(x)</code> that map a vector to a vector. Each function is called as <code>f(inp)</code> and must return an <code>AbstractVector</code>.</li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>include_input</code>: If <code>true</code> (default), the original input vector <code>inp</code> is included as the first block in the feature vector. If <code>false</code>, the output contains only the concatenation of <code>features(inp)</code>.</li></ul><p><strong>Inputs</strong></p><ul><li><code>inp :: AbstractVector</code> The current feature vector, typically the output of a <code>DelayLayer</code> or a reservoir state.</li></ul><p><strong>Returns</strong></p><ul><li><code>out :: AbstractVector</code> Concatenation of:<ul><li>the original input <code>inp</code> (if <code>include_input=true</code>), and</li><li>the outputs of each function in <code>features</code> applied to <code>inp</code>.</li></ul></li><li>The unchanged state <code>st</code> (this layer is stateless).</li></ul><p><strong>Parameters</strong></p><ul><li>None. <code>NonlinearFeaturesLayer</code> has no trainable parameters.</li></ul><p><strong>States</strong></p><ul><li>None. <code>initialstates</code> returns an empty <code>NamedTuple</code>.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/SciML/ReservoirComputing.jl/blob/f5ea20f44ef287a45c84eece8dd0af54556e4ffe/src/layers/basic.jl#L372-L418">source</a></section></details></article><h2 id="Readout-Layers"><a class="docs-heading-anchor" href="#Readout-Layers">Readout Layers</a><a id="Readout-Layers-1"></a><a class="docs-heading-anchor-permalink" href="#Readout-Layers" title="Permalink"></a></h2><article><details class="docstring" open="true"><summary id="ReservoirComputing.LinearReadout"><a class="docstring-binding" href="#ReservoirComputing.LinearReadout"><code>ReservoirComputing.LinearReadout</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">LinearReadout(in_dims =&gt; out_dims, [activation];
        use_bias=false, include_collect=true)</code></pre><p>Linear readout layer with optional bias and elementwise activation. Intended as the final, trainable mapping from collected features (e.g., reservoir state) to outputs. When <code>include_collect=true</code>, training will collect features immediately before this layer (logically inserting a <a href="#ReservoirComputing.Collect"><code>Collect</code></a> right before it).</p><p><strong>Equations</strong></p><p class="math-container">\[\mathbf{y} = \psi\!\left(\mathbf{W}\,\mathbf{x} + \mathbf{b}\right)\]</p><p><strong>Arguments</strong></p><ul><li><code>in_dims</code>: Input/feature dimension (e.g., reservoir size).</li><li><code>out_dims</code>: Output dimension (e.g., number of targets).</li><li><code>activation</code>: Elementwise output nonlinearity. Default: <code>identity</code>.</li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>use_bias</code>: Include an additive bias vector <code>b</code>. Default: <code>false</code>.</li><li><code>include_collect</code>: If <code>true</code> (default), training collects features immediately before this layer (as if a <a href="#ReservoirComputing.Collect"><code>Collect</code></a> were inserted right before it).</li></ul><p><strong>Parameters</strong></p><ul><li><code>weight :: (out_dims × in_dims)</code></li><li><code>bias   :: (out_dims,)</code> — present only if <code>use_bias=true</code></li></ul><p><strong>States</strong></p><ul><li>None.</li></ul><p><strong>Notes</strong></p><ul><li>In ESN workflows, readout weights are typically replaced via ridge regression in <a href="../train/#ReservoirComputing.train!"><code>train!</code></a>. Therefore, how <code>LinearReadout</code> gets initialized is of no consequence. Additionally, the dimensions will also not be taken into account, as <a href="../train/#ReservoirComputing.train!"><code>train!</code></a> will replace the weights.</li><li>If you set <code>include_collect=false</code>, make sure a <a href="#ReservoirComputing.Collect"><code>Collect</code></a> appears earlier in the chain. Otherwise training may operate on the post-readout signal, which is usually unintended.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/SciML/ReservoirComputing.jl/blob/f5ea20f44ef287a45c84eece8dd0af54556e4ffe/src/layers/basic.jl#L7-L52">source</a></section></details></article><article><details class="docstring" open="true"><summary id="ReservoirComputing.SVMReadout"><a class="docstring-binding" href="#ReservoirComputing.SVMReadout"><code>ReservoirComputing.SVMReadout</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">SVMReadout(in_dims =&gt; out_dims;
    include_collect=true, kwargs...)</code></pre><p>Readout layer based on support vector machines. Requires LIBSVM.jl.</p><p><strong>Arguments</strong></p><ul><li><code>in_dims</code>: Input/feature dimension (e.g., reservoir size).</li><li><code>out_dims</code>: Output dimension (e.g., number of targets).</li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>include_collect</code>: If <code>true</code> (default), training collects features immediately before this layer (as if a <a href="#ReservoirComputing.Collect"><code>Collect()</code></a> were inserted right before it).</li><li><code>kwags</code>: specific keyword arguments for LIBSVM elements.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/SciML/ReservoirComputing.jl/blob/f5ea20f44ef287a45c84eece8dd0af54556e4ffe/src/layers/svmreadout.jl#L1-L17">source</a></section></details></article><h2 id="Echo-State-Networks"><a class="docs-heading-anchor" href="#Echo-State-Networks">Echo State Networks</a><a id="Echo-State-Networks-1"></a><a class="docs-heading-anchor-permalink" href="#Echo-State-Networks" title="Permalink"></a></h2><article><details class="docstring" open="true"><summary id="ReservoirComputing.EIESNCell"><a class="docstring-binding" href="#ReservoirComputing.EIESNCell"><code>ReservoirComputing.EIESNCell</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">EIESNCell(in_dims =&gt; out_dims, [activation]; kwargs...)</code></pre><p>Excitatory-Inhibitory Echo State Network (EIESN) cell (<a href="../../references/#Panahi2025">Panahi <em>et al.</em>, 2025</a>).</p><p class="math-container">\[\mathbf{x}(t) =
b_{\mathrm{ex}} \, \phi_{\mathrm{ex}}\!\left(
  \mathbf{W}_{\mathrm{in}} \mathbf{u}(t) + a_{\mathrm{ex}} \mathbf{A} \mathbf{x}(t-1) +
  \mathbf{\beta}_{\mathrm{ex}}\right)
- b_{\mathrm{inh}} \, \phi_{\mathrm{inh}}\!\left(
  \mathbf{W}_{\mathrm{in}} \mathbf{u}(t) + a_{\mathrm{inh}} \mathbf{A} \mathbf{x}(t-1) +
  \mathbf{\beta}_{\mathrm{inh}}\right)\]</p><p><strong>Symbols</strong></p><ul><li><p class="math-container">\[\mathbf{x}(t)\]</p>: Reservoir state at time <span>$t$</span>.</li><li><p class="math-container">\[\mathbf{u}(t)\]</p>: Input at time <span>$t$</span>.</li><li><p class="math-container">\[\mathbf{A}\]</p>: Reservoir (recurrent) matrix.</li><li><p class="math-container">\[\mathbf{W}_{\mathrm{in}}\]</p>: Input matrix.</li><li><p class="math-container">\[\mathbf{\beta}_{\mathrm{ex}}, \mathbf{\beta}_{\mathrm{inh}}\]</p>: Bias vectors (optional).</li><li><p class="math-container">\[a_{\mathrm{ex}}, a_{\mathrm{inh}}\]</p>: Excitatory and inhibitory recurrence scales.</li><li><p class="math-container">\[b_{\mathrm{ex}}, b_{\mathrm{inh}}\]</p>: Excitatory and inhibitory output scales.</li><li><p class="math-container">\[\phi_{\mathrm{ex}}, \phi_{\mathrm{inh}}\]</p>: Excitatory and inhibitory activation functions.</li></ul><p>The reservoir parameters are fixed after initialization; only the readout layer is intended to be trained, following the standard reservoir computing paradigm.</p><p><strong>Arguments</strong></p><ul><li><code>in_dims</code>: Input dimension.</li><li><code>out_dims</code>: Reservoir (hidden state) dimension.</li><li><code>activation</code>: Activation function. Can be a single function (applied to both terms) or a <code>Tuple</code> of two functions <span>$(\phi_{\mathrm{ex}}, \phi_{\mathrm{inh}})$</span>. Default: <code>tanh_fast</code>.</li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>use_bias</code>: Boolean to enable/disable bias vectors (<span>$\mathbf{\beta}$</span>). Default: <code>true</code>.</li><li><code>exc_recurrence_scale</code>: Excitatory recurrence scaling factor (<span>$a_{\mathrm{ex}}$</span>). Default: <code>0.9</code>.</li><li><code>inh_recurrence_scale</code>: Inhibitory recurrence scaling factor (<span>$a_{\mathrm{inh}}$</span>). Default: <code>0.5</code>.</li><li><code>exc_output_scale</code>: Excitatory output scaling factor (<span>$b_{\mathrm{ex}}$</span>). Default: <code>1.0</code>.</li><li><code>inh_output_scale</code>: Inhibitory output scaling factor (<span>$b_{\mathrm{inh}}$</span>). Default: <code>1.0</code>.</li><li><code>init_reservoir</code>: Initializer for the reservoir matrix <span>$\mathbf{A}$</span>. Default: <code>rand_sparse</code>.</li><li><code>init_input</code>: Initializer for the input matrix <span>$\mathbf{W}_{\mathrm{in}}$</span>. Default: <code>scaled_rand</code>.</li><li><code>init_bias</code>: Initializer for the bias vectors. Default: <code>zeros32</code>.</li><li><code>init_state</code>: Initializer for the initial hidden state <span>$\mathbf{x}(0)$</span>. Default: <code>randn32</code>.</li></ul><p><strong>Parameters</strong></p><p>Created by <code>initialparameters(rng, cell)</code>:</p><ul><li><code>input_matrix :: (out_dims × in_dims)</code> — <span>$\mathbf{W}_{\mathrm{in}}$</span>.</li><li><code>reservoir_matrix :: (out_dims × out_dims)</code> — <span>$\mathbf{A}$</span>.</li><li><code>bias_ex</code>, <code>bias_inh</code> — Bias vectors (present only if <code>use_bias=true</code>).</li></ul><p><strong>States</strong></p><p>Created by <code>initialstates(rng, cell)</code>:</p><ul><li><code>rng</code>: Replicated RNG used to initialize the hidden state when an external state is not provided.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/SciML/ReservoirComputing.jl/blob/f5ea20f44ef287a45c84eece8dd0af54556e4ffe/src/layers/eiesn_cell.jl#L1-L74">source</a></section></details></article><article><details class="docstring" open="true"><summary id="ReservoirComputing.AdditiveEIESNCell"><a class="docstring-binding" href="#ReservoirComputing.AdditiveEIESNCell"><code>ReservoirComputing.AdditiveEIESNCell</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">AdditiveEIESNCell(in_dims =&gt; out_dims, [activation]; kwargs...)</code></pre><p>Excitatory-Inhibitory Echo State Network (EIESN) cell with additive input (<a href="../../references/#Panahi2025">Panahi <em>et al.</em>, 2025</a>).</p><p class="math-container">\[\mathbf{x}(t) =
b_{\mathrm{ex}} \, \phi_{\mathrm{ex}}\!\left(
  a_{\mathrm{ex}} \mathbf{A} \mathbf{x}(t-1) + \mathbf{\beta}_{\mathrm{ex}}\right)
- b_{\mathrm{inh}} \, \phi_{\mathrm{inh}}\!\left(
  a_{\mathrm{inh}} \mathbf{A} \mathbf{x}(t-1) + \mathbf{\beta}_{\mathrm{inh}}\right)
+ g\!\left(\mathbf{W}_{\mathrm{in}} \mathbf{u}(t) + \mathbf{\beta}_{\mathrm{in}}\right)\]</p><p><strong>Symbols</strong></p><ul><li><p class="math-container">\[\mathbf{x}(t)\]</p>: Reservoir state at time <span>$t$</span>.</li><li><p class="math-container">\[\mathbf{u}(t)\]</p>: Input at time <span>$t$</span>.</li><li><p class="math-container">\[\mathbf{A}\]</p>: Reservoir (recurrent) matrix.</li><li><p class="math-container">\[\mathbf{W}_{\mathrm{in}}\]</p>: Input matrix.</li><li><p class="math-container">\[\mathbf{\beta}_{\mathrm{ex}}, \mathbf{\beta}_{\mathrm{inh}},
\mathbf{\beta}_{\mathrm{in}}\]</p>: Bias vectors (optional).</li><li><p class="math-container">\[a_{\mathrm{ex}}, a_{\mathrm{inh}}\]</p>: Excitatory and inhibitory recurrence scales.</li><li><p class="math-container">\[b_{\mathrm{ex}}, b_{\mathrm{inh}}\]</p>: Excitatory and inhibitory output scales.</li><li><p class="math-container">\[g\]</p>: Input activation function.</li><li><p class="math-container">\[\phi_{\mathrm{ex}}, \phi_{\mathrm{inh}}\]</p>: Excitatory and inhibitory activation functions.</li></ul><p>The reservoir parameters are fixed after initialization; only the readout layer is intended to be trained, following the standard reservoir computing paradigm.</p><p><strong>Arguments</strong></p><ul><li><code>in_dims</code>: Input dimension.</li><li><code>out_dims</code>: Reservoir (hidden state) dimension.</li><li><code>activation</code>: Activation function. Can be a single function (applied to both terms) or a <code>Tuple</code> of two functions <span>$(\phi_{\mathrm{ex}}, \phi_{\mathrm{inh}})$</span>. Default: <code>tanh_fast</code>.</li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>input_activation</code>: The non-linear function <span>$g$</span> applied to the input term. Default: <code>identity</code>.</li><li><code>use_bias</code>: Boolean to enable/disable bias vectors (<span>$\mathbf{\beta}$</span>). Default: <code>true</code>.</li><li><code>exc_recurrence_scale</code>: Excitatory recurrence scaling factor (<span>$a_{\mathrm{ex}}$</span>). Default: <code>0.9</code>.</li><li><code>inh_recurrence_scale</code>: Inhibitory recurrence scaling factor (<span>$a_{\mathrm{inh}}$</span>). Default: <code>0.5</code>.</li><li><code>exc_output_scale</code>: Excitatory output scaling factor (<span>$b_{\mathrm{ex}}$</span>). Default: <code>1.0</code>.</li><li><code>inh_output_scale</code>: Inhibitory output scaling factor (<span>$b_{\mathrm{inh}}$</span>). Default: <code>1.0</code>.</li><li><code>init_reservoir</code>: Initializer for the reservoir matrix <span>$\mathbf{A}$</span>. Default: <code>rand_sparse</code>.</li><li><code>init_input</code>: Initializer for the input matrix <span>$\mathbf{W}_{\mathrm{in}}$</span>. Default: <code>scaled_rand</code>.</li><li><code>init_bias</code>: Initializer for the bias vectors. Default: <code>zeros32</code>.</li><li><code>init_state</code>: Initializer for the initial hidden state <span>$\mathbf{x}(0)$</span>. Default: <code>randn32</code>.</li></ul><p><strong>Parameters</strong></p><p>Created by <code>initialparameters(rng, cell)</code>:</p><ul><li><code>input_matrix :: (out_dims × in_dims)</code> — <span>$\mathbf{W}_{\mathrm{in}}$</span>.</li><li><code>reservoir_matrix :: (out_dims × out_dims)</code> — <span>$\mathbf{A}$</span>.</li><li><code>bias_ex</code>, <code>bias_inh</code>, <code>bias_in</code> — Bias vectors (present only if <code>use_bias=true</code>).</li></ul><p><strong>States</strong></p><p>Created by <code>initialstates(rng, cell)</code>:</p><ul><li><code>rng</code>: Replicated RNG used to initialize the hidden state when an external state is not provided.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/SciML/ReservoirComputing.jl/blob/f5ea20f44ef287a45c84eece8dd0af54556e4ffe/src/layers/additive_eiesn_cell.jl#L1-L78">source</a></section></details></article><article><details class="docstring" open="true"><summary id="ReservoirComputing.ES2NCell"><a class="docstring-binding" href="#ReservoirComputing.ES2NCell"><code>ReservoirComputing.ES2NCell</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">ES2NCell(in_dims =&gt; out_dims, [activation];
    use_bias=False(), init_bias=zeros32,
    init_reservoir=rand_sparse, init_input=scaled_rand,
    init_state=randn32, init_orthogonal=orthogonal,
    proximity=1.0))</code></pre><p>Edge of Stability Echo State Network (ES2N) cell (<a href="../../references/#Ceni2025">Ceni and Gallicchio, 2025</a>).</p><p><strong>Equations</strong></p><p class="math-container">\[\begin{aligned}
    \mathbf{x}(t) &amp;= (1-\beta)\, \mathbf{O}\, \mathbf{x}(t-1) +
        \beta\, \phi\!\left(\mathbf{W}_{\text{in}} \mathbf{u}(t) +
        \mathbf{W}_r \mathbf{x}(t-1) + \mathbf{b} \right)
\end{aligned}\]</p><p><strong>Arguments</strong></p><pre><code class="language-julia hljs">- `in_dims`: Input dimension.
- `out_dims`: Reservoir (hidden state) dimension.
- `activation`: Activation function. Default: `tanh_fast`.</code></pre><p><strong>Keyword arguments</strong></p><ul><li><code>use_bias</code>: Whether to include a bias term. Default: <code>false</code>.</li><li><code>init_bias</code>: Initializer for the bias. Used only if <code>use_bias=true</code>. Default is <code>rand32</code>.</li><li><code>init_reservoir</code>: Initializer for the reservoir matrix <code>W_res</code>. Default is <a href="../inits/rand_sparse/#rand_sparse"><code>rand_sparse</code></a>.</li><li><code>init_orthogonal</code>: Initializer for the orthogonal matrix <code>O</code>. Default is <code>orthogonal</code>.</li><li><code>init_input</code>: Initializer for the input matrix <code>W_in</code>. Default is <a href="../inits/scaled_rand/#scaled_rand"><code>scaled_rand</code></a>.</li><li><code>init_state</code>: Initializer for the hidden state when an external state is not provided. Default is <code>randn32</code>.</li><li><code>proximity</code>: Proximity coefficient <code>α ∈ (0,1]</code>. Default: <code>1.0</code>.</li></ul><p><strong>Inputs</strong></p><ul><li><strong>Case 1:</strong> <code>x :: AbstractArray (in_dims, batch)</code> A fresh state is created via <code>init_state</code>; the call is forwarded to Case 2.</li><li><strong>Case 2:</strong> <code>(x, (h,))</code> where <code>h :: AbstractArray (out_dims, batch)</code> Computes the update and returns the new state.</li></ul><p>In both cases, the forward returns <code>((h_new, (h_new,)), st_out)</code> where <code>st_out</code> contains any updated internal state.</p><p><strong>Returns</strong></p><ul><li>Output/hidden state <code>h_new :: out_dims</code> and state tuple <code>(h_new,)</code>.</li><li>Updated layer state (NamedTuple).</li></ul><p><strong>Parameters</strong></p><ul><li><code>input_matrix :: (out_dims × in_dims)</code> — <code>W_in</code></li><li><code>reservoir_matrix :: (out_dims × out_dims)</code> — <code>W_res</code></li><li><code>orthogonal_matrix :: (res_dims × res_dims)</code> — <code>O</code></li><li><code>bias :: (out_dims,)</code> — present only if <code>use_bias=true</code></li></ul><p><strong>States</strong></p><p>Created by <code>initialstates(rng, esn)</code>:</p><ul><li><code>rng</code>: a replicated RNG used to sample initial hidden states when needed.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/SciML/ReservoirComputing.jl/blob/f5ea20f44ef287a45c84eece8dd0af54556e4ffe/src/layers/es2n_cell.jl#L3-L69">source</a></section></details></article><article><details class="docstring" open="true"><summary id="ReservoirComputing.ESNCell"><a class="docstring-binding" href="#ReservoirComputing.ESNCell"><code>ReservoirComputing.ESNCell</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">ESNCell(in_dims =&gt; out_dims, [activation];
    use_bias=false, init_bias=rand32,
    init_reservoir=rand_sparse, init_input=scaled_rand,
    init_state=randn32, leak_coefficient=1.0)</code></pre><p>Echo State Network (ESN) recurrent cell with optional leaky integration.</p><p><strong>Equations</strong></p><p class="math-container">\[\begin{aligned}
    \mathbf{x}(t) &amp;= (1-\alpha)\, \mathbf{x}(t-1)
        + \alpha\, \phi\!\left(\mathbf{W}_{\text{in}}\, \mathbf{u}(t)
        + \mathbf{W}_r\, \mathbf{x}(t-1) + \mathbf{b} \right)
\end{aligned}\]</p><p><strong>Arguments</strong></p><ul><li><code>in_dims</code>: Input dimension.</li><li><code>out_dims</code>: Reservoir (hidden state) dimension.</li><li><code>activation</code>: Activation function. Default: <code>tanh_fast</code>.</li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>use_bias</code>: Whether to include a bias term. Default: <code>false</code>.</li><li><code>init_bias</code>: Initializer for the bias. Used only if <code>use_bias=true</code>.   Default is <code>rand32</code>.</li><li><code>init_reservoir</code>: Initializer for the reservoir matrix <code>W_res</code>. Default is <a href="../inits/rand_sparse/#rand_sparse"><code>rand_sparse</code></a>.</li><li><code>init_input</code>: Initializer for the input matrix <code>W_in</code>. Default is <a href="../inits/scaled_rand/#scaled_rand"><code>scaled_rand</code></a>.</li><li><code>init_state</code>: Initializer for the hidden state when an external state is not provided. Default is <code>randn32</code>.</li><li><code>leak_coefficient</code>: Leak rate <code>α ∈ (0,1]</code>. Default: <code>1.0</code>.</li></ul><p><strong>Inputs</strong></p><ul><li><strong>Case 1:</strong> <code>x :: AbstractArray (in_dims, batch)</code> A fresh state is created via <code>init_state</code>; the call is forwarded to Case 2.</li><li><strong>Case 2:</strong> <code>(x, (h,))</code> where <code>h :: AbstractArray (out_dims, batch)</code> Computes the update and returns the new state.</li></ul><p>In both cases, the forward returns <code>((h_new, (h_new,)), st_out)</code> where <code>st_out</code> contains any updated internal state.</p><p><strong>Returns</strong></p><ul><li>Output/hidden state <code>h_new :: out_dims</code> and state tuple <code>(h_new,)</code>.</li><li>Updated layer state (NamedTuple).</li></ul><p><strong>Parameters</strong></p><p>Created by <code>initialparameters(rng, esn)</code>:</p><ul><li><code>input_matrix :: (out_dims × in_dims)</code> — <code>W_in</code></li><li><code>reservoir_matrix :: (out_dims × out_dims)</code> — <code>W_res</code></li><li><code>bias :: (out_dims,)</code> — present only if <code>use_bias=true</code></li></ul><p><strong>States</strong></p><p>Created by <code>initialstates(rng, esn)</code>:</p><ul><li><code>rng</code>: a replicated RNG used to sample initial hidden states when needed.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/SciML/ReservoirComputing.jl/blob/f5ea20f44ef287a45c84eece8dd0af54556e4ffe/src/layers/esn_cell.jl#L3-L67">source</a></section></details></article><article><details class="docstring" open="true"><summary id="ReservoirComputing.EuSNCell"><a class="docstring-binding" href="#ReservoirComputing.EuSNCell"><code>ReservoirComputing.EuSNCell</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">EuSNCell(in_dims =&gt; out_dims, [activation];
    use_bias=false, init_bias=rand32,
    init_reservoir=rand_sparse, init_input=scaled_rand,
    init_state=randn32, leak_coefficient=1.0, diffusion=1.0)</code></pre><p>Euler State Network (EuSN) cell (<a href="../../references/#Gallicchio2024">Gallicchio, 2024</a>).</p><p><strong>Equations</strong></p><p class="math-container">\[\begin{aligned}
    \mathbf{x}(t) &amp;= \mathbf{x}(t-1) + \varepsilon\, \phi\!\left(
        \mathbf{W}_{\text{in}}\, \mathbf{u}(t) +
        (\mathbf{W}_r - \gamma\, \mathbf{I})\, \mathbf{x}(t-1)
        + \mathbf{b} \right)
\end{aligned}\]</p><p><strong>Arguments</strong></p><ul><li><code>in_dims</code>: Input dimension.</li><li><code>out_dims</code>: Reservoir (hidden state) dimension.</li><li><code>activation</code>: Activation function. Default: <code>tanh_fast</code>.</li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>use_bias</code>: Whether to include a bias term. Default: <code>false</code>.</li><li><code>init_bias</code>: Initializer for the bias. Used only if <code>use_bias=true</code>.   Default is <code>rand32</code>.</li><li><code>init_reservoir</code>: Initializer for the reservoir matrix <code>W_res</code>. Default is <a href="../inits/rand_sparse/#rand_sparse"><code>rand_sparse</code></a>.</li><li><code>init_input</code>: Initializer for the input matrix <code>W_in</code>. Default is <a href="../inits/scaled_rand/#scaled_rand"><code>scaled_rand</code></a>.</li><li><code>init_state</code>: Initializer for the hidden state when an external state is not provided. Default is <code>randn32</code>.</li><li><code>leak_coefficient</code>: Leak rate <code>α ∈ (0,1]</code>. Default: <code>1.0</code>.</li><li><code>diffusion</code>: Diffusiona parameter <code>∈ (0,1]</code>. Default: <code>1.0</code>.</li></ul><p><strong>Inputs</strong></p><ul><li><strong>Case 1:</strong> <code>x :: AbstractArray (in_dims, batch)</code> A fresh state is created via <code>init_state</code>; the call is forwarded to Case 2.</li><li><strong>Case 2:</strong> <code>(x, (h,))</code> where <code>h :: AbstractArray (out_dims, batch)</code> Computes the update and returns the new state.</li></ul><p>In both cases, the forward returns <code>((h_new, (h_new,)), st_out)</code> where <code>st_out</code> contains any updated internal state.</p><p><strong>Returns</strong></p><ul><li>Output/hidden state <code>h_new :: out_dims</code> and state tuple <code>(h_new,)</code>.</li><li>Updated layer state (NamedTuple).</li></ul><p><strong>Parameters</strong></p><p>Created by <code>initialparameters(rng, esn)</code>:</p><ul><li><code>input_matrix :: (out_dims × in_dims)</code> — <code>W_in</code></li><li><code>reservoir_matrix :: (out_dims × out_dims)</code> — <code>W_res</code></li><li><code>bias :: (out_dims,)</code> — present only if <code>use_bias=true</code></li></ul><p><strong>States</strong></p><p>Created by <code>initialstates(rng, esn)</code>:</p><ul><li><code>rng</code>: a replicated RNG used to sample initial hidden states when needed.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/SciML/ReservoirComputing.jl/blob/f5ea20f44ef287a45c84eece8dd0af54556e4ffe/src/layers/eusn_cell.jl#L1-L67">source</a></section></details></article><h2 id="Reservoir-computing-with-cellular-automata"><a class="docs-heading-anchor" href="#Reservoir-computing-with-cellular-automata">Reservoir computing with cellular automata</a><a id="Reservoir-computing-with-cellular-automata-1"></a><a class="docs-heading-anchor-permalink" href="#Reservoir-computing-with-cellular-automata" title="Permalink"></a></h2><article><details class="docstring" open="true"><summary id="ReservoirComputing.RECACell"><a class="docstring-binding" href="#ReservoirComputing.RECACell"><code>ReservoirComputing.RECACell</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">RECACell(automaton, enc::RandomMaps)</code></pre><p>Cellular Automata (CA)–based reservoir recurrent cell. At each time step, the input vector is randomly embedded into a CA configuration, the CA is evolved for a fixed number of generations, and the flattened CA evolution is emitted as the reservoir state. The last CA configuration is carried to the next step. For more details please refer to (<a href="../../references/#Nichele2017">Nichele and Molund, 2017</a>), and (<a href="../../references/#Yilmaz2014">Yilmaz, 2014</a>).</p><p><strong>Arguments</strong></p><ul><li><p><code>automaton</code>: A cellular automaton rule/object from <code>CellularAutomata.jl</code> (e.g., <code>DCA(90)</code>, <code>DCA(30)</code>, …).</p></li><li><p><code>enc</code>: Precomputed random-mapping/encoding metadata given as a <a href="../models/#ReservoirComputing.RandomMapping"><code>RandomMapping</code></a>.</p></li></ul><p><strong>Inputs</strong></p><ul><li><p>Case A: a single input vector <code>x</code> with length <code>in_dims</code>. The cell internally uses the stored CA state (<code>st.ca</code>) as the previous configuration.</p></li><li><p>Case B: a tuple <code>(x, (ca,))</code> where <code>x</code> is as above and <code>ca</code> has length <code>enc.ca_size</code>.</p></li></ul><p><strong>Computation</strong></p><ol><li><p>Random embedding of <code>x</code> into a CA initial condition <code>c₀</code> using <code>enc.maps</code> across <code>enc.permutations</code> blocks of length <code>enc.expansion_size</code>.</p></li><li><p>CA evolution for <code>G = enc.generations</code> steps with the given <code>automaton</code>, producing an evolution matrix <code>E ∈ ℝ^{(G+1) × ca_size}</code> where <code>E[1,:] = c₀</code> and <code>E[t+1,:] = F(E[t,:])</code>.</p></li><li><p>Feature vector is the flattened stack of <code>E[2:end, :]</code> (dropping the initial row), shaped as a column vector of length <code>enc.states_size</code>.</p></li><li><p>Carry is the final CA configuration <code>E[end, :]</code>.</p></li></ol><p><strong>Returns</strong></p><ul><li>Output: <code>(h, (caₙ,))</code> where<ul><li><code>h</code> has length <code>enc.states_size</code> (the CA features),</li><li><code>caₙ</code> has length <code>enc.ca_size</code> (next carry).</li></ul></li><li>Updated (unchanged) cell state (parameters-free layer state).</li></ul><p><strong>Parameters &amp; State</strong></p><ul><li>Parameters: none</li><li>State: <code>(ca = zeros(Float32, enc.ca_size))</code></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/SciML/ReservoirComputing.jl/blob/f5ea20f44ef287a45c84eece8dd0af54556e4ffe/src/extensions/reca.jl#L76-L129">source</a></section></details></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../../examples/model_es2n/">« Building a model to add to ReservoirComputing.jl</a><a class="docs-footer-nextpage" href="../models/">Models »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.16.1 on <span class="colophon-date" title="Thursday 29 January 2026 12:49">Thursday 29 January 2026</span>. Using Julia version 1.12.4.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
