var documenterSearchIndex = {"docs":
[{"location":"api/inits/low_connectivity/#low_connectivity","page":"low_connectivity","title":"low_connectivity","text":"","category":"section"},{"location":"api/inits/low_connectivity/#References","page":"low_connectivity","title":"References","text":"Griffith, A.; Pomerance, A. and Gauthier, D. J. (2019). Forecasting chaotic systems with very low connectivity reservoir computers. Chaos: An Interdisciplinary Journal of Nonlinear Science 29.\n\n\n\n","category":"section"},{"location":"api/inits/low_connectivity/#ReservoirComputing.low_connectivity","page":"low_connectivity","title":"ReservoirComputing.low_connectivity","text":"low_connectivity([rng], [T], dims...;\n    connected=false, in_degree = 1, radius = 1.0,\n    cut_cycle = false, radius=nothing, return_sparse = false)\n\nConstruct an internal reservoir connectivity matrix with low connectivity.\n\nThis function creates a reservoir matrix with the specified in-degree for each node (Griffith et al., 2019). When in_degree is 1, the function can enforce a fully connected cycle if connected is true; otherwise, it generates a random connectivity pattern.\n\nArguments\n\nrng: Random number generator. Default is Utils.default_rng()from WeightInitializers.\nT: Type of the elements in the reservoir matrix. Default is Float32.\ndims: Dimensions of the reservoir matrix.\n\nKeyword Arguments\n\nconnected: For in_degree == 1, if true a connected cycle is enforced. Default is false.\nin_degree: The number of incoming connections per node. Must not exceed the number of nodes. Default is 1.\nradius: The desired spectral radius of the reservoir. Defaults to 1.0.\ncut_cycle: If true, removes one edge from the cycle to cut it. Default is false.\nradius: The desired spectral radius of the reservoir. If nothing is passed, no scaling takes place. Defaults to nothing.\nreturn_sparse: flag for returning a sparse matrix. true requires SparseArrays to be loaded. Default is false.\n\nExamples\n\njulia> low_connectivity(10, 10)\n10×10 Matrix{Float32}:\n 0.0        0.0       0.0       …  0.0      0.0   0.2207\n 0.0        0.0       0.0          0.0      0.0   0.564821\n 0.318999   0.0       0.0          0.0      0.0   0.0\n 0.670023   0.0       0.0          0.0      0.0   0.0\n 0.0        0.0       0.0          1.79705  0.0   0.0\n 0.0       -1.95711   0.0       …  0.0      0.0   0.0\n 0.0        0.0       0.0          0.0      0.0   0.0\n 0.0        0.0       0.0          0.0      0.0   0.0\n 0.0        0.0      -0.650657     0.0      0.0   0.0\n 0.0        0.0       0.0          0.0      0.0  -1.0\n\n\n\n\n\n","category":"function"},{"location":"tutorials/reca/#Reservoir-Computing-using-Cellular-Automata","page":"Reservoir Computing with Cellular Automata","title":"Reservoir Computing using Cellular Automata","text":"We showcase how to use reservoir computing models with cellular automata (ReCA) with ReservoirComputing.jl. While introduced in (Yilmaz, 2014) (Margem and Yilmaz, 2017), the implementation in this package follows (Nichele and Molund, 2017). To showcase ReCA models we show how to solve the 5 bit memory task.","category":"section"},{"location":"tutorials/reca/#5-bit-memory-task","page":"Reservoir Computing with Cellular Automata","title":"5 bit memory task","text":"We read the data can be read as follows:\n\nusing DelimitedFiles\n\ninput = readdlm(\"./5bitinput.txt\", ',', Float64)\noutput = readdlm(\"./5bitoutput.txt\", ',', Float64)\n\nTo use a ReCA model, it is necessary to define the rule one intends to use. To do so, ReservoirComputing.jl leverages CellularAutomata.jl that needs to be called as well to define the RECA struct:\n\nusing ReservoirComputing, CellularAutomata, Random\nRandom.seed!(42)\nrng = MersenneTwister(17)\n\nca = DCA(90)\n\nTo define the ReCA model, it suffices to call:\n\nreca = RECA(4, 4, DCA(90);\n           generations=16,\n           input_encoding=RandomMapping(16, 40))\nps, st = setup(rng, reca)\n\nAfter this, the training can be performed with the chosen method.\n\nps, st = train!(reca, input, output, ps, st, StandardRidge(0.00001))\n\nWe are going to test the recall ability of the model, feeding the input data and investigating whether the predicted output equals the output data.\n\nst0 = resetcarry!(rng, reca, st) #reset the first ca state\npred_out, st = predict(reca, input, ps, st0)\nfinal_pred = convert(AbstractArray{Float32}, pred_out .> 0.5)\nfinal_pred == output","category":"section"},{"location":"api/inits/backward_connection!/#backward_connection!","page":"backward_connection!","title":"backward_connection!","text":"","category":"section"},{"location":"api/inits/backward_connection!/#ReservoirComputing.backward_connection!","page":"backward_connection!","title":"ReservoirComputing.backward_connection!","text":"backward_connection!([rng], reservoir_matrix, weight, shift;\n    sampling_type=:no_sample, irrational=pi, start=1,\n    p=0.5)\n\nAdds a backward connection in the reservoir_matrix, with given shift and weight. The weight can be a single number or an array.\n\nArguments\n\nrng: Random number generator. Default is Utils.default_rng() from WeightInitializers.\nreservoir_matrix: matrix to be changed.\nweight: weight to add as a backward connection. Can be either a single number or an array.\nshift: How far the backward connection will be from the diagonal.\n\nKeyword arguments\n\nsampling_type: Sampling that decides the distribution of weight negative numbers. If set to :no_sample the sign is unchanged. If set to :bernoulli_sample! then each weight can be positive with a probability set by positive_prob. If set to :irrational_sample! the weight is negative if the decimal number of the irrational number chosen is odd. If set to :regular_sample!, each weight will be assigned a negative sign after the chosen strides. strides can be a single number or an array. Default is :no_sample.\npositive_prob: probability of the weight being positive when sampling_type is set to :bernoulli_sample!. Default is 0.5.\nirrational: Irrational number whose decimals decide the sign of weight. Default is pi.\nstart: Which place after the decimal point the counting starts for the irrational sign counting. Default is 1.\nstrides: number of strides for assigning negative value to a weight. It can be an integer or an array. Default is 2.\n\nExamples\n\njulia> matrix = zeros(Float32, 5, 5)\n5×5 Matrix{Float32}:\n 0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n\njulia> backward_connection!(matrix, 3.0, 1)\n5×5 Matrix{Float32}:\n 0.0  3.0  0.0  0.0  0.0\n 0.0  0.0  3.0  0.0  0.0\n 0.0  0.0  0.0  3.0  0.0\n 0.0  0.0  0.0  0.0  3.0\n 0.0  0.0  0.0  0.0  0.0\n\njulia> backward_connection!(matrix, 3.0, 1; sampling_type = :bernoulli_sample!)\n5×5 Matrix{Float32}:\n 0.0  3.0   0.0  0.0   0.0\n 0.0  0.0  -3.0  0.0   0.0\n 0.0  0.0   0.0  3.0   0.0\n 0.0  0.0   0.0  0.0  -3.0\n 0.0  0.0   0.0  0.0   0.0\n\n\n\n\n\n","category":"function"},{"location":"tutorials/scratch/#Building-a-model-from-scratch","page":"Building a model from scratch","title":"Building a model from scratch","text":"ReservoirComputing.jl provides utilities to build reservoir reservoir computing models from scratch. In this tutorial we are going to build an echo state network (ESN) and showcase how this custom implementation is equivalent to the provided model (minus some comfort utilities).","category":"section"},{"location":"tutorials/scratch/#Using-provided-layers:-ReservoirChain,-ESNCell,-and-LinearReadout","page":"Building a model from scratch","title":"Using provided layers: ReservoirChain, ESNCell, and LinearReadout","text":"The library provides a ReservoirChain, which is virtually equivalent to Lux's Chain. Passing layers, or functions, to the chain will concatenate them, and will allow the flow of the input data through the model.\n\nTo build an ESN we also need a ESNCell to provide the ESN forward pass. However, the cell is stateless, so to keep the memory of the input we need to wrap it in a StatefulLayer, which saves the internal state in the model states st and feeds it to the cell in the next step.\n\nFinally, we need the trainable readout for the reservoir computing. The library provides LinearReadout, a dense layer the weights of which will be trained using linear regression.\n\nPutting it all together we get the following\n\nusing ReservoirComputing\n\nesn_scratch = ReservoirChain(\n    StatefulLayer(\n        ESNCell(3=>50)\n    ),\n    LinearReadout(50=>1)\n)\n\nNow, this implementation, elements naming aside, is completely equivalent to the following\n\nesn = ESN(3, 50, 1)\n\nand we can check it initializing the two models and comparing, for instance, the weights of the input layer:\n\nusing Random\nRandom.seed!(43)\n\nrng = MersenneTwister(17)\nps_s, st_s = setup(rng, esn_scratch)\n\nrng = MersenneTwister(17)\nps, st = setup(rng, esn)\n\nps_s.layer_1.input_matrix == ps.reservoir.input_matrix\n\nBoth the models can be trained using train!, and predictions can be obtained with predict. The internal states collected for linear regression are computed by traversing the ReservoirChain, and stopping right before the LinearReadout.","category":"section"},{"location":"tutorials/scratch/#Manual-state-collection-with-Collect","page":"Building a model from scratch","title":"Manual state collection with Collect","text":"For more complicated models usually you would want to control when the state collection happens. In a ReservoirChain, the collection of states is controlled by the layer Collect. The role of this layer is to tell the collectstates function where to stop for state collection. All the readout layers have a include_collect=true keyword, which forces a Collect layer bvefore the readout. The model we wrote before can be written as\n\nesn_scratch = ReservoirChain(\n    StatefulLayer(\n        ESNCell(3=>50)\n    ),\n    Collect(),\n    LinearReadout(50=>1; include_collect=false)\n)\n\nto make the collection explicit. This layer is useful in case one needs to build more complicated models such as a DeepESN. We can build a deep model in multiple ways:\n\ndeepesn_scratch = ReservoirChain(\n    StatefulLayer(\n        ESNCell(3=>50)\n    ),\n    StatefulLayer(\n        ESNCell(50=>50)\n    ),\n    StatefulLayer(\n        ESNCell(50=>50)\n    ),\n    Collect(),\n    LinearReadout(50=>1; include_collect=false)\n)\n\nthis first approach is the one provided by default in the library through DeepESN. However, you could want the state collection to be after each cell\n\ndeepesn_scratch = ReservoirChain(\n    StatefulLayer(\n        ESNCell(3=>50)\n    ),\n    Collect(),\n    StatefulLayer(\n        ESNCell(50=>50)\n    ),\n    Collect(),\n    StatefulLayer(\n        ESNCell(50=>50)\n    ),\n    Collect(),\n    LinearReadout(50=>1; include_collect=false)\n)\n\nWith this approach, the resulting state will be a concatenation of the states at each Collect point. So the resulting states for this architecture will be vector of size 150.\n\nps, st = setup(rng, deepesn_scratch)\nstates, st = collectstates(deepesn_scratch, rand(3, 300), ps, st)\nsize(states[:,1])\n\nThis allows for even more complex constructions, where the state collection follows specific patterns\n\ndeepesn_scratch = ReservoirChain(\n    StatefulLayer(\n        ESNCell(3=>50)\n    ),\n    StatefulLayer(\n        ESNCell(50=>50)\n    ),\n    Collect(),\n    StatefulLayer(\n        ESNCell(50=>50)\n    ),\n    Collect(),\n    LinearReadout(50=>1; include_collect=false)\n)\n\nHere, for instance, we have a Collect after the first two cells and then one at the very end. You can see how the size of the states is now 100:\n\nps, st = setup(rng, deepesn_scratch)\nstates, st = collectstates(deepesn_scratch, rand(3, 300), ps, st)\nsize(states[:,1])\n\nSimilar approaches could be leveraged, for instance, when the data show multiscale dynamics that require specific modeling approaches.","category":"section"},{"location":"api/inits/delay_line!/#delay_line!","page":"delay_line!","title":"delay_line!","text":"","category":"section"},{"location":"api/inits/delay_line!/#ReservoirComputing.delay_line!","page":"delay_line!","title":"ReservoirComputing.delay_line!","text":"delay_line!([rng], reservoir_matrix, weight, shift;\n    sampling_type=:no_sample, irrational=pi, start=1,\n    p=0.5)\n\nAdds a delay line in the reservoir_matrix, with given shift and weight. The weight can be a single number or an array.\n\nArguments\n\nrng: Random number generator. Default is Utils.default_rng() from WeightInitializers.\nreservoir_matrix: matrix to be changed.\nweight: weight to add as a delay line. Can be either a single number or an array.\nshift: How far the delay line will be from the diagonal.\n\nKeyword arguments\n\nsampling_type: Sampling that decides the distribution of weight negative numbers. If set to :no_sample the sign is unchanged. If set to :bernoulli_sample! then each weight can be positive with a probability set by positive_prob. If set to :irrational_sample! the weight is negative if the decimal number of the irrational number chosen is odd. If set to :regular_sample!, each weight will be assigned a negative sign after the chosen strides. strides can be a single number or an array. Default is :no_sample.\npositive_prob: probability of the weight being positive when sampling_type is set to :bernoulli_sample!. Default is 0.5.\nirrational: Irrational number whose decimals decide the sign of weight. Default is pi.\nstart: Which place after the decimal point the counting starts for the irrational sign counting. Default is 1.\nstrides: number of strides for assigning negative value to a weight. It can be an integer or an array. Default is 2.\n\nExamples\n\njulia> matrix = zeros(Float32, 5, 5)\n5×5 Matrix{Float32}:\n 0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n\njulia> delay_line!(matrix, 5.0, 2)\n5×5 Matrix{Float32}:\n 0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n 5.0  0.0  0.0  0.0  0.0\n 0.0  5.0  0.0  0.0  0.0\n 0.0  0.0  5.0  0.0  0.0\n\n julia> delay_line!(matrix, 5.0, 2; sampling_type=:bernoulli_sample!)\n5×5 Matrix{Float32}:\n 0.0   0.0  0.0  0.0  0.0\n 0.0   0.0  0.0  0.0  0.0\n 5.0   0.0  0.0  0.0  0.0\n 0.0  -5.0  0.0  0.0  0.0\n 0.0   0.0  5.0  0.0  0.0\n\n\n\n\n\n","category":"function"},{"location":"tutorials/deep_esn/#Deep-Echo-State-Networks","page":"Deep Echo State Networks","title":"Deep Echo State Networks","text":"In this example we showcase how to build a deep echo state network (DeepESN) following the work of (Gallicchio and Micheli, 2017). The DeepESN stacks reservoirs on top of each other, feeding the output from one into the next. In the version implemented in ReservoirComputing.jl the final state is the state used for training.","category":"section"},{"location":"tutorials/deep_esn/#Lorenz-Example","page":"Deep Echo State Networks","title":"Lorenz Example","text":"We are going to reuse the Lorenz data used in the Lorenz System Forecasting example.\n\nusing OrdinaryDiffEq\n\n#define lorenz system\nfunction lorenz!(du, u, p, t)\n    du[1] = 10.0 * (u[2] - u[1])\n    du[2] = u[1] * (28.0 - u[3]) - u[2]\n    du[3] = u[1] * u[2] - (8 / 3) * u[3]\nend\n\n#solve and take data\nprob = ODEProblem(lorenz!, [1.0, 0.0, 0.0], (0.0, 200.0))\ndata = solve(prob, ABM54(); dt=0.02)\ndata = reduce(hcat, data.u)\n\n#determine shift length, training length and prediction length\nshift = 300\ntrain_len = 5000\npredict_len = 1250\n\n#split the data accordingly\ninput_data = data[:, shift:(shift + train_len - 1)]\ntarget_data = data[:, (shift + 1):(shift + train_len)]\ntest_data = data[:, (shift + train_len + 1):(shift + train_len + predict_len)]\n\nThe call for the DeepESN works similarly to the ESN. The only difference is that the reservoir (and corresponding kwargs) can be fed as an array.\n\nusing ReservoirComputing\ninput_size = 3\nres_size = 300\ndesn = DeepESN(input_size, [res_size, res_size], input_size;\n    init_reservoir=rand_sparse(; radius=1.2, sparsity=6/300),\n    state_modifiers=[NLAT2, ExtendedSquare]\n)\n\n\nThe training and prediction follow the usual framework:\n\nusing Random\nRandom.seed!(42)\nrng = MersenneTwister(17)\n\nps, st = setup(rng, desn)\nps, st = train!(desn, input_data, target_data, ps, st)\n\noutput, st = predict(desn, 1250, ps, st; initialdata=test_data[:, 1])\n\nPlotting the results:\n\nusing Plots\n\nts = 0.0:0.02:200.0\nlorenz_maxlyap = 0.9056\npredict_ts = ts[(shift + train_len + 1):(shift + train_len + predict_len)]\nlyap_time = (predict_ts .- predict_ts[1]) * (1 / lorenz_maxlyap)\n\np1 = plot(lyap_time, [test_data[1, :] output[1, :]]; label=[\"actual\" \"predicted\"],\n    ylabel=\"x(t)\", linewidth=2.5, xticks=false, yticks=-15:15:15);\np2 = plot(lyap_time, [test_data[2, :] output[2, :]]; label=[\"actual\" \"predicted\"],\n    ylabel=\"y(t)\", linewidth=2.5, xticks=false, yticks=-20:20:20);\np3 = plot(lyap_time, [test_data[3, :] output[3, :]]; label=[\"actual\" \"predicted\"],\n    ylabel=\"z(t)\", linewidth=2.5, xlabel=\"max(λ)*t\", yticks=10:15:40);\n\nplot(p1, p2, p3; plot_title=\"Lorenz System Coordinates\",\n    layout=(3, 1), xtickfontsize=12, ytickfontsize=12, xguidefontsize=15,\n    yguidefontsize=15,\n    legendfontsize=12, titlefontsize=20)","category":"section"},{"location":"tutorials/deep_esn/#References","page":"Deep Echo State Networks","title":"References","text":"Gallicchio, C. and Micheli, A. (2017). Deep echo state network (deepesn): A brief survey, arXiv preprint arXiv:1712.04323.\n\n\n\n","category":"section"},{"location":"api/inits/true_doublecycle/#true*double*cycle","page":"truedoublecycle","title":"truedoublecycle","text":"","category":"section"},{"location":"api/inits/true_doublecycle/#References","page":"truedoublecycle","title":"References","text":"Fu, J.; Li, G.; Tang, J.; Xia, L.; Wang, L. and Duan, S. (2023). A double-cycle echo state network topology for time series prediction. Chaos: An Interdisciplinary Journal of Nonlinear Science 33.\n\n\n\nRodan, A. and Tino, P. (2011). Minimum Complexity Echo State Network. IEEE Transactions on Neural Networks 22, 131–144.\n\n\n\n","category":"section"},{"location":"api/inits/true_doublecycle/#ReservoirComputing.true_doublecycle","page":"truedoublecycle","title":"ReservoirComputing.true_doublecycle","text":"true_doublecycle([rng], [T], dims...;\n    cycle_weight=0.1, second_cycle_weight=0.1, radius=nothing,\n    return_sparse=false, cycle_kwargs=(), second_cycle_kwargs=())\n\nCreates a true double cycle reservoir, ispired by (Fu et al., 2023), with cycles built on the definition by (Rodan and Tino, 2011).\n\nW_ij =\nbegincases\n    r_1  textif  i = j + 1 j in 1 D_mathrmres - 1 4pt\n    r_1  textif  i = 1 j = D_mathrmres 6pt\n    r_2  textif  j = i + 1 i in 1 D_mathrmres - 1 4pt\n    r_2  textif  i = D_mathrmres j = 1 6pt\n    0    textotherwise\nendcases\n\nArguments\n\nrng: Random number generator. Default is Utils.default_rng()from WeightInitializers.\nT: Type of the elements in the reservoir matrix. Default is Float32.\ndims: Dimensions of the reservoir matrix.\n\nKeyword arguments\n\ncycle_weight: Weight of the upper cycle connections in the reservoir matrix. Default is 0.1.\nsecond_cycle_weight: Weight of the lower cycle connections in the reservoir matrix. Default is 0.1.\nradius: The desired spectral radius of the reservoir. If nothing is passed, no scaling takes place. Defaults to nothing.\nreturn_sparse: flag for returning a sparse matrix. true requires SparseArrays to be loaded. Default is false.\ncycle_kwargs, and second_cycle_kwargs: named tuples that control the kwargs for the weights generation. The kwargs are as follows:\nsampling_type: Sampling that decides the distribution of weight negative numbers. If set to :no_sample the sign is unchanged. If set to :bernoulli_sample! then each weight can be positive with a probability set by positive_prob. If set to :irrational_sample! the weight is negative if the decimal number of the irrational number chosen is odd. If set to :regular_sample!, each weight will be assigned a negative sign after the chosen strides. strides can be a single number or an array. Default is :no_sample.\npositive_prob: probability of the weight being positive when sampling_type is set to :bernoulli_sample!. Default is 0.5.\nirrational: Irrational number whose decimals decide the sign of weight. Default is pi.\nstart: Which place after the decimal point the counting starts for the irrational sign counting. Default is 1.\nstrides: number of strides for assigning negative value to a weight. It can be an integer or an array. Default is 2.\n\nExamples\n\nDefault call:\n\njulia> res_matrix = true_doublecycle(5, 5)\n5×5 Matrix{Float32}:\n 0.0  0.1  0.0  0.0  0.1\n 0.1  0.0  0.1  0.0  0.0\n 0.0  0.1  0.0  0.1  0.0\n 0.0  0.0  0.1  0.0  0.1\n 0.1  0.0  0.0  0.1  0.0\n\nChanging weights:\n\njulia> res_matrix = true_doublecycle(5, 5; cycle_weight = 0.1, second_cycle_weight = 0.3)\n5×5 Matrix{Float32}:\n 0.0  0.3  0.0  0.0  0.1\n 0.1  0.0  0.3  0.0  0.0\n 0.0  0.1  0.0  0.3  0.0\n 0.0  0.0  0.1  0.0  0.3\n 0.3  0.0  0.0  0.1  0.0\n\nChanging weights to custom arrays:\n\njulia> res_matrix = true_doublecycle(5, 5; cycle_weight = rand(5), second_cycle_weight = .-rand(5))\n5×5 Matrix{Float32}:\n  0.0       -0.647066   0.0        0.0        0.604095\n  0.6687     0.0       -0.853307   0.0        0.0\n  0.0        0.40399    0.0       -0.565928   0.0\n  0.0        0.0        0.960196   0.0       -0.120321\n -0.120321   0.0        0.0        0.874008   0.0\n\nChanging sign of the weights with different samplings:\n\njulia> res_matrix = true_doublecycle(5, 5; cycle_kwargs=(;sampling_type=:irrational_sample!))\n5×5 Matrix{Float32}:\n  0.0  0.1   0.0   0.0  -0.1\n -0.1  0.0   0.1   0.0   0.0\n  0.0  0.1   0.0   0.1   0.0\n  0.0  0.0  -0.1   0.0   0.1\n  0.1  0.0   0.0  -0.1   0.0\n\njulia> res_matrix = true_doublecycle(5, 5; second_cycle_kwargs=(;sampling_type=:bernoulli_sample!))\n5×5 Matrix{Float32}:\n 0.0  -0.1  0.0   0.0  0.1\n 0.1   0.0  0.1   0.0  0.0\n 0.0   0.1  0.0  -0.1  0.0\n 0.0   0.0  0.1   0.0  0.1\n 0.1   0.0  0.0   0.1  0.0\n\nReturning as sparse:\n\njulia> res_matrix = true_doublecycle(5, 5; return_sparse=true)\n5×5 SparseMatrixCSC{Float32, Int64} with 10 stored entries:\n  ⋅   0.1   ⋅    ⋅   0.1\n 0.1   ⋅   0.1   ⋅    ⋅\n  ⋅   0.1   ⋅   0.1   ⋅\n  ⋅    ⋅   0.1   ⋅   0.1\n 0.1   ⋅    ⋅   0.1   ⋅\n\n\n\n\n\n","category":"function"},{"location":"tutorials/lorenz_basic/#Lorenz-System-Forecasting","page":"Chaos forecasting with an ESN","title":"Lorenz System Forecasting","text":"This example expands on the readme Lorenz system forecasting to showcase how to use models and methods provided in the library for Echo State Networks.","category":"section"},{"location":"tutorials/lorenz_basic/#Generating-the-data","page":"Chaos forecasting with an ESN","title":"Generating the data","text":"Starting off the workflow, the first step is to obtain the data. We use OrdinaryDiffEq to derive the Lorenz system data. The data is passed to the model as a matrix, where the columns represent the time steps.\n\nusing OrdinaryDiffEq\n\n#define lorenz system\nfunction lorenz!(du, u, p, t)\n    du[1] = 10.0 * (u[2] - u[1])\n    du[2] = u[1] * (28.0 - u[3]) - u[2]\n    du[3] = u[1] * u[2] - (8 / 3) * u[3]\nend\n\n#solve and take data\nprob = ODEProblem(lorenz!, [1.0, 0.0, 0.0], (0.0, 200.0))\ndata = solve(prob, ABM54(); dt=0.02)\ndata = reduce(hcat, data.u)\n\nNow we split the data in training and testing. To do an autoregressive forecast we want the model to be trained on the next step, so we are going to shift the target data by one. Additionally, we discard the transient period.\n\n#determine shift length, training length and prediction length\nshift = 300\ntrain_len = 5000\npredict_len = 1250\n\n#split the data accordingly\ninput_data = data[:, shift:(shift + train_len - 1)]\ntarget_data = data[:, (shift + 1):(shift + train_len)]\ntest_data = data[:, (shift + train_len + 1):(shift + train_len + predict_len)]\n\nIt is important to notice that the data needs to be formatted in a matrix with the features as rows and time steps as columns as in this example This is needed even if the time series consists of single values.","category":"section"},{"location":"tutorials/lorenz_basic/#Building-the-Echo-State-Network","page":"Chaos forecasting with an ESN","title":"Building the Echo State Network","text":"Once the data is ready, it is possible to define the parameters for the ESN and the ESN struct itself. In this example, the values from (Pathak et al., 2017) are loosely followed as general guidelines.\n\nusing ReservoirComputing\n\n#define ESN parameters\nres_size = 300\nin_size = 3\nres_radius = 1.2\nres_sparsity = 6 / 300\ninput_scaling = 0.1\n\n#build ESN struct\nesn = ESN(in_size, res_size, in_size; #autoregressive so in_size = out_size\n    init_reservoir = rand_sparse(; radius = res_radius, sparsity = res_sparsity),\n    init_input = weighted_init(; scaling = input_scaling),\n    state_modifiers = NLAT2\n)\n\nIn this case, a size of 300 has been chosen, so the reservoir matrix will be 300 x 300. However, this is not always the case, since some input layer constructions can modify the dimensions of the reservoir. Please make sure to read the API documentation of the initializer you intend to use if you think that is cause of errors.\n\nThe res_radius determines the scaling of the spectral radius of the reservoir matrix; a proper scaling is necessary to assure the Echo State Property. The default value in the rand_sparse method is 1.0 in accordance with the most commonly followed guidelines found in the literature (see (Lukoševičius, 2012) and references therein).\n\nThe value of input_scaling determines the upper and lower bounds of the uniform distribution of the weights in the weighted_init. The value of 0.1 represents the default. The default input layer is the scaled_rand, a dense matrix. The details of the weighted version can be found in (Lu et al., 2017), for this example, this version returns the best results.","category":"section"},{"location":"tutorials/lorenz_basic/#Training-and-Prediction","page":"Chaos forecasting with an ESN","title":"Training and Prediction","text":"Training for ESNs usually means solving a linear regression. The libbrary supports solvers from 'MLILinearModels.jl', in addition to a custom implementation of ridge regression. In this example we will use the latter.\n\nSince ReservoirComputing.jl builds on LuxCore.jl we first need to setup the state and the parameters\n\nusing Random\nRandom.seed!(42)\nrng = MersenneTwister(17)\n\nps, st = setup(rng, esn)\n\nNow we can proceed with training the ESN model. Usually an initial transient is discarded, to account for the dynamics of the ESN to settle. This can be done by passing the washout keyword argument to train.\n\n#define training method\ntraining_method = StandardRidge(0.0)\n\nps, st = train!(esn, input_data, target_data, ps, st, training_method)\n\nps now contains the trained parameters for the ESN.\n\ninfo: Returning training states\nThe ESN states are internally used the training, however they are not returned by default. To inspect the states, it is necessary to set the boolean keyword argument return_states as true in the train! call.\n\nReservoirComputing.jl provides additional utilities functions for autoregressive forecasting:\n\noutput, st = predict(esn, predict_len, ps, st; initialdata=test_data[:, 1])\n\nTo inspect the results, they can easily be plotted using an external library. In this case, we will use Plots.jl:\n\nusing Plots, Plots.PlotMeasures\n\nts = 0.0:0.02:200.0\nlorenz_maxlyap = 0.9056\npredict_ts = ts[(shift + train_len + 1):(shift + train_len + predict_len)]\nlyap_time = (predict_ts .- predict_ts[1]) * (1 / lorenz_maxlyap)\n\np1 = plot(lyap_time, [test_data[1, :] output[1, :]]; label=[\"actual\" \"predicted\"],\n    ylabel=\"x(t)\", linewidth=2.5, xticks=false, yticks=-15:15:15);\np2 = plot(lyap_time, [test_data[2, :] output[2, :]]; label=[\"actual\" \"predicted\"],\n    ylabel=\"y(t)\", linewidth=2.5, xticks=false, yticks=-20:20:20);\np3 = plot(lyap_time, [test_data[3, :] output[3, :]]; label=[\"actual\" \"predicted\"],\n    ylabel=\"z(t)\", linewidth=2.5, xlabel=\"max(λ)*t\", yticks=10:15:40);\n\nplot(p1, p2, p3; plot_title=\"Lorenz System Coordinates\",\n    layout=(3, 1), xtickfontsize=12, ytickfontsize=12, xguidefontsize=15,\n    yguidefontsize=15,\n    legendfontsize=12, titlefontsize=20)","category":"section"},{"location":"tutorials/lorenz_basic/#References","page":"Chaos forecasting with an ESN","title":"References","text":"Lu, Z.; Pathak, J.; Hunt, B.; Girvan, M.; Brockett, R. and Ott, E. (2017). Reservoir observers: Model-free inference of unmeasured variables in chaotic systems. Chaos: An Interdisciplinary Journal of Nonlinear Science 27.\n\n\n\nLukoševičius, M. (2012). A Practical Guide to Applying Echo State Networks. In: Neural Networks: Tricks of the Trade (Springer Berlin Heidelberg); pp. 659–686.\n\n\n\nPathak, J.; Lu, Z.; Hunt, B. R.; Girvan, M. and Ott, E. (2017). Using machine learning to replicate chaotic attractors and calculate Lyapunov exponents from data. Chaos: An Interdisciplinary Journal of Nonlinear Science 27.\n\n\n\n","category":"section"},{"location":"api/inits/selfloop_backward_cycle/#selfloop*feedback*cycle","page":"selfloopfeedbackcycle","title":"selfloopfeedbackcycle","text":"","category":"section"},{"location":"api/inits/selfloop_backward_cycle/#References","page":"selfloopfeedbackcycle","title":"References","text":"Elsarraj, D.; Qisi, M. A.; Rodan, A.; Obeid, N.; Sharieh, A. and Faris, H. (2019). Demystifying echo state network with deterministic simple topologies. International Journal of Computational Science and Engineering 19, 407–417.\n\n\n\n","category":"section"},{"location":"api/inits/selfloop_backward_cycle/#ReservoirComputing.selfloop_backward_cycle","page":"selfloopfeedbackcycle","title":"ReservoirComputing.selfloop_backward_cycle","text":"selfloop_backward_cycle([rng], [T], dims...;\n    cycle_weight=0.1, selfloop_weight=0.1,\n    fb_weight = 0.1, radius=nothing, return_sparse=false)\n\nCreates a cycle reservoir with feedback connections on even neurons and self loops on odd neurons (Elsarraj et al., 2019).\n\nThis architecture is referred to as TP2 in the original paper.\n\nW_ij =\nbegincases\n    r  textif  j = i - 1 text for  i = 2 dots N \n    r  textif  i = 1 j = N \n    ll  textif  i = j text and  i text is odd \n    r  textif  j = i + 1 text and  i text is even i neq N \n    0  textotherwise\nendcases\n\nArguments\n\nrng: Random number generator. Default is Utils.default_rng()from WeightInitializers.\nT: Type of the elements in the reservoir matrix. Default is Float32.\ndims: Dimensions of the reservoir matrix.\n\nKeyword arguments\n\ncycle_weight: Weight of the cycle connections in the reservoir matrix. This can be provided as a single value or an array. In case it is provided as an array please make sure that the length of the array matches the length of the cycle you want to populate. Default is 0.1.\nselfloop_weight: Weight of the self loops in the reservoir matrix. Default is 0.1.\nfb_weight: Weight of the self loops in the reservoir matrix. Default is 0.1.\nradius: The desired spectral radius of the reservoir. If nothing is passed, no scaling takes place. Defaults to nothing.\nreturn_sparse: flag for returning a sparse matrix. true requires SparseArrays to be loaded. Default is false.\n\nExamples\n\njulia> reservoir_matrix = selfloop_backward_cycle(5, 5)\n5×5 Matrix{Float32}:\n 0.1  0.1  0.0  0.0  0.1\n 0.1  0.0  0.0  0.0  0.0\n 0.0  0.1  0.1  0.1  0.0\n 0.0  0.0  0.1  0.0  0.0\n 0.0  0.0  0.0  0.1  0.1\n\njulia> reservoir_matrix = selfloop_backward_cycle(5, 5; self_loop_weight=0.5)\n5×5 Matrix{Float32}:\n 0.5  0.1  0.0  0.0  0.1\n 0.1  0.0  0.0  0.0  0.0\n 0.0  0.1  0.5  0.1  0.0\n 0.0  0.0  0.1  0.0  0.0\n 0.0  0.0  0.0  0.1  0.5\n\n\n\n\n\n","category":"function"},{"location":"api/inits/block_diagonal/#block_diagonal","page":"block_diagonal","title":"block_diagonal","text":"","category":"section"},{"location":"api/inits/block_diagonal/#References","page":"block_diagonal","title":"References","text":"Ma, H.; Prosperino, D.; Haluszczynski, A. and Räth, C. (2023). Efficient forecasting of chaotic systems with block-diagonal and binary reservoir computing. Chaos: An Interdisciplinary Journal of Nonlinear Science 33.\n\n\n\n","category":"section"},{"location":"api/inits/block_diagonal/#ReservoirComputing.block_diagonal","page":"block_diagonal","title":"ReservoirComputing.block_diagonal","text":"block_diagonal([rng], [T], dims...;\n    block_weight=1, block_size=1,\n    radius=nothing, return_sparse=false)\n\nCreates a block‐diagonal matrix consisting of square blocks of size block_size along the main diagonal (Ma et al., 2023). Each block may be filled with\n\na single scalar\na vector of per‐block weights (length = number of blocks)\n\nEquations\n\nW_ij =\nbegincases\n    w_b  textif leftlfloorfraci-1srightrfloor =\n        leftlfloorfracj-1srightrfloor = b\n           s = textblock_size b=0dotsnb-1 \n    0    textotherwise\nendcases\n\nArguments\n\nrng: Random number generator. Default is Utils.default_rng() from WeightInitializers.\nT: Element type of the matrix. Default is Float32.\ndims: Dimensions of the output matrix (must be two-dimensional).\n\nKeyword arguments\n\nblock_weight:\nscalar: every block is filled with that value\nvector: length = number of blocks, one constant per block\nDefault is 1.0.\nblock_size: Size(s) of each square block on the diagonal. Default is 1.0.\nradius: The desired spectral radius of the reservoir. If nothing is passed, no scaling takes place. Defaults to nothing.\nreturn_sparse: If true, returns the matrix as sparse. SparseArrays.jl must be lodead. Default is false.\n\nExamples\n\nChanging the block size\n\njulia> res_matrix = block_diagonal(10, 10; block_size=2)\n10×10 Matrix{Float32}:\n 1.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 1.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  1.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  1.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  1.0  1.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  1.0  1.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  1.0  1.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  1.0  1.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  1.0\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  1.0\n\nChanging the weights, per block. Please note that you have to know the number of blocks that you are going to have (which usually is res_size/block_size).\n\njulia> res_matrix = block_diagonal(10, 10; block_size=2, block_weight=[0.5, 2.0, -0.99, 1.0, -99.0])\n10×10 Matrix{Float32}:\n 0.5  0.5  0.0  0.0   0.0    0.0   0.0  0.0    0.0    0.0\n 0.5  0.5  0.0  0.0   0.0    0.0   0.0  0.0    0.0    0.0\n 0.0  0.0  2.0  2.0   0.0    0.0   0.0  0.0    0.0    0.0\n 0.0  0.0  2.0  2.0   0.0    0.0   0.0  0.0    0.0    0.0\n 0.0  0.0  0.0  0.0  -0.99  -0.99  0.0  0.0    0.0    0.0\n 0.0  0.0  0.0  0.0  -0.99  -0.99  0.0  0.0    0.0    0.0\n 0.0  0.0  0.0  0.0   0.0    0.0   1.0  1.0    0.0    0.0\n 0.0  0.0  0.0  0.0   0.0    0.0   1.0  1.0    0.0    0.0\n 0.0  0.0  0.0  0.0   0.0    0.0   0.0  0.0  -99.0  -99.0\n 0.0  0.0  0.0  0.0   0.0    0.0   0.0  0.0  -99.0  -99.0\n\n\n\n\n\n","category":"function"},{"location":"api/inits/#Echo-State-Networks-Initializers","page":"Initializers","title":"Echo State Networks Initializers","text":"This page lists all initializers available in ReservoirComputing.jl.   Clicking on any initializer name will take you to its dedicated documentation page, where full details and examples are provided.","category":"section"},{"location":"api/inits/#Input-layers","page":"Initializers","title":"Input layers","text":"chebyshev_mapping: Creates an input matrix   using sine initialization followed by Chebyshev iterative mapping.\ninformed_init: Builds an informed ESN input   layer allocating input vs. model channels based on γ-split.\nlogistic_mapping: Generates an input   layer using sine initialization followed by logistic-map recursion.\nminimal_init: Creates a uniform-weight input   layer with signs determined by a sampling scheme.\nmodified_lm: Builds an input-expanding   logistic-map chain for each input dimension.\nscaled_rand: Produces a uniformly scaled random   input matrix with per-column or global scaling.\nweighted_init: Creates a block-structured   weighted input layer with random weights per block.\nweighted_minimal: Generates a deterministic   block-structured weighted input layer with optional sign sampling.","category":"section"},{"location":"api/inits/#Reservoirs","page":"Initializers","title":"Reservoirs","text":"block_diagonal: Constructs a block-diagonal   reservoir with constant-valued square blocks.\nchaotic_init: Generates a reservoir from a digital   chaotic adjacency graph with rescaled spectral radius.\ncycle_jumps: Builds a cycle reservoir augmented   with periodic jump connections.\ndelay_line: Creates a delay-line reservoir using   fixed offsets from the diagonal.\ndelayline_backward: Produces a delay-line   reservoir with additional backward (feedback) connections.\ndouble_cycle: Creates two interlaced directed   cycles (upper & lower) with independent weights.\nforward_connection: Builds a reservoir   where each node connects forward by two steps.\nlow_connectivity: Creates a low-degree   random (or enforced cycle) connectivity reservoir.\npseudo_svd: Builds a reservoir by iteratively   perturbing a diagonal matrix using pseudo-SVD rotations.\nrand_sparse: Generates a random sparse   reservoir with controlled sparsity and spectral radius.\nselfloop_cycle: Builds a simple cycle   reservoir enhanced with self-loops on all nodes.\nselfloop_delayline_backward: Combines delay   line, self-loops, and backward offsets into one architecture.\nselfloop_backward_cycle: Creates a   cycle where odd nodes self-loop and even nodes have forward/backward links.\nselfloop_forwardconnection: Adds self-loops   onto a forward-connection reservoir (stride-2).\nsimple_cycle: Builds a basic directed ring   reservoir with uniform weights.\ntrue_doublecycle: Constructs two overlapping   cycles (forward + backward) using Rodan-style cycle rules.","category":"section"},{"location":"api/inits/#Building-functions","page":"Initializers","title":"Building functions","text":"add_jumps!: Inserts jump connections at   fixed intervals into an existing reservoir.\nbackward_connection!: Adds backward   (feedback) connections at a fixed shift.\ndelay_line!: Writes delay connections into   an existing matrix at a specified diagonal offset.\nreverse_simple_cycle!: Adds a   reversed directed cycle (descending indices) to a matrix.\nscale_radius!: Rescales a reservoir   matrix to match a target spectral radius.\nself_loop!: Adds self-loop weights   along the diagonal of an existing matrix.\nsimple_cycle!: Writes a directed   cycle pattern into a preallocated reservoir matrix.","category":"section"},{"location":"api/inits/weighted_init/#weighted_init","page":"weighted_init","title":"weighted_init","text":"","category":"section"},{"location":"api/inits/weighted_init/#References","page":"weighted_init","title":"References","text":"Lu, Z.; Pathak, J.; Hunt, B.; Girvan, M.; Brockett, R. and Ott, E. (2017). Reservoir observers: Model-free inference of unmeasured variables in chaotic systems. Chaos: An Interdisciplinary Journal of Nonlinear Science 27.\n\n\n\n","category":"section"},{"location":"api/inits/weighted_init/#ReservoirComputing.weighted_init","page":"weighted_init","title":"ReservoirComputing.weighted_init","text":"weighted_init([rng], [T], dims...;\n    scaling=0.1, return_sparse=false)\n\nCreate and return a weighted input layer matrix. In this matrix, each of the input signals in_size connects to the reservoir nodes res_size/in_size. The nonzero entries are distributed uniformly within a range defined by scaling (Lu et al., 2017).\n\nPlease note that this initializer computes its own reservoir size! If the computed reservoir size is different than the provided one it will raise a warning.\n\nArguments\n\nrng: Random number generator. Default is Utils.default_rng() from WeightInitializers.\nT: Type of the elements in the reservoir matrix. Default is Float32.\ndims: Dimensions of the matrix. Should follow res_size x in_size.\n\nKeyword arguments\n\nscaling: A scaling factor to define the range of the uniform distribution. The factor can be passed in three different ways:\nA single number. In this case, the matrix elements will be randomly chosen from the range [-scaling, scaling]. Default option, with a the scaling value set to 0.1.\nA tuple (lower, upper). The values define the range of the distribution. the matrix elements will be randomly created and scaled the range [lower, upper].\nA vector of length = in_size. In this case, the columns will be scaled individually by the entries of the vector. The entries can be numbers or tuples, which will mirror the behavior described above.\nreturn_sparse: flag for returning a sparse matrix. Default is false.\n\nExamples\n\nStandard call with scaling provided by a scalar:\n\njulia> res_input = weighted_init(9, 3; scaling = 0.1)\n9×3 Matrix{Float32}:\n  0.0452399   0.0         0.0\n -0.0348047   0.0         0.0\n -0.0386004   0.0         0.0\n  0.0         0.0577838   0.0\n  0.0        -0.0562827   0.0\n  0.0         0.0441522   0.0\n  0.0         0.0         0.00627948\n  0.0         0.0        -0.0293777\n  0.0         0.0        -0.0352914\n\nScaling with a tuple, providing lower and upper bound of the uniform distribution from which the weights will be sampled:\n\njulia> res_input = weighted_init(9, 3; scaling = (0.1, 0.5))\n9×3 Matrix{Float32}:\n 0.39048   0.0       0.0\n 0.230391  0.0       0.0\n 0.222799  0.0       0.0\n 0.0       0.415568  0.0\n 0.0       0.187435  0.0\n 0.0       0.388304  0.0\n 0.0       0.0       0.312559\n 0.0       0.0       0.241245\n 0.0       0.0       0.229417\n\nScaling with a vector of scalars, where each provides the upper bound and its negative provides the lower bound. Each column is scaled in order: first element provides bounds for the first column, and so on:\n\njulia> res_input = weighted_init(9, 3; scaling = [0.1, 0.5, 0.9])\n9×3 Matrix{Float32}:\n  0.0452399   0.0        0.0\n -0.0348047   0.0        0.0\n -0.0386004   0.0        0.0\n  0.0         0.288919   0.0\n  0.0        -0.281413   0.0\n  0.0         0.220761   0.0\n  0.0         0.0        0.0565153\n  0.0         0.0       -0.264399\n  0.0         0.0       -0.317622\n\nScaling with a vector of tuples, each providing both upper and lower bound. Each column is scaled in order: first element provides bounds for the first column, and so on:\n\njulia> res_input = weighted_init(9, 3; scaling = [(0.1, 0.2), (-0.2, -0.1), (0.3, 0.5)])\n9×3 Matrix{Float32}:\n 0.17262    0.0       0.0\n 0.132598   0.0       0.0\n 0.1307     0.0       0.0\n 0.0       -0.121108  0.0\n 0.0       -0.178141  0.0\n 0.0       -0.127924  0.0\n 0.0        0.0       0.40628\n 0.0        0.0       0.370622\n 0.0        0.0       0.364709\n\nExample of matrix size change:\n\njulia> res_input = weighted_init(8, 3)\n┌ Warning: Reservoir size has changed!\n│\n│     Computed reservoir size (6) does not equal the provided reservoir size (8).\n│\n│     Using computed value (6). Make sure to modify the reservoir initializer accordingly.\n│\n└ @ ReservoirComputing ~/.julia/dev/ReservoirComputing/src/inits/inits_components.jl:20\n6×3 Matrix{Float32}:\n  0.0452399   0.0          0.0\n -0.0348047   0.0          0.0\n  0.0        -0.0386004    0.0\n  0.0         0.00981022   0.0\n  0.0         0.0          0.0577838\n  0.0         0.0         -0.0562827\n\nReturn sparse:\n\njulia> using SparseArrays\n\njulia> res_input = weighted_init(9, 3; return_sparse = true)\n9×3 SparseMatrixCSC{Float32, Int64} with 9 stored entries:\n  0.0452399    ⋅           ⋅\n -0.0348047    ⋅           ⋅\n -0.0386004    ⋅           ⋅\n   ⋅          0.0577838    ⋅\n   ⋅         -0.0562827    ⋅\n   ⋅          0.0441522    ⋅\n   ⋅           ⋅          0.00627948\n   ⋅           ⋅         -0.0293777\n   ⋅           ⋅         -0.0352914\n\n\n\n\n\n","category":"function"},{"location":"api/inits/double_cycle/#double_cycle","page":"double_cycle","title":"double_cycle","text":"","category":"section"},{"location":"api/inits/double_cycle/#References","page":"double_cycle","title":"References","text":"Fu, J.; Li, G.; Tang, J.; Xia, L.; Wang, L. and Duan, S. (2023). A double-cycle echo state network topology for time series prediction. Chaos: An Interdisciplinary Journal of Nonlinear Science 33.\n\n\n\n","category":"section"},{"location":"api/inits/double_cycle/#ReservoirComputing.double_cycle","page":"double_cycle","title":"ReservoirComputing.double_cycle","text":"double_cycle([rng], [T], dims...;\n    cycle_weight=0.1, second_cycle_weight=0.1,\n    radius=nothing, return_sparse=false)\n\nCreates a double cycle reservoir (Fu et al., 2023).\n\nW_ij =\nbegincases\n    r_1  textif  i = j + 1 j in 1 D_mathrmres - 1 4pt\n    r_1  textif  i = D_mathrmres j = 1 6pt\n    r_2  textif  i = 1 j = D_mathrmres 6pt\n    r_2  textif  j = i + 1 i in 1 D_mathrmres - 1 4pt\n    0    textotherwise\nendcases\n\nArguments\n\nrng: Random number generator. Default is Utils.default_rng()from WeightInitializers.\nT: Type of the elements in the reservoir matrix. Default is Float32.\ndims: Dimensions of the reservoir matrix.\n\nKeyword arguments\n\ncycle_weight: Weight of the upper cycle connections in the reservoir matrix. Default is 0.1.\nsecond_cycle_weight: Weight of the lower cycle connections in the reservoir matrix. Default is 0.1.\nradius: The desired spectral radius of the reservoir. If nothing is passed, no scaling takes place. Defaults to nothing.\nreturn_sparse: flag for returning a sparse matrix. true requires SparseArrays to be loaded. Default is false.\n\nExamples\n\nDefault call:\n\njulia> res_matrix = double_cycle(5, 5)\n5×5 Matrix{Float32}:\n 0.0  0.1  0.0  0.0  0.1\n 0.1  0.0  0.1  0.0  0.0\n 0.0  0.1  0.0  0.1  0.0\n 0.0  0.0  0.1  0.0  0.1\n 0.1  0.0  0.0  0.1  0.0\n\nChanging weights:\n\njulia> res_matrix = double_cycle(5, 5; cycle_weight = -0.1, second_cycle_weight = 0.3)\n5×5 Matrix{Float32}:\n  0.0   0.3   0.0   0.0  0.3\n -0.1   0.0   0.3   0.0  0.0\n  0.0  -0.1   0.0   0.3  0.0\n  0.0   0.0  -0.1   0.0  0.3\n -0.1   0.0   0.0  -0.1  0.0\n\n\n\n\n\n","category":"function"},{"location":"api/inits/reverse_simple_cycle!/#reverse*simple*cycle!","page":"reversesimplecycle!","title":"reversesimplecycle!","text":"","category":"section"},{"location":"api/inits/reverse_simple_cycle!/#ReservoirComputing.reverse_simple_cycle!","page":"reversesimplecycle!","title":"ReservoirComputing.reverse_simple_cycle!","text":"reverse_simple_cycle!([rng], reservoir_matrix, weight;\n    sampling_type=:no_sample, irrational=pi, start=1,\n    p=0.5)\n\nAdds a reverse simple cycle in the reservoir_matrix, with given weight. The weight can be a single number or an array.\n\nArguments\n\nrng: Random number generator. Default is Utils.default_rng() from WeightInitializers.\nreservoir_matrix: matrix to be changed.\nweight: weight to add as a simple cycle. Can be either a single number or an array.\n\nKeyword arguments\n\nsampling_type: Sampling that decides the distribution of weight negative numbers. If set to :no_sample the sign is unchanged. If set to :bernoulli_sample! then each weight can be positive with a probability set by positive_prob. If set to :irrational_sample! the weight is negative if the decimal number of the irrational number chosen is odd. If set to :regular_sample!, each weight will be assigned a negative sign after the chosen strides. strides can be a single number or an array. Default is :no_sample.\npositive_prob: probability of the weight being positive when sampling_type is set to :bernoulli_sample!. Default is 0.5.\nirrational: Irrational number whose decimals decide the sign of weight. Default is pi.\nstart: Which place after the decimal point the counting starts for the irrational sign counting. Default is 1.\nstrides: number of strides for assigning negative value to a weight. It can be an integer or an array. Default is 2.\n\nExamples\n\njulia> matrix = zeros(Float32, 5, 5)\n5×5 Matrix{Float32}:\n 0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n\njulia> reverse_simple_cycle!(matrix, 1.0; sampling_type = :regular_sample!)\n5×5 Matrix{Float32}:\n 0.0  -1.0  0.0   0.0  0.0\n 0.0   0.0  1.0   0.0  0.0\n 0.0   0.0  0.0  -1.0  0.0\n 0.0   0.0  0.0   0.0  1.0\n 1.0   0.0  0.0   0.0  0.0\n\n\n\n\n\n","category":"function"},{"location":"api/train/#Train","page":"Train","title":"Train","text":"","category":"section"},{"location":"api/train/#Training-methods","page":"Train","title":"Training methods","text":"","category":"section"},{"location":"api/train/#ReservoirComputing.train!","page":"Train","title":"ReservoirComputing.train!","text":"train!(rc, train_data, target_data, ps, st,\n       train_method=StandardRidge(0.0);\n       washout=0, return_states=false)\n\nTrains a given reservoir computing by creating the reservoir states from train_data, and then fiting the readout layer using target_data as target. The learned weights/layer are written into ps.\n\nArguments\n\nrc: A reservoir computing model, either provided by ReservoirComputing.jl or built with ReservoirChain. Must contain a trainable layer (for example LinearReadout), and a collection point Collect.\ntrain_data: input sequence where columns are time steps.\ntarget_data: targets aligned with train_data.\nps: model parameters.\nst: model states.\ntrain_method: training algorithm. Default is StandardRidge.\n\nKeyword arguments\n\nwashout: number of initial time steps to discard (applied equally to features and targets). Default 0.\nreturn_states: if true, also returns the feature matrix used for the fit.\nkwargs...: additional keyword arguments for the training algorithm, if needed. Defaults vary according to the different training method.\n\nReturns\n\n(ps, st): updated model parameters and states.\n(ps, st), states: If return_states=true.\n\nNotes\n\nFeatures are produced by collectstates(rc, train_data, ps, st). If you rely on the implicit collection of a LinearReadout, make sure that readout was created with include_collect=true, or insert an explicit Collect() earlier in the ReservoirChain.\n\n\n\n\n\n","category":"function"},{"location":"api/train/#ReservoirComputing.train","page":"Train","title":"ReservoirComputing.train","text":"train(train_method, states, target_data; kwargs...)\n\nLower level training hook to fit a readout from precomputed reservoir features and given targets.\n\nDispatching on this method with different training methods allows one to hook directly into train! without additional changes.\n\nArguments\n\ntrain_method: An object describing the training algorithm and its hyperparameters (e.g. regularization strength, solver choice, constraints).\nstates: Feature matrix with reservoir states (ie. obtained with collectstates). Shape (n_features, T), where T is the number of samples (e.g. time steps).\ntarget_data: Target matrix aligned with states. Shape (n_outputs, T).\n\nReturns\n\noutput_weights: Trained readout. Should be a forward method to be hooked into a layer. For instance, in case of linear regression output_weights is a mtrix consumable by LinearReadout.\n\nNotes\n\nAny sequence pre-processing (e.g. washout) should be handled by the caller before invoking train. See train! for an end-to-end workflow.\nFor very long T, consider chunked or iterative solvers to reduce memory usage.\nIf your approach returns additional artifacts (e.g. diagnostics), prefer storing them inside train_method or exposing a separate API; keep train’s return value as the forward method only.\n\n\n\n\n\n","category":"function"},{"location":"api/train/#ReservoirComputing.StandardRidge","page":"Train","title":"ReservoirComputing.StandardRidge","text":"StandardRidge([Type], [reg])\n\nRidge regression method.\n\nEquations\n\nmathbfw = (mathbfX^top mathbfX +\nlambda mathbfI)^-1 mathbfX^top mathbfy\n\nArguments\n\nType: type of the regularization argument. Default is inferred internally, there's usually no need to tweak this\nreg: regularization coefficient. Default is set to 0.0 (linear regression).\n\n\n\n\n\n","category":"type"},{"location":"api/inits/selfloop_forwardconnection/#selfloop*forward*connection","page":"selfloopforwardconnection","title":"selfloopforwardconnection","text":"","category":"section"},{"location":"api/inits/selfloop_forwardconnection/#References","page":"selfloopforwardconnection","title":"References","text":"Elsarraj, D.; Qisi, M. A.; Rodan, A.; Obeid, N.; Sharieh, A. and Faris, H. (2019). Demystifying echo state network with deterministic simple topologies. International Journal of Computational Science and Engineering 19, 407–417.\n\n\n\n","category":"section"},{"location":"api/inits/selfloop_forwardconnection/#ReservoirComputing.selfloop_forwardconnection","page":"selfloopforwardconnection","title":"ReservoirComputing.selfloop_forwardconnection","text":"selfloop_forwardconnection([rng], [T], dims...;\n    delay_weight=0.1, selfloop_weight=0.1,\n    radius=nothing, return_sparse=false,\n    selfloop_kwargs=(), delay_kwargs=())\n\nCreates a reservoir based on a forward connection of weights between even nodes with the addition of self loops (Elsarraj et al., 2019).\n\nThis architecture is referred to as TP4 in the original paper.\n\nW_ij =\nbegincases\n    ll  textif  i = j text for  i = 1 dots N \n    r  textif  j = i - 2 text for  i = 3 dots N \n    0  textotherwise\nendcases\n\nArguments\n\nrng: Random number generator. Default is Utils.default_rng()from WeightInitializers.\nT: Type of the elements in the reservoir matrix. Default is Float32.\ndims: Dimensions of the reservoir matrix.\n\nKeyword arguments\n\nforward_weight: Weight of the forward connections in the reservoir matrix. This can be provided as a single value or an array. In case it is provided as an array please make sure that the length of the array matches the length of the cycle you want to populate. Default is 0.1.\nselfloop_weight: Weight of the self loops in the reservoir matrix. This can be provided as a single value or an array. In case it is provided as an array please make sure that the length of the array matches the length of the diagonal you want to populate. Default is 0.1.\nradius: The desired spectral radius of the reservoir. If nothing is passed, no scaling takes place. Defaults to nothing.\nreturn_sparse: flag for returning a sparse matrix. true requires SparseArrays to be loaded. Default is false.\ndelay_kwargs and selfloop_kwargs: named tuples that control the kwargs for the delay line weight and self loop weights respectively. The kwargs are as follows:\nsampling_type: Sampling that decides the distribution of weight negative numbers.   If set to :no_sample the sign is unchanged. If set to :bernoulli_sample! then each   weight can be positive with a probability set by positive_prob. If set to   :irrational_sample! the weight is negative if the decimal number of the   irrational number chosen is odd. If set to :regular_sample!, each weight will be   assigned a negative sign after the chosen strides. strides can be a single   number or an array. Default is :no_sample.\npositive_prob: probability of the weight being positive when sampling_type is set to :bernoulli_sample!. Default is 0.5.\nirrational: Irrational number whose decimals decide the sign of weight. Default is pi.\nstart: Which place after the decimal point the counting starts for the irrational sign counting. Default is 1.\nstrides: number of strides for assigning negative value to a weight. It can be an integer or an array. Default is 2.\n\nExamples\n\nDefault call:\n\njulia> res_matrix = selfloop_forwardconnection(5, 5)\n5×5 Matrix{Float32}:\n 0.1  0.0  0.0  0.0  0.0\n 0.0  0.1  0.0  0.0  0.0\n 0.1  0.0  0.1  0.0  0.0\n 0.0  0.1  0.0  0.1  0.0\n 0.0  0.0  0.1  0.0  0.1\n\nChanging weights:\n\njulia> res_matrix = selfloop_forwardconnection(5, 5; forward_weight=0.5, selfloop_weight=0.99)\n5×5 Matrix{Float32}:\n 0.99  0.0   0.0   0.0   0.0\n 0.0   0.99  0.0   0.0   0.0\n 0.5   0.0   0.99  0.0   0.0\n 0.0   0.5   0.0   0.99  0.0\n 0.0   0.0   0.5   0.0   0.99\n\nChanging weights to custom arrays:\n\njulia> res_matrix = selfloop_forwardconnection(5, 5; forward_weight=rand(5), selfloop_weight=.-rand(5))\n5×5 Matrix{Float32}:\n -0.0420509   0.0        0.0        0.0        0.0\n  0.0        -0.116113   0.0        0.0        0.0\n  0.69173     0.0       -0.513592   0.0        0.0\n  0.0         0.522245   0.0       -0.199966   0.0\n  0.0         0.0        0.784556   0.0       -0.918653\n\njulia> res_matrix = selfloop_forwardconnection(5, 5; delay_kwargs=(;sampling_type=:irrational_sample!))\n5×5 Matrix{Float32}:\n  0.1  0.0   0.0  0.0  0.0\n  0.0  0.1   0.0  0.0  0.0\n -0.1  0.0   0.1  0.0  0.0\n  0.0  0.1   0.0  0.1  0.0\n  0.0  0.0  -0.1  0.0  0.1\n\njulia> res_matrix = selfloop_forwardconnection(5, 5; selfloop_kwargs=(;sampling_type=:bernoulli_sample!))\n5×5 Matrix{Float32}:\n 0.1   0.0  0.0   0.0  0.0\n 0.0  -0.1  0.0   0.0  0.0\n 0.1   0.0  0.1   0.0  0.0\n 0.0   0.1  0.0  -0.1  0.0\n 0.0   0.0  0.1   0.0  0.1\n\nReturning as sparse:\n\njulia> using SparseArrays\n\njulia> res_matrix = selfloop_forwardconnection(5, 5; return_sparse=true)\n5×5 SparseMatrixCSC{Float32, Int64} with 8 stored entries:\n 0.1   ⋅    ⋅    ⋅    ⋅\n  ⋅   0.1   ⋅    ⋅    ⋅\n 0.1   ⋅   0.1   ⋅    ⋅\n  ⋅   0.1   ⋅   0.1   ⋅\n  ⋅    ⋅   0.1   ⋅   0.1\n\n\n\n\n\n","category":"function"},{"location":"api/layers/#Layers","page":"Layers","title":"Layers","text":"","category":"section"},{"location":"api/layers/#Base-Layers","page":"Layers","title":"Base Layers","text":"","category":"section"},{"location":"api/layers/#Readout-Layers","page":"Layers","title":"Readout Layers","text":"","category":"section"},{"location":"api/layers/#Echo-State-Networks","page":"Layers","title":"Echo State Networks","text":"","category":"section"},{"location":"api/layers/#Reservoir-computing-with-cellular-automata","page":"Layers","title":"Reservoir computing with cellular automata","text":"","category":"section"},{"location":"api/layers/#ReservoirComputing.ReservoirComputer","page":"Layers","title":"ReservoirComputing.ReservoirComputer","text":"ReservoirComputer(reservoir, states_modifiers, readout)\n\nGeneric reservoir-computing container that wires together:\n\na reservoir (any Lux-compatible layer producing features),\nzero or more states_modifiers applied sequentially to the reservoir features,\na readout layer (typically LinearReadout).\n\nThe container exposes a standard (x, ps, st) -> (y, st′) interface and utility functions to initialize parameters/states, stream sequences to collect features, and install trained readout weights.\n\nArguments\n\nreservoir: a layer that consumes inputs and produces feature vectors.\nstates_modifiers: a tuple (or vector converted to Tuple) of layers applied after the reservoir (may be empty).\nreadout: the final trainable layer mapping features to outputs.\n\nInputs\n\nx: input to the reservoir (shape determined by the reservoir).\nps: reservoir computing parameters.\nst: reservoir computing states.\n\nReturns\n\n(y, st′) where y is the readout output and st′ contains the updated states of the reservoir, modifiers, and readout.\n\n\n\n\n\n","category":"type"},{"location":"api/layers/#ReservoirComputing.ReservoirChain","page":"Layers","title":"ReservoirComputing.ReservoirChain","text":"ReservoirChain(layers...; name=nothing)\nReservoirChain(xs::AbstractVector; name=nothing)\nReservoirChain(nt::NamedTuple; name=nothing)\nReservoirChain(; name=nothing, kwargs...)\n\nA lightweight, Lux-compatible container that composes a sequence of layers and executes them in order. The implementation of ReservoirChain is equivalent to Lux's own Chain.\n\nConstruction\n\nYou can build a chain from:\n\nPositional layers: ReservoirChain(l1, l2, ...)\nA vector of layers: ReservoirChain([l1, l2, ...])\nA named tuple of layers: ReservoirChain((; layer_a=l1, layer_b=l2))\nKeywords (sugar for a named tuple): ReservoirChain(; layer_a=l1, layer_b=l2)\n\nIn all cases, function objects are automatically wrapped via WrappedFunction so they can participate like regular layers. If a LinearReadout with include_collect=true is present, the chain automatically inserts a Collect layer immediately before that readout.\n\nUse name to optionally tag the chain instance.\n\nInputs\n\n(x, ps, st) where:\n\nx: input to the first layer.\nps: parameters as a named tuple with the same fields and order as the chain's layers.\nst: states as a named tuple with the same fields and order as the chain's layers.\n\nThe call (c::ReservoirChain)(x, ps, st) forwards x through each layer: (x, ps_i, st_i) -> (x_next, st_i′) and returns the final output and the updated states for every layer.\n\nReturns\n\n(y, st′) where y is the output of the last layer and st′ is a named tuple collecting the updated states for each layer.\n\nParameters\n\nA NamedTuple whose fields correspond 1:1 with the layers. Each field holds the parameters for that layer.\nField names are generated as :layer_1, :layer_2, ... when constructed positionally, or preserved when you pass a NamedTuple/keyword constructor.\n\nStates\n\nA NamedTuple whose fields correspond 1:1 with the layers. Each field holds the state for that layer.\n\nLayer access & indexing\n\nc[i]: get the i-th layer (1-based).\nc[indices]: return a new ReservoirChain formed by selecting a subset of layers.\ngetproperty(c, :layer_k): access layer k by its generated/explicit name.\nlength(c), firstindex(c), lastindex(c): standard collection interfaces.\n\nNotes\n\nFunction wrapping: Any plain Function in the constructor is wrapped as WrappedFunction(f). Non-layer, non-function objects will error.\nAuto-collect for readouts: When a LinearReadout has include_collect=true, the constructor expands it to (Collect(), readout) so that downstream tooling can capture features consistently.\n\n\n\n\n\n","category":"type"},{"location":"api/layers/#ReservoirComputing.Collect","page":"Layers","title":"ReservoirComputing.Collect","text":"Collect()\n\nMarker layer that passes data through unchanged but marks a feature checkpoint for collectstates. At each time step, whenever a Collect is encountered in the chain, the current vector is recorded as part of the feature vector used to train the readout. If multiple Collect layers exist, their vectors are concatenated with vcat in order of appearance.\n\nArguments\n\nNone.\n\nKeyword arguments\n\nNone.\n\nInputs\n\nx :: AbstractArray (d, batch) — the current tensor flowing through the chain.\n\nReturns\n\n(x, st) — the same tensor x and the unchanged state st.\n\nParameters\n\nNone.\n\nStates\n\nNone.\n\nNotes\n\nWhen used with a single Collect before a LinearReadout, training uses exactly the tensor right before the readout (e.g., the reservoir state).\nWith multiple Collect layers (e.g., after different submodules), the per-step features are vcat-ed in chain order to form one feature vector.\nIf the readout is constructed with include_collect=true, an implicit collection point is assumed immediately before the readout. Use an explicit Collect only when you want to control where/what is collected (or to stack multiple features).\n\nrc = ReservoirChain(\n    StatefulLayer(ESNCell(3 => 300)),\n    NLAT2(),\n    Collect(), # <-- collect the 300-dim reservoir after NLAT2\n    LinearReadout(300 => 3; include_collect=false) # <-- toggle off the default Collect()\n)\n\n\n\n\n\n","category":"type"},{"location":"api/layers/#ReservoirComputing.StatefulLayer","page":"Layers","title":"ReservoirComputing.StatefulLayer","text":"StatefulLayer(cell::AbstractReservoirRecurrentCell)\n\nA lightweight wrapper that makes a recurrent cell carry its input state to the next step.\n\nArguments\n\ncell: AbstractReservoirRecurrentCell (e.g. ESNCell).\n\nStates\n\ncell: internal states for the wrapped cell (e.g., RNG replicas, etc.).\ncarry: the per-sequence hidden state; initialized to nothing.\n\n\n\n\n\n","category":"type"},{"location":"api/layers/#ReservoirComputing.DelayLayer","page":"Layers","title":"ReservoirComputing.DelayLayer","text":"DelayLayer(input_dim; num_delays=2, stride=1)\n\nStateful delay layer that augments the current vector with a fixed number of time-delayed copies of itself. Intended to be used as a state_modifier in a ReservoirComputer, for example to build NVAR-style feature vectors.\n\nAt each call, the layer:\n\nTakes the current input vector h(t) of length input_dim.\nProduces an output vector that concatenates:\n\nthe current input h(t), and\nnum_delays previous inputs stored in an internal buffer.\n\nUpdates its internal delay buffer with h(t) every stride calls.\n\nNewly initialized buffers are filled with zeros, so at the beginning of a sequence, delayed entries correspond to zero padding.\n\nArguments\n\ninput_dim: Dimension of the input/state vector at each time step.\n\nKeyword arguments\n\nnum_delays: Number of delayed copies to keep. The output will have (num_delays + 1) * input_dim entries: the current vector plus num_delays past vectors. Default is 2.\nstride: Delay stride in layer calls. The internal buffer is updated only when clock % stride == 0. Default is 1.\ninit_delay: Initializer(s) for the delays. Must be either a single function (e.g. zeros32, randn32) or an NTuple of num_delays functions, e.g. (zeros32, randn32). If a single function fn is provided, it is automatically expanded into a num_delays-element tuple (fn, fn, ..., fn). Default is zeros32.\n\nInputs\n\nh(t) :: AbstractVector (input_dim,)\n\nTypically the current reservoir state at time t.\n\nReturns\n\nz :: AbstractVector ((num_delays + 1) * input_dim,)\n\nConcatenation of the current input and its delayed copies.\n\nParameters\n\nNone\n\nStates\n\nhistory: a matrix whose column j holds the j-th most recent stored input. On a stride update, the columns are shifted and the current input is placed in column 1.\nclock: A counter that updates each call of the layer. The delay buffer is updated when clock % stride == 0.\n\n\n\n\n\n","category":"type"},{"location":"api/layers/#ReservoirComputing.NonlinearFeaturesLayer","page":"Layers","title":"ReservoirComputing.NonlinearFeaturesLayer","text":"NonlinearFeaturesLayer(features...; include_input=true)\n\nLayer that builds a feature vector by applying one or more user-defined functions to a single input vector and concatenating the results. Intended to be used as a state_modifier (for example, after a DelayLayer) to construct NGRC/NVAR-style feature maps.\n\nAt each call, for an input vector x, the layer:\n\nOptionally includes x itself (if include_input=true).\nApplies each function in features to x.\nReturns the vertical concatenation of all results.\n\nArguments\n\nfeatures...: One or more functions f(x) that map a vector to a vector. Each function is called as f(inp) and must return an AbstractVector.\n\nKeyword arguments\n\ninclude_input: If true (default), the original input vector inp is included as the first block in the feature vector. If false, the output contains only the concatenation of features(inp).\n\nInputs\n\ninp :: AbstractVector The current feature vector, typically the output of a DelayLayer or a reservoir state.\n\nReturns\n\nout :: AbstractVector Concatenation of:\nthe original input inp (if include_input=true), and\nthe outputs of each function in features applied to inp.\nThe unchanged state st (this layer is stateless).\n\nParameters\n\nNone. NonlinearFeaturesLayer has no trainable parameters.\n\nStates\n\nNone. initialstates returns an empty NamedTuple.\n\n\n\n\n\n","category":"type"},{"location":"api/layers/#ReservoirComputing.LinearReadout","page":"Layers","title":"ReservoirComputing.LinearReadout","text":"LinearReadout(in_dims => out_dims, [activation];\n        use_bias=false, include_collect=true)\n\nLinear readout layer with optional bias and elementwise activation. Intended as the final, trainable mapping from collected features (e.g., reservoir state) to outputs. When include_collect=true, training will collect features immediately before this layer (logically inserting a Collect right before it).\n\nEquation\n\nmathbfy = psileft(mathbfWmathbfz + mathbfbright)\n\nArguments\n\nin_dims: Input/feature dimension (e.g., reservoir size).\nout_dims: Output dimension (e.g., number of targets).\nactivation: Elementwise output nonlinearity. Default: identity.\n\nKeyword arguments\n\nuse_bias: Include an additive bias vector b. Default: false.\ninclude_collect: If true (default), training collects features immediately before this layer (as if a Collect were inserted right before it).\n\nParameters\n\nweight :: (out_dims × in_dims)\nbias   :: (out_dims,) — present only if use_bias=true\n\nStates\n\nNone.\n\nNotes\n\nIn ESN workflows, readout weights are typically replaced via ridge regression in train!. Therefore, how LinearReadout gets initialized is of no consequence. Additionally, the dimensions will also not be taken into account, as train! will replace the weights.\nIf you set include_collect=false, make sure a Collect appears earlier in the chain. Otherwise training may operate on the post-readout signal, which is usually unintended.\n\n\n\n\n\n","category":"type"},{"location":"api/layers/#ReservoirComputing.SVMReadout","page":"Layers","title":"ReservoirComputing.SVMReadout","text":"SVMReadout(in_dims => out_dims;\n    include_collect=true, kwargs...)\n\nReadout layer based on support vector machines. Requires LIBSVM.jl.\n\nArguments\n\nin_dims: Input/feature dimension (e.g., reservoir size).\nout_dims: Output dimension (e.g., number of targets).\n\nKeyword arguments\n\ninclude_collect: If true (default), training collects features immediately before this layer (as if a Collect() were inserted right before it).\nkwags: specific keyword arguments for LIBSVM elements.\n\n\n\n\n\n","category":"type"},{"location":"api/layers/#ReservoirComputing.ESNCell","page":"Layers","title":"ReservoirComputing.ESNCell","text":"ESNCell(in_dims => out_dims, [activation];\n    use_bias=false, init_bias=rand32,\n    init_reservoir=rand_sparse, init_input=scaled_rand,\n    init_state=randn32, leak_coefficient=1.0)\n\nEcho State Network (ESN) recurrent cell with optional leaky integration.\n\nEquations\n\nbeginaligned\n    tildemathbfh(t) = phileft(mathbfW_inmathbfx(t) +\n        mathbfW_resmathbfh(t-1) + mathbfbright) \n    mathbfh(t) = (1-alpha)mathbfh(t-1) + alphatildemathbfh(t)\nendaligned\n\nArguments\n\nin_dims: Input dimension.\nout_dims: Reservoir (hidden state) dimension.\nactivation: Activation function. Default: tanh.\n\nKeyword arguments\n\nuse_bias: Whether to include a bias term. Default: false.\ninit_bias: Initializer for the bias. Used only if use_bias=true.   Default is rand32.\ninit_reservoir: Initializer for the reservoir matrix W_res. Default is rand_sparse.\ninit_input: Initializer for the input matrix W_in. Default is scaled_rand.\ninit_state: Initializer for the hidden state when an external state is not provided. Default is randn32.\nleak_coefficient: Leak rate α ∈ (0,1]. Default: 1.0.\n\nInputs\n\nCase 1: x :: AbstractArray (in_dims, batch) A fresh state is created via init_state; the call is forwarded to Case 2.\nCase 2: (x, (h,)) where h :: AbstractArray (out_dims, batch) Computes the update and returns the new state.\n\nIn both cases, the forward returns ((h_new, (h_new,)), st_out) where st_out contains any updated internal state.\n\nReturns\n\nOutput/hidden state h_new :: out_dims and state tuple (h_new,).\nUpdated layer state (NamedTuple).\n\nParameters\n\nCreated by initialparameters(rng, esn):\n\ninput_matrix :: (out_dims × in_dims) — W_in\nreservoir_matrix :: (out_dims × out_dims) — W_res\nbias :: (out_dims,) — present only if use_bias=true\n\nStates\n\nCreated by initialstates(rng, esn):\n\nrng: a replicated RNG used to sample initial hidden states when needed.\n\n\n\n\n\n","category":"type"},{"location":"api/layers/#ReservoirComputing.ES2NCell","page":"Layers","title":"ReservoirComputing.ES2NCell","text":"ES2NCell(in_dims => out_dims, [activation];\n    use_bias=False(), init_bias=zeros32,\n    init_reservoir=rand_sparse, init_input=scaled_rand,\n    init_state=randn32, init_orthogonal=orthogonal,\n    proximity=1.0))\n\nEdge of Stability Echo State Network (ES2N) cell (Ceni and Gallicchio, 2025).\n\nEquations\n\nbeginaligned\nx(t) = beta phileft( rho mathbfW_r x(t-1) + omega\n    mathbfW_in u(t) right) + (1-beta) mathbfO x(t-1)\nendaligned\n\nArguments\n\n- `in_dims`: Input dimension.\n- `out_dims`: Reservoir (hidden state) dimension.\n- `activation`: Activation function. Default: `tanh`.\n\nKeyword arguments\n\n- `use_bias`: Whether to include a bias term. Default: `false`.\n- `init_bias`: Initializer for the bias. Used only if `use_bias=true`.\n  Default is `rand32`.\n- `init_reservoir`: Initializer for the reservoir matrix `W_res`.\n  Default is [`rand_sparse`](@ref).\n- `init_orthogonal`: Initializer for the orthogonal matrix `O`.\n  Default is [`orthogonal`](@ref).\n- `init_input`: Initializer for the input matrix `W_in`.\n  Default is [`scaled_rand`](@ref).\n- `init_state`: Initializer for the hidden state when an external\n  state is not provided. Default is `randn32`.\n- `proximity`: Proximity coefficient `α ∈ (0,1]`. Default: `1.0`.\n\nInputs\n\n- **Case 1:** `x :: AbstractArray (in_dims, batch)`\nA fresh state is created via `init_state`; the call is forwarded to Case 2.\n- **Case 2:** `(x, (h,))` where `h :: AbstractArray (out_dims, batch)`\nComputes the update and returns the new state.\n\nIn both cases, the forward returns ((h_new, (h_new,)), st_out) where st_out contains any updated internal state.\n\nReturns\n\n- Output/hidden state `h_new :: out_dims` and state tuple `(h_new,)`.\n- Updated layer state (NamedTuple).\n\nParameters\n\n- `input_matrix :: (out_dims × in_dims)` — `W_in`\n- `reservoir_matrix :: (out_dims × out_dims)` — `W_res`\n- `orthogonal_matrix :: (res_dims × res_dims)` — `O`\n- `bias :: (out_dims,)` — present only if `use_bias=true`\n\nStates\n\nCreated by initialstates(rng, esn):\n\n- `rng`: a replicated RNG used to sample initial hidden states when needed.\n\n\n\n\n\n","category":"type"},{"location":"api/layers/#ReservoirComputing.RECACell","page":"Layers","title":"ReservoirComputing.RECACell","text":"RECACell(automaton, enc::RandomMaps)\n\nCellular Automata (CA)–based reservoir recurrent cell. At each time step, the input vector is randomly embedded into a CA configuration, the CA is evolved for a fixed number of generations, and the flattened CA evolution is emitted as the reservoir state. The last CA configuration is carried to the next step. For more details please refer to (Nichele and Molund, 2017), and (Yilmaz, 2014).\n\nArguments\n\nautomaton: A cellular automaton rule/object from CellularAutomata.jl (e.g., DCA(90), DCA(30), …).\nenc: Precomputed random-mapping/encoding metadata given as a RandomMapping.\n\nInputs\n\nCase A: a single input vector x with length in_dims. The cell internally uses the stored CA state (st.ca) as the previous configuration.\nCase B: a tuple (x, (ca,)) where x is as above and ca has length enc.ca_size.\n\nComputation\n\nRandom embedding of x into a CA initial condition c₀ using enc.maps across enc.permutations blocks of length enc.expansion_size.\nCA evolution for G = enc.generations steps with the given automaton, producing an evolution matrix E ∈ ℝ^{(G+1) × ca_size} where E[1,:] = c₀ and E[t+1,:] = F(E[t,:]).\nFeature vector is the flattened stack of E[2:end, :] (dropping the initial row), shaped as a column vector of length enc.states_size.\nCarry is the final CA configuration E[end, :].\n\nReturns\n\nOutput: (h, (caₙ,)) where\nh has length enc.states_size (the CA features),\ncaₙ has length enc.ca_size (next carry).\nUpdated (unchanged) cell state (parameters-free layer state).\n\nParameters & State\n\nParameters: none\nState: (ca = zeros(Float32, enc.ca_size))\n\n\n\n\n\n","category":"type"},{"location":"api/inits/minimal_init/#minimal_init","page":"minimal_init","title":"minimal_init","text":"","category":"section"},{"location":"api/inits/minimal_init/#References","page":"minimal_init","title":"References","text":"Rodan, A. and Tino, P. (2011). Minimum Complexity Echo State Network. IEEE Transactions on Neural Networks 22, 131–144.\n\n\n\n","category":"section"},{"location":"api/inits/minimal_init/#ReservoirComputing.minimal_init","page":"minimal_init","title":"ReservoirComputing.minimal_init","text":"minimal_init([rng], [T], dims...;\n    sampling_type=:bernoulli_sample!, weight=0.1, irrational=pi,\n    start=1, p=0.5)\n\nCreate a dense matrix with same weights magnitudes determined by weight (Rodan and Tino, 2011). The sign difference is randomly determined by the sampling chosen.\n\nArguments\n\nrng: Random number generator. Default is Utils.default_rng() from WeightInitializers.\nT: Type of the elements in the reservoir matrix. Default is Float32.\ndims: Dimensions of the matrix. Should follow res_size x in_size.\n\nKeyword arguments\n\nweight: The weight used to fill the layer matrix. Default is 0.1.\nsampling_type: Sampling that decides the distribution of weight negative numbers. If set to :no_sample the sign is unchanged. If set to :bernoulli_sample! then each weight can be positive with a probability set by positive_prob. If set to :irrational_sample! the weight is negative if the decimal number of the irrational number chosen is odd. If set to :regular_sample!, each weight will be assigned a negative sign after the chosen strides. strides can be a single number or an array. Default is :no_sample.\npositive_prob: probability of the weight being positive when sampling_type is set to :bernoulli_sample!. Default is 0.5.\nirrational: Irrational number whose decimals decide the sign of weight. Default is pi.\nstart: Which place after the decimal point the counting starts for the irrational sign counting. Default is 1.\nstrides: number of strides for assigning negative value to a weight. It can be an integer or an array. Default is 2.\n\nExamples\n\nStandard call:\n\njulia> res_input = minimal_init(8, 3)\n8×3 Matrix{Float32}:\n  0.1  -0.1   0.1\n -0.1   0.1   0.1\n -0.1  -0.1   0.1\n -0.1  -0.1  -0.1\n  0.1   0.1   0.1\n -0.1  -0.1  -0.1\n -0.1  -0.1   0.1\n  0.1  -0.1   0.1\n\nSampling weight sign from an irrational number:\n\njulia> res_input = minimal_init(8, 3; sampling_type = :irrational)\n8×3 Matrix{Float32}:\n -0.1   0.1  -0.1\n  0.1  -0.1  -0.1\n  0.1   0.1  -0.1\n  0.1   0.1   0.1\n -0.1  -0.1  -0.1\n  0.1   0.1   0.1\n  0.1   0.1  -0.1\n -0.1   0.1  -0.1\n\nChanging probability for the negative sign\n\njulia> res_input = minimal_init(8, 3; p = 0.1) # lower p -> more negative signs\n8×3 Matrix{Float32}:\n-0.1  -0.1  -0.1\n-0.1  -0.1  -0.1\n-0.1  -0.1  -0.1\n-0.1  -0.1  -0.1\n 0.1  -0.1  -0.1\n-0.1  -0.1  -0.1\n-0.1  -0.1  -0.1\n-0.1  -0.1  -0.1\n\njulia> res_input = minimal_init(8, 3; p = 0.8)# higher p -> more positive signs\n8×3 Matrix{Float32}:\n 0.1   0.1  0.1\n-0.1   0.1  0.1\n-0.1   0.1  0.1\n 0.1   0.1  0.1\n 0.1   0.1  0.1\n 0.1  -0.1  0.1\n-0.1   0.1  0.1\n 0.1   0.1  0.1\n\n\n\n\n\n","category":"function"},{"location":"api/inits/chaotic_init/#chaotic_init","page":"chaotic_init","title":"chaotic_init","text":"","category":"section"},{"location":"api/inits/chaotic_init/#References","page":"chaotic_init","title":"References","text":"Xie, M.; Wang, Q. and Yu, S. (2024). Time Series Prediction of ESN Based on Chebyshev Mapping and Strongly Connected Topology. Neural Processing Letters 56.\n\n\n\n","category":"section"},{"location":"api/inits/chaotic_init/#ReservoirComputing.chaotic_init","page":"chaotic_init","title":"ReservoirComputing.chaotic_init","text":"chaotic_init([rng], [T], dims...;\n    extra_edge_probability=T(0.1), radius=one(T),\n    return_sparse=false)\n\nConstruct a chaotic reservoir matrix using a digital chaotic system (Xie et al., 2024).\n\nThe matrix topology is derived from a strongly connected adjacency matrix based on a digital chaotic system operating at finite precision. If the requested matrix order does not exactly match a valid order the closest valid order is used.\n\nArguments\n\nrng: Random number generator. Default is Utils.default_rng()from WeightInitializers.\nT: Type of the elements in the reservoir matrix. Default is Float32.\ndims: Dimensions of the reservoir matrix.\n\nKeyword arguments\n\nextra_edge_probability: Probability of adding extra random edges in the adjacency matrix to enhance connectivity. Default is 0.1.\nradius: The target spectral radius for the reservoir matrix. Default is one.\nreturn_sparse: flag for returning a sparse matrix. true requires SparseArrays to be loaded. Default is false.\n\nExamples\n\njulia> res_matrix = chaotic_init(8, 8)\n┌ Warning:\n│\n│     Adjusting reservoir matrix order:\n│         from 8 (requested) to 4\n│     based on computed bit precision = 1.\n│\n└ @ ReservoirComputing ~/.julia/dev/ReservoirComputing/src/esn/esn_inits.jl:805\n4×4 SparseArrays.SparseMatrixCSC{Float32, Int64} with 6 stored entries:\n   ⋅        -0.600945   ⋅          ⋅\n   ⋅          ⋅        0.132667   2.21354\n   ⋅        -2.60383    ⋅        -2.90391\n -0.578156    ⋅         ⋅          ⋅\n\n\n\n\n\n","category":"function"},{"location":"api/inits/forward_connection/#forward_connection","page":"forward_connection","title":"forward_connection","text":"","category":"section"},{"location":"api/inits/forward_connection/#References","page":"forward_connection","title":"References","text":"Elsarraj, D.; Qisi, M. A.; Rodan, A.; Obeid, N.; Sharieh, A. and Faris, H. (2019). Demystifying echo state network with deterministic simple topologies. International Journal of Computational Science and Engineering 19, 407–417.\n\n\n\n","category":"section"},{"location":"api/inits/forward_connection/#ReservoirComputing.forward_connection","page":"forward_connection","title":"ReservoirComputing.forward_connection","text":"forward_connection([rng], [T], dims...;\n    forward_weight=0.1, radius=nothing, return_sparse=false,\n    kwargs...)\n\nCreates a reservoir based on a forward connection of weights (Elsarraj et al., 2019).\n\nThis architecture is referred to as TP5 in the original paper.\n\nW_ij =\nbegincases\n    r  textif  j = i - 2 text for  i = 3 dots N \n    0  textotherwise\nendcases\n\nArguments\n\nrng: Random number generator. Default is Utils.default_rng()from WeightInitializers.\nT: Type of the elements in the reservoir matrix. Default is Float32.\ndims: Dimensions of the reservoir matrix.\n\nKeyword arguments\n\nforward_weight: Weight of the cycle connections in the reservoir matrix. This can be provided as a single value or an array. In case it is provided as an array please make sure that the length of the array matches the length of the sub-diagonal you want to populate. Default is 0.1.\nradius: The desired spectral radius of the reservoir. If nothing is passed, no scaling takes place. Defaults to nothing.\nreturn_sparse: flag for returning a sparse matrix. true requires SparseArrays to be loaded. Default is false.\nsampling_type: Sampling that decides the distribution of forward_weight negative numbers. If set to :no_sample the sign is unchanged. If set to :bernoulli_sample! then each forward_weight can be positive with a probability set by positive_prob. If set to :irrational_sample! the forward_weight is negative if the decimal number of the irrational number chosen is odd. If set to :regular_sample!, each weight will be assigned a negative sign after the chosen strides. strides can be a single number or an array. Default is :no_sample.\npositive_prob: probability of the forward_weight being positive when sampling_type is set to :bernoulli_sample!. Default is 0.5.\nirrational: Irrational number whose decimals decide the sign of forward_weight. Default is pi.\nstart: Which place after the decimal point the counting starts for the irrational sign counting. Default is 1.\nstrides: number of strides for assigning negative value to a weight. It can be an integer or an array. Default is 2.\n\nExamples\n\nDefault kwargs:\n\njulia> reservoir_matrix = forward_connection(5, 5)\n5×5 Matrix{Float32}:\n 0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n 0.1  0.0  0.0  0.0  0.0\n 0.0  0.1  0.0  0.0  0.0\n 0.0  0.0  0.1  0.0  0.0\n\nChanging the weights magnitudes to a different unique value:\n\njulia> forward_connection(5, 5; forward_weight=0.99)\n5×5 Matrix{Float32}:\n 0.0   0.0   0.0   0.0  0.0\n 0.0   0.0   0.0   0.0  0.0\n 0.99  0.0   0.0   0.0  0.0\n 0.0   0.99  0.0   0.0  0.0\n 0.0   0.0   0.99  0.0  0.0\n\nChanging the weights signs with different sampling techniques:\n\njulia> forward_connection(5, 5; sampling_type=:irrational_sample!)\n5×5 Matrix{Float32}:\n  0.0  0.0   0.0  0.0  0.0\n  0.0  0.0   0.0  0.0  0.0\n -0.1  0.0   0.0  0.0  0.0\n  0.0  0.1   0.0  0.0  0.0\n  0.0  0.0  -0.1  0.0  0.0\n\njulia> forward_connection(5, 5; sampling_type=:irrational_sample!)\n5×5 Matrix{Float32}:\n  0.0  0.0   0.0  0.0  0.0\n  0.0  0.0   0.0  0.0  0.0\n  -0.1  0.0   0.0  0.0  0.0\n  0.0  0.1   0.0  0.0  0.0\n  0.0  0.0  -0.1  0.0  0.0\n\nChanging the weights to random numbers. Note that the length of the given array must be at least as long as the subdiagonal one wants to fill:\n\njulia> reservoir_matrix = forward_connection(5, 5; forward_weight=rand(Float32, 3))\n5×5 Matrix{Float32}:\n 0.0       0.0       0.0       0.0  0.0\n 0.0       0.0       0.0       0.0  0.0\n 0.274221  0.0       0.0       0.0  0.0\n 0.0       0.111511  0.0       0.0  0.0\n 0.0       0.0       0.618345  0.0  0.0\n\n\n\nReturning a sparse matrix:\n\n\njulia> reservoir_matrix = forward_connection(10, 10; return_sparse=true)\n10×10 SparseMatrixCSC{Float32, Int64} with 8 stored entries:\n  ⋅    ⋅    ⋅    ⋅    ⋅    ⋅    ⋅    ⋅    ⋅    ⋅\n  ⋅    ⋅    ⋅    ⋅    ⋅    ⋅    ⋅    ⋅    ⋅    ⋅\n 0.1   ⋅    ⋅    ⋅    ⋅    ⋅    ⋅    ⋅    ⋅    ⋅\n  ⋅   0.1   ⋅    ⋅    ⋅    ⋅    ⋅    ⋅    ⋅    ⋅\n  ⋅    ⋅   0.1   ⋅    ⋅    ⋅    ⋅    ⋅    ⋅    ⋅\n  ⋅    ⋅    ⋅   0.1   ⋅    ⋅    ⋅    ⋅    ⋅    ⋅\n  ⋅    ⋅    ⋅    ⋅   0.1   ⋅    ⋅    ⋅    ⋅    ⋅\n  ⋅    ⋅    ⋅    ⋅    ⋅   0.1   ⋅    ⋅    ⋅    ⋅\n  ⋅    ⋅    ⋅    ⋅    ⋅    ⋅   0.1   ⋅    ⋅    ⋅\n  ⋅    ⋅    ⋅    ⋅    ⋅    ⋅    ⋅   0.1   ⋅    ⋅\n\n\n\n\n\n","category":"function"},{"location":"api/inits/simple_cycle/#simple_cycle","page":"simple_cycle","title":"simple_cycle","text":"","category":"section"},{"location":"api/inits/simple_cycle/#References","page":"simple_cycle","title":"References","text":"Rodan, A. and Tino, P. (2011). Minimum Complexity Echo State Network. IEEE Transactions on Neural Networks 22, 131–144.\n\n\n\n","category":"section"},{"location":"api/inits/simple_cycle/#ReservoirComputing.simple_cycle","page":"simple_cycle","title":"ReservoirComputing.simple_cycle","text":"simple_cycle([rng], [T], dims...;\n    cycle_weight=0.1, return_sparse=false,\n    radius=nothing, kwargs...)\n\nCreate a simple cycle reservoir (Rodan and Tino, 2011).\n\nW_ij =\nbegincases\n    r  textif  i = j + 1 j in 1 D_mathrmres - 1 4pt\n    r  textif  i = 1 j = D_mathrmres 6pt\n    0  textotherwise\nendcases\n\nArguments\n\nrng: Random number generator. Default is Utils.default_rng()from WeightInitializers.\nT: Type of the elements in the reservoir matrix. Default is Float32.\ndims: Dimensions of the reservoir matrix.\n\nKeyword arguments\n\ncycle_weight: Weight of the connections in the reservoir matrix. This can be provided as a single value or an array. In case it is provided as an array please make sure that the length of the array matches the length of the cycle you want to populate. Default is 0.1.\nradius: The desired spectral radius of the reservoir. If nothing is passed, no scaling takes place. Defaults to nothing.\nreturn_sparse: flag for returning a sparse matrix. true requires SparseArrays to be loaded. Default is false.\nsampling_type: Sampling that decides the distribution of cycle_weight negative numbers. If set to :no_sample the sign is unchanged. If set to :bernoulli_sample! then each cycle_weight can be positive with a probability set by positive_prob. If set to :irrational_sample! the cycle_weight is negative if the decimal number of the irrational number chosen is odd. If set to :regular_sample!, each weight will be assigned a negative sign after the chosen strides. strides can be a single number or an array. Default is :no_sample.\npositive_prob: probability of the cycle_weight being positive when sampling_type is set to :bernoulli_sample!. Default is 0.5.\nirrational: Irrational number whose decimals decide the sign of cycle_weight. Default is pi.\nstart: Which place after the decimal point the counting starts for the irrational sign counting. Default is 1.\nstrides: number of strides for assigning negative value to a weight. It can be an integer or an array. Default is 2.\n\nExamples\n\nDefault call:\n\njulia> res_matrix = simple_cycle(5, 5)\n5×5 Matrix{Float32}:\n 0.0  0.0  0.0  0.0  0.1\n 0.1  0.0  0.0  0.0  0.0\n 0.0  0.1  0.0  0.0  0.0\n 0.0  0.0  0.1  0.0  0.0\n 0.0  0.0  0.0  0.1  0.0\n\nChanging weights:\n\njulia> res_matrix = simple_cycle(5, 5; cycle_weight=0.99)\n5×5 Matrix{Float32}:\n 0.0   0.0   0.0   0.0   0.99\n 0.99  0.0   0.0   0.0   0.0\n 0.0   0.99  0.0   0.0   0.0\n 0.0   0.0   0.99  0.0   0.0\n 0.0   0.0   0.0   0.99  0.0\n\nChanging weights to a custom array:\n\njulia> res_matrix = simple_cycle(5, 5; cycle_weight=rand(5))\n5×5 Matrix{Float32}:\n 0.0       0.0        0.0       0.0       0.471823\n 0.534782  0.0        0.0       0.0       0.0\n 0.0       0.0764598  0.0       0.0       0.0\n 0.0       0.0        0.507883  0.0       0.0\n 0.0       0.0        0.0       0.546656  0.0\n\nChanging sign of the weights with different samplings:\n\njulia> res_matrix = simple_cycle(5, 5; sampling_type=:irrational_sample!)\n5×5 Matrix{Float32}:\n  0.0  0.0   0.0   0.0  -0.1\n -0.1  0.0   0.0   0.0   0.0\n  0.0  0.1   0.0   0.0   0.0\n  0.0  0.0  -0.1   0.0   0.0\n  0.0  0.0   0.0  -0.1   0.0\n\njulia> res_matrix = simple_cycle(5, 5; sampling_type=:bernoulli_sample!)\n5×5 Matrix{Float32}:\n 0.0   0.0  0.0   0.0  0.1\n 0.1   0.0  0.0   0.0  0.0\n 0.0  -0.1  0.0   0.0  0.0\n 0.0   0.0  0.1   0.0  0.0\n 0.0   0.0  0.0  -0.1  0.0\n\nReturning as sparse:\n\njulia> using SparseArrays\n\njulia> res_matrix = simple_cycle(5, 5; return_sparse=true)\n5×5 SparseMatrixCSC{Float32, Int64} with 5 stored entries:\n  ⋅    ⋅    ⋅    ⋅   0.1\n 0.1   ⋅    ⋅    ⋅    ⋅\n  ⋅   0.1   ⋅    ⋅    ⋅\n  ⋅    ⋅   0.1   ⋅    ⋅\n  ⋅    ⋅    ⋅   0.1   ⋅\n\n\n\n\n\n","category":"function"},{"location":"#ReservoirComputing.jl","page":"ReservoirComputing.jl","title":"ReservoirComputing.jl","text":"ReservoirComputing.jl is a versatile and user-friendly Julia package designed for the implementation of Reservoir Computing models, such as Echo State Networks (ESNs). Reservoir Computing expands the input data into a higher-dimensional space, leveraging regression techniques for effective model training. This approach can be thought as a kernel method with an explicit kernel trick.\n\ninfo: Introductory material\nThis library assumes some basic knowledge of Reservoir Computing. For a good introduction, we suggest the following papers: the first two are the seminal papers about ESN and liquid state machines, the others are in-depth review papers that should cover all the needed information. For the majority of the algorithms implemented in this library we cited in the documentation the original work introducing them. If you ever are in doubt about a method or a function just type ? function in the Julia REPL to read the relevant notes.Jaeger, Herbert: The “echo state” approach to analyzing and training   recurrent neural networks-with an erratum note.\nMaass W, Natschläger T, Markram H: Real-time computing without   stable states: a new framework for neural computation based on   perturbations.\nLukoševičius, Mantas: A practical guide to applying echo state networks.   Neural networks: Tricks of the trade.\nLukoševičius, Mantas, and Herbert Jaeger: Reservoir computing approaches   to recurrent neural network training.\n\ninfo: Performance tip\nFor faster computations on the CPU it is suggested to add using MKL to the script. For clarity's sake this library will not be indicated  under every example in the documentation.\n\nwarn: Lux's states vs reservoir computing states\nReservoirComputing.jl builds on Lux.jl. As such, it inherits the model’s states st juggling and naming. This contrasts with the naming of the internal expansions of the input data, which in reservoir computing literature are also known as states. Since we cannot avoid using the same name for two different things, we tried to make it as explicit as possible in the documentation when we refer to one or the other. If you feel that this is causing confusion in some places, please open an issue!","category":"section"},{"location":"#Installation","page":"ReservoirComputing.jl","title":"Installation","text":"To install ReservoirComputing.jl, ensure you have Julia version 1.10 or higher. Follow these steps:\n\nOpen the Julia command line.\nEnter the Pkg REPL mode by pressing ].\nType add ReservoirComputing and press Enter.\n\nAlternatively, do:\n\nusing Pkg\nPkg.add(\"ReservoirComputing\")\n\nFor a more customized installation or to contribute to the package, consider cloning the repository:\n\nusing Pkg\nPkg.clone(\"https://github.com/SciML/ReservoirComputing.jl.git\")\n\nor dev the package.","category":"section"},{"location":"#Features-Overview","page":"ReservoirComputing.jl","title":"Features Overview","text":"Base layers for reservoir computing model construction. Main layers provide high level reservoir computing building blocks, such as ReservoirComputer and ReservoirChain. Additional, lower level layers provide the building blocks for custom reservoir computers, such as LinearReadout, Collect, ESNCell, ES2NCell, DelayLayer, NonlinearFeaturesLayer, and more\nFully built models:\nEcho state networks ESN\nDeep echo state networks DeepESN\nEcho state networks with delayed states DelayESN\nEdge of stability echo state networks ES2N\nHybrid echo state networks HybridESN\nNext generation reservoir computing NGRC\n15+ reservoir initializers and 5+ input layer initializers\n5+ reservoir states modification algorithms\nSparse matrix computation through SparseArrays.jl\nMultiple training algorithms via LIBSVM.jl and MLJLinearModels.jl","category":"section"},{"location":"#Contributing","page":"ReservoirComputing.jl","title":"Contributing","text":"Contributions to ReservoirComputing.jl are highly encouraged and appreciated. Whether it's through implementing new RC model variations, enhancing documentation, adding examples, or any improvement, your contribution is valuable. We welcome posts of relevant papers or ideas in the issues section. For deeper insights into the library's functionality, the API section in the documentation is a great resource. For any queries not suited for issues, please reach out to the lead developers via Slack or email.","category":"section"},{"location":"#Citing","page":"ReservoirComputing.jl","title":"Citing","text":"If you use ReservoirComputing.jl in your work, we kindly ask you to cite it. Here is the BibTeX entry for your convenience:\n\n@article{martinuzzi2022reservoircomputing,\n  author  = {Francesco Martinuzzi and Chris Rackauckas and Anas Abdelrehim and Miguel D. Mahecha and Karin Mora},\n  title   = {ReservoirComputing.jl: An Efficient and Modular Library for Reservoir Computing Models},\n  journal = {Journal of Machine Learning Research},\n  year    = {2022},\n  volume  = {23},\n  number  = {288},\n  pages   = {1--8},\n  url     = {http://jmlr.org/papers/v23/22-0611.html}\n}","category":"section"},{"location":"#Reproducibility","page":"ReservoirComputing.jl","title":"Reproducibility","text":"<details><summary>The documentation of this SciML package was built using these direct dependencies,</summary>\n\nusing Pkg # hide\nPkg.status() # hide\n\n</details>\n\n<details><summary>and using this machine and Julia version.</summary>\n\nusing InteractiveUtils # hide\nversioninfo() # hide\n\n</details>\n\n<details><summary>A more complete overview of all dependencies and their versions is also provided.</summary>\n\nusing Pkg # hide\nPkg.status(; mode=PKGMODE_MANIFEST) # hide\n\n</details>\n\nusing TOML\nusing Markdown\nversion = TOML.parse(read(\"../../Project.toml\", String))[\"version\"]\nname = TOML.parse(read(\"../../Project.toml\", String))[\"name\"]\nlink_manifest = \"https://github.com/SciML/\" * name * \".jl/tree/gh-pages/v\" * version *\n                \"/assets/Manifest.toml\"\nlink_project = \"https://github.com/SciML/\" * name * \".jl/tree/gh-pages/v\" * version *\n               \"/assets/Project.toml\"\nMarkdown.parse(\"\"\"You can also download the\n[manifest]($link_manifest)\nfile and the\n[project]($link_project)\nfile.\n\"\"\")","category":"section"},{"location":"api/models/#Models","page":"Models","title":"Models","text":"","category":"section"},{"location":"api/models/#Echo-State-Networks","page":"Models","title":"Echo State Networks","text":"","category":"section"},{"location":"api/models/#Next-generation-reservoir-computing","page":"Models","title":"Next generation reservoir computing","text":"","category":"section"},{"location":"api/models/#Utilities","page":"Models","title":"Utilities","text":"","category":"section"},{"location":"api/models/#Reservoir-Computing-with-Cellular-Automata","page":"Models","title":"Reservoir Computing with Cellular Automata","text":"The input encodings are the equivalent of the input matrices of the ESNs. These are the available encodings:","category":"section"},{"location":"api/models/#ReservoirComputing.ES2N","page":"Models","title":"ReservoirComputing.ES2N","text":"ES2N(in_dims, res_dims, out_dims, activation=tanh;\n    proximity=1.0, init_reservoir=rand_sparse, init_input=scaled_rand,\n    init_bias=zeros32, init_state=randn32, use_bias=False(),\n    state_modifiers=(), readout_activation=identity,\n    init_orthogonal=orthogonal,)\n\nEdge of Stability Echo State Network (ES2N) (Ceni and Gallicchio, 2025).\n\nEquations\n\nbeginaligned\nx(t) = beta phileft( rho mathbfW_r x(t-1) + omega\n    mathbfW_in u(t) right) + (1-beta) mathbfO x(t-1)\nendaligned\n\nArguments\n\nin_dims: Input dimension.\nres_dims: Reservoir (hidden state) dimension.\nout_dims: Output dimension.\nactivation: Reservoir activation (for ESNCell). Default: tanh.\n\nKeyword arguments\n\nproximity: proximity α ∈ (0,1]. Default: 1.0.\ninit_reservoir: Initializer for W_res. Default: rand_sparse.\ninit_input: Initializer for W_in. Default: scaled_rand.\ninit_orthogonal: Initializer for O. Default: [orthogonal].\ninit_bias: Initializer for reservoir bias (used if use_bias=true). Default: zeros32.\ninit_state: Initializer used when an external state is not provided. Default: randn32.\nuse_bias: Whether the reservoir uses a bias term. Default: false.\nstate_modifiers: A layer or collection of layers applied to the reservoir state before the readout. Accepts a single layer, an AbstractVector, or a Tuple. Default: empty ().\nreadout_activation: Activation for the linear readout. Default: identity.\n\nInputs\n\nx :: AbstractArray (in_dims, batch)\n\nReturns\n\nOutput y :: (out_dims, batch).\nUpdated layer state (NamedTuple).\n\nParameters\n\nreservoir — parameters of the internal ESNCell, including:\ninput_matrix :: (res_dims × in_dims) — W_in\nreservoir_matrix :: (res_dims × res_dims) — W_res\northogonal_matrix :: (res_dims × res_dims) — O\nbias :: (res_dims,) — present only if use_bias=true\nstates_modifiers — a Tuple with parameters for each modifier layer (may be empty).\nreadout — parameters of LinearReadout, typically:\nweight :: (out_dims × res_dims) — W_out\nbias :: (out_dims,) — b_out (if the readout uses bias)\n\nExact field names for modifiers/readout follow their respective layer definitions.\n\nStates\n\nreservoir — states for the internal ES2NCell (e.g. rng used to sample initial hidden states).\nstates_modifiers — a Tuple with states for each modifier layer.\nreadout — states for LinearReadout.\n\n\n\n\n\n","category":"type"},{"location":"api/models/#ReservoirComputing.ESN","page":"Models","title":"ReservoirComputing.ESN","text":"ESN(in_dims, res_dims, out_dims, activation=tanh;\n    leak_coefficient=1.0, init_reservoir=rand_sparse, init_input=scaled_rand,\n    init_bias=zeros32, init_state=randn32, use_bias=false,\n    state_modifiers=(), readout_activation=identity)\n\nEcho State Network (ESN): a reservoir (recurrent) layer followed by an optional sequence of state-modifier layers and a linear readout.\n\nESN composes:\n\na stateful ESNCell (reservoir),\nzero or more state_modifiers applied to the reservoir state, and\na LinearReadout mapping reservoir features to outputs.\n\nEquations\n\nFor input \\mathbf{x}(t) ∈ \\mathbb{R}^{in\\_dims}, reservoir state \\mathbf{h}(t) ∈ \\mathbb{R}^{res\\_dims}, and output \\mathbf{y}(t) ∈ \\mathbb{R}^{out\\_dims}:\n\nbeginaligned\n    tildemathbfh(t) = phileft(mathbfW_inmathbfx(t) +\n        mathbfW_resmathbfh(t-1) + mathbfbright) \n    mathbfh(t) = (1-alpha)mathbfh(t-1) + alphatildemathbfh(t) \n    mathbfz(t) = psileft(mathrmModsbig(mathbfh(t)big)right) \n    mathbfy(t) = rholeft(mathbfW_outmathbfz(t) + mathbfb_outright)\nendaligned\n\nArguments\n\nin_dims: Input dimension.\nres_dims: Reservoir (hidden state) dimension.\nout_dims: Output dimension.\nactivation: Reservoir activation (for ESNCell). Default: tanh.\n\nKeyword arguments\n\nReservoir (passed to ESNCell):\n\nleak_coefficient: Leak rate α ∈ (0,1]. Default: 1.0.\ninit_reservoir: Initializer for W_res. Default: rand_sparse.\ninit_input: Initializer for W_in. Default: scaled_rand.\ninit_bias: Initializer for reservoir bias (used if use_bias=true). Default: zeros32.\ninit_state: Initializer used when an external state is not provided. Default: randn32.\nuse_bias: Whether the reservoir uses a bias term. Default: false.\n\nComposition:\n\nstate_modifiers: A layer or collection of layers applied to the reservoir state before the readout. Accepts a single layer, an AbstractVector, or a Tuple. Default: empty ().\nreadout_activation: Activation for the linear readout. Default: identity.\n\nInputs\n\nx :: AbstractArray (in_dims, batch)\n\nReturns\n\nOutput y :: (out_dims, batch).\nUpdated layer state (NamedTuple).\n\nParameters\n\nreservoir — parameters of the internal ESNCell, including:\ninput_matrix :: (res_dims × in_dims) — W_in\nreservoir_matrix :: (res_dims × res_dims) — W_res\nbias :: (res_dims,) — present only if use_bias=true\nstates_modifiers — a Tuple with parameters for each modifier layer (may be empty).\nreadout — parameters of LinearReadout, typically:\nweight :: (out_dims × res_dims) — W_out\nbias :: (out_dims,) — b_out (if the readout uses bias)\n\nExact field names for modifiers/readout follow their respective layer definitions.\n\nStates\n\nreservoir — states for the internal ESNCell (e.g. rng used to sample initial hidden states).\nstates_modifiers — a Tuple with states for each modifier layer.\nreadout — states for LinearReadout.\n\n\n\n\n\n","category":"type"},{"location":"api/models/#ReservoirComputing.DeepESN","page":"Models","title":"ReservoirComputing.DeepESN","text":"DeepESN(in_dims, res_dims, out_dims,\n        activation=tanh; depth=2, leak_coefficient=1.0, init_reservoir=rand_sparse,\n        init_input=scaled_rand, init_bias=zeros32, init_state=randn32,\n        use_bias=false, state_modifiers=(), readout_activation=identity)\n\nDeep Echo State Network (DeepESN): a stack of stateful ESNCell layers (optionally with per-layer state modifiers) followed by a linear readout.\n\nDeepESN composes, for L = length(res_dims) layers:\n\na sequence of stateful ESNCell with widths res_dims[ℓ],\nzero or more per-layer state_modifiers[ℓ] applied to the layer's state, and\na final LinearReadout from the last layer's features to the output.\n\nEquations\n\nFor input \\mathbf{x}(t) ∈ \\mathbb{R}^{in\\_dims}, per-layer reservoir states \\mathbf{h}^{(\\ell)}(t) ∈ \\mathbb{R}^{res\\_dims[\\ell]} (\\ell = 1..L), and output \\mathbf{y}(t) ∈ \\mathbb{R}^{out\\_dims}:\n\n```math \\begin{aligned}     \\tilde{\\mathbf{h}}^{(1)}(t) &= \\phi1!\\left(         \\mathbf{W}^{(1)}{in}\\,\\mathbf{x}(t) + \\mathbf{W}^{(1)}{res}\\,\\mathbf{h}^{(1)}(t-1)         + \\mathbf{b}^{(1)}\\right) \\\n    \\mathbf{h}^{(1)}(t) &= (1-\\alpha1)\\,\\mathbf{h}^{(1)}(t-1) + \\alpha1\\,\\tilde{\\mathbf{h}}^{(1)}(t) \\\n    \\mathbf{u}^{(1)}(t) &= \\mathrm{Mods}1!\\big(\\mathbf{h}^{(1)}(t)\\big) \\\n    \\tilde{\\mathbf{h}}^{(\\ell)}(t) &= \\phi\\ell!\\left(         \\mathbf{W}^{(\\ell)}{in}\\,\\mathbf{u}^{(\\ell-1)}(t) +         \\mathbf{W}^{(\\ell)}{res}\\,\\mathbf{h}^{(\\ell)}(t-1) + \\mathbf{b}^{(\\ell)}\\right),         \\quad \\ell=2..L \\\n    \\mathbf{h}^{(\\ell)}(t) &= (1-\\alpha\\ell)\\,\\mathbf{h}^{(\\ell)}(t-1) + \\alpha\\ell\\,\\tilde{\\mathbf{h}}^{(\\ell)}(t),         \\quad \\ell=2..L \\\n    \\mathbf{u}^{(\\ell)}(t) &= \\mathrm{Mods}\\ell!\\big(\\mathbf{h}^{(\\ell)}(t)\\big), \\quad \\ell=2..L \\\n    \\mathbf{y}(t) &= \\rho!\\left(\\mathbf{W}{out}\\,\\mathbf{u}^{(L)}(t) + \\mathbf{b}{out}\\right) \\end{aligned}\n\nWhere\n\n\\mathbf{x}(t) ∈ ℝ^{in_dims × batch} — input at time t.\n\\mathbf{h}^{(\\ell)}(t) ∈ ℝ^{res_dims[ℓ] × batch} — hidden state of layer ℓ.\n\\tilde{\\mathbf{h}}^{(\\ell)}(t) — candidate state before leaky mixing.\n\\mathbf{u}^{(\\ell)}(t) — features after applying the ℓ-th state_modifiers (identity if none).\n\\mathbf{y}(t) ∈ ℝ^{out_dims × batch} — network output.\n\\mathbf{W}^{(\\ell)}_{in} ∈ ℝ^{res_dims[ℓ] × in\\_size[ℓ]} — input matrix at layer ℓ (in_size[1]=in_dims, in_size[ℓ]=res_dims[ℓ-1] for ℓ>1).\n\\mathbf{W}^{(\\ell)}_{res} ∈ ℝ^{res_dims[ℓ] × res_dims[ℓ]} — reservoir matrix at layer ℓ.\n\\mathbf{b}^{(\\ell)} ∈ ℝ^{res_dims[ℓ] × 1} — reservoir bias (broadcast over batch), present iff use_bias[ℓ]=true.\n\\mathbf{W}_{out} ∈ ℝ^{out_dims × res_dims[L]} — readout matrix.\n\\mathbf{b}_{out} ∈ ℝ^{out_dims × 1} — readout bias (if used by the readout).\n\\phi_\\ell — activation of layer ℓ (activation[ℓ], default tanh).\n\\alpha_\\ell ∈ (0,1] — leak coefficient of layer ℓ (leak_coefficient[ℓ]).\n\\mathrm{Mods}_\\ell(·) — composition of modifiers for layer ℓ (may be empty).\n\\rho — readout activation (readout_activation, default identity).\n\nArguments\n\nin_dims: Input dimension.\nres_dims: Vector of reservoir (hidden) dimensions per layer; its length sets the depth L.\nout_dims: Output dimension.\nactivation: Reservoir activation(s). Either a single function (broadcast to all layers) or a vector/tuple of length L. Default: tanh.\n\nKeyword arguments\n\nPer-layer reservoir options (passed to each ESNCell):\n\nleak_coefficient: Leak rate(s) α_ℓ ∈ (0,1]. Scalar or length-L collection. Default: 1.0.\ninit_reservoir: Initializer(s) for W_res^{(ℓ)}. Scalar or length-L. Default: rand_sparse.\ninit_input: Initializer(s) for W_in^{(ℓ)}. Scalar or length-L. Default: scaled_rand.\ninit_bias: Initializer(s) for reservoir bias (used iff use_bias[ℓ]=true). Scalar or length-L. Default: zeros32.\ninit_state: Initializer(s) used when an external state is not provided. Scalar or length-L. Default: randn32.\nuse_bias: Whether each reservoir uses a bias term. Boolean scalar or length-L. Default: false.\ndepth: Depth of the DeepESN. If the reservoir size is given as a number instead of a vector, this parameter controls the depth of the model. Default is 2.\n\nComposition:\n\nstate_modifiers: Per-layer modifier(s) applied to each layer’s state before it feeds into the next layer (and the readout for the last layer). Accepts nothing, a single layer, a vector/tuple of length L, or per-layer collections. Defaults to no modifiers.\nreadout_activation: Activation for the final linear readout. Default: identity.\n\nInputs\n\nx :: AbstractArray (in_dims, batch)\n\nReturns\n\nOutput y :: (out_dims, batch).\nUpdated layer state (NamedTuple) containing states for all cells, modifiers, and readout.\n\nParameters\n\ncells :: NTuple{L,NamedTuple} — parameters for each ESNCell, including:\ninput_matrix :: (res_dims[ℓ] × in_size[ℓ]) — W_in^{(ℓ)}\nreservoir_matrix :: (res_dims[ℓ] × res_dims[ℓ]) — W_res^{(ℓ)}\nbias :: (res_dims[ℓ],) — present only if use_bias[ℓ]=true\nstates_modifiers :: NTuple{L,Tuple} — per-layer tuples of modifier parameters (empty tuples if none).\nreadout — parameters of LinearReadout, typically:\nweight :: (out_dims × res_dims[L]) — W_out\nbias :: (out_dims,) — b_out (if the readout uses bias)\n\nExact field names for modifiers/readout follow their respective layer definitions.\n\nStates\n\ncells :: NTuple{L,NamedTuple} — states for each ESNCell.\nstates_modifiers :: NTuple{L,Tuple} — per-layer tuples of modifier states.\nreadout — states for LinearReadout.\n\n\n\n\n\n","category":"type"},{"location":"api/models/#ReservoirComputing.DelayESN","page":"Models","title":"ReservoirComputing.DelayESN","text":"DelayESN(in_dims, res_dims, out_dims, activation=tanh;\n         num_delays=1, stride=1, leak_coefficient=1.0,\n         init_reservoir=rand_sparse, init_input=scaled_rand,\n         init_bias=zeros32, init_state=randn32, use_bias=false,\n         state_modifiers=(), readout_activation=identity)\n\nEcho State Network whose reservoir state is first passed through a delay feature expansion before the readout. This implements a state-delayed ESN (Fleddermann et al., 2025), where the readout sees the current reservoir state together with a fixed number of its past values.\n\nDelayESN composes:\n\na stateful ESNCell (reservoir),\na DelayLayer applied to the reservoir state to build tapped-delay features,\nzero or more additional state_modifiers applied after the delay, and\na LinearReadout mapping delayed reservoir features to outputs.\n\nAt each time step, the reservoir produces a state vector h(t) of length res_dims. The DelayLayer then constructs a feature vector that stacks h(t) together with num_delays past states, spaced according to stride, before passing it on to any further modifiers and the readout.\n\nArguments\n\nin_dims: Input dimension.\nres_dims: Reservoir (hidden state) dimension.\nout_dims: Output dimension.\nactivation: Reservoir activation (for ESNCell). Default: tanh.\n\nKeyword arguments\n\nReservoir (passed to ESNCell):\n\nleak_coefficient: Leak rate in (0, 1]. Default: 1.0.\ninit_reservoir: Initializer for W_res. Default: rand_sparse.\ninit_input: Initializer for W_in. Default: scaled_rand.\ninit_bias: Initializer for reservoir bias (used iff use_bias=true). Default: zeros32.\ninit_state: Initializer used when an external state is not provided. Default: randn32.\nuse_bias: Whether the reservoir uses a bias term. Default: false.\n\nDelay expansion:\n\nnum_delays: Number of past reservoir states to include in the tapped-delay vector. The DelayLayer output has (num_delays + 1) * res_dims entries (current state plus num_delays past states). Default: 1.\nstride: Delay stride in layer calls. The delay buffer is updated only when the internal clock is a multiple of stride. Default: 1.\n\nComposition:\n\nstate_modifiers: A layer or collection of layers applied to the delayed reservoir features before the readout. These run after the internal DelayLayer. Accepts a single layer, an AbstractVector, or a Tuple. Default: empty ().\nreadout_activation: Activation for the linear readout. Default: identity.\n\nInputs\n\nx :: AbstractArray (in_dims, batch)\n\nReturns\n\nOutput y :: (out_dims, batch).\nUpdated layer state (NamedTuple).\n\nParameters\n\nreservoir — parameters of the internal ESNCell, including:\ninput_matrix :: (res_dims × in_dims) — W_in\nreservoir_matrix :: (res_dims × res_dims) — W_res\nbias :: (res_dims,) — present only if use_bias=true\nstates_modifiers — a Tuple with parameters for:\nthe internal DelayLayer, and\nany user-provided modifier layers (may be empty).\nreadout — parameters of LinearReadout, typically:\nweight :: (out_dims × ((num_delays + 1) * res_dims)) — W_out\nbias :: (out_dims,) — b_out (if the readout uses bias)\n\nExact field names for modifiers/readout follow their respective layer definitions.\n\nStates\n\nreservoir — states for the internal ESNCell (e.g. rng used to sample initial hidden states).\nstates_modifiers — a Tuple with states for the internal DelayLayer (its delay buffer and clock) and each additional modifier layer.\nreadout — states for LinearReadout (typically empty).\n\n\n\n\n\n","category":"type"},{"location":"api/models/#ReservoirComputing.HybridESN","page":"Models","title":"ReservoirComputing.HybridESN","text":"HybridESN(km, km_dims, in_dims, res_dims, out_dims, [activation];\n    state_modifiers=(), readout_activation=identity,\n    include_collect=true, kwargs...)\n\nHybrid Echo State Network (HybridESN): an Echo State Network augmented with a knowledge model whose outputs are concatenated to the ESN’s input and used throughout the reservoir and readout computations.\n\nHybridESN composes:\n\na knowledge model km producing auxiliary features from the input,\na stateful ESNCell that receives the concatenated input [km(x(t)); x(t)],\nzero or more state_modifiers applied to the reservoir state, and\na LinearReadout mapping the combined features [km(x(t)); h*(t)] to the output.\n\nArguments\n\nkm: Knowledge model applied to the input (e.g. a physical model, neural   submodule, or differentiable function). May be a WrappedFunction or any   callable layer.\nkm_dims: Output dimension of the knowledge model km.\nin_dims: Input dimension.\nres_dims: Reservoir (hidden state) dimension.\nout_dims: Output dimension.\nactivation: Reservoir activation (for ESNCell). Default: tanh.\n\nKeyword arguments\n\nleak_coefficient: Leak rate α ∈ (0,1]. Default: 1.0.\ninit_reservoir: Initializer for W_res. Default: rand_sparse.\ninit_input: Initializer for W_in. Default: scaled_rand.\ninit_bias: Initializer for reservoir bias (used if use_bias=true).   Default: zeros32.\ninit_state: Initializer used when an external state is not provided.   Default: randn32.\nuse_bias: Whether the reservoir uses a bias term. Default: false.\nstate_modifiers: A layer or collection of layers applied to the reservoir   state before the readout. Accepts a single layer, an AbstractVector, or a   Tuple. Default: empty ().\nreadout_activation: Activation for the linear readout. Default: identity.\ninclude_collect: Whether the readout should include collection mode.   Default: true.\n\nInputs\n\nx :: AbstractArray (in_dims, batch)\n\nReturns\n\nOutput y :: (out_dims, batch).\nUpdated layer state (NamedTuple).\n\nParameters\n\nknowledge_model — parameters of the knowledge model km.\nreservoir — parameters of the internal ESNCell, including:\ninput_matrix :: (res_dims × (in_dims + km_dims)) — W_in\nreservoir_matrix :: (res_dims × res_dims) — W_res\nbias :: (res_dims,) — present only if use_bias=true\nstates_modifiers — a Tuple with parameters for each modifier layer (may be empty).\nreadout — parameters of LinearReadout, typically:\nweight :: (out_dims × (res_dims + km_dims)) — W_out\nbias :: (out_dims,) — b_out (if the readout uses bias)\n\nExact field names for modifiers/readout follow their respective layer definitions.\n\nStates\n\nCreated by initialstates(rng, hesn):\n\nknowledge_model — states for the internal knowledge model.\nreservoir — states for the internal ESNCell.\nstates_modifiers — a Tuple with states for each modifier layer.\nreadout — states for LinearReadout.\n\n\n\n\n\n","category":"type"},{"location":"api/models/#ReservoirComputing.NGRC","page":"Models","title":"ReservoirComputing.NGRC","text":"NGRC(in_dims, out_dims; num_delays=2, stride=1,\n     features=(), include_input=true, init_delay=zeros32,\n     readout_activation=identity, state_modifiers=(),\n     ro_dims=nothing)\n\nNext Generation Reservoir Computing (NGRC) / NVAR-style model (Gauthier et al., 2021): a tapped-delay embedding of the input, followed by user-defined nonlinear feature maps and a linear readout. This is a \"reservoir-free\" architecture where all dynamics come from explicit input delays rather than a recurrent state.\n\nNGRC composes:\n\na DelayLayer applied directly to the input, producing a vector containing the current input and a fixed number of past inputs,\na NonlinearFeaturesLayer that applies user-provided functions to this delayed vector and concatenates the results, and\na LinearReadout mapping the resulting feature vector to outputs.\n\nInternally, NGRC is represented as a ReservoirComputer with:\n\nreservoir = the DelayLayer,\nstates_modifiers = the NonlinearFeaturesLayer plus any extra state_modifiers,\nreadout = the LinearReadout.\n\nArguments\n\nin_dims: Input dimension.\nout_dims: Output dimension.\n\nKeyword arguments\n\nnum_delays: Number of past input vectors to include. The internal DelayLayer outputs a vector of length (num_delays + 1) * in_dims (current input plus num_delays past inputs). Default: 2.\nstride: Delay stride in layer calls. The delay buffer is updated only when the internal clock is a multiple of stride. Default: 1.\ninit_delay: Initializer (or tuple of initializers) for the delay history, passed to DelayLayer. Each initializer function is called as init(rng, in_dims, 1) to fill one delay column. Default: zeros32.\nfeatures: A function or tuple of functions (f₁, f₂, ...) used by NonlinearFeaturesLayer. Each f is called as f(x) where x is the delayed input vector. By default it is assumed that each f returns a vector of the same length as x when ro_dims is not provided. Default: empty ().\ninclude_input: Whether to include the raw delayed input vector itself as the first block of the feature vector (passed to NonlinearFeaturesLayer). Default: true.\nstate_modifiers: Extra layers applied after the NonlinearFeaturesLayer and before the readout. Accepts a single layer, an AbstractVector, or a Tuple. Default: empty ().\nreadout_activation: Activation for the linear readout. Default: identity.\nro_dims: Input dimension of the readout. If nothing (default), it is estimated under the assumption that each feature function returns a vector with the same length as the delayed input. In that case, ro_dims ≈ (num_delays + 1) * in_dims * n_blocks, where n_blocks is the number of concatenated vectors (original delayed input if include_input=true plus one block per feature function). If your feature functions change the length (e.g. constant features, higher-order polynomial expansions with cross terms), you should pass ro_dims explicitly.\n\nInputs\n\nx :: AbstractArray (in_dims, batch) or (in_dims,)\n\nReturns\n\nOutput y :: (out_dims, batch) (or (out_dims,) for vector input).\nUpdated layer state (NamedTuple).\n\n\n\n\n\n","category":"type"},{"location":"api/models/#ReservoirComputing.polynomial_monomials","page":"Models","title":"ReservoirComputing.polynomial_monomials","text":"polynomial_monomials(input_vector;\n    degrees = 1:2)\n\nGenerate all unordered polynomial monomials of the entries in input_vector for the given set of degrees.\n\nFor each d in degrees, this function produces all degree-d monomials of the form\n\ndegree 1: x₁, x₂, …\ndegree 2: x₁², x₁x₂, x₁x₃, x₂², …\ndegree 3: x₁³, x₁²x₂, x₁x₂x₃, x₂³, …\n\nwhere combinations are taken with repetition and in non-decreasing index order. This means that, for example, x₁x₂ and x₂x₁ are represented only once.\n\nThe returned vector is a flat list of all such products, in a deterministic order determined by the recursive enumeration.\n\nArguments\n\ninput_vector Input vector whose entries define the variables used to build monomials.\n\nKeyword arguments\n\ndegrees: An iterable of positive integers specifying which monomial degrees to generate. Each degree less than 1 is skipped. Default: 1:2.\n\nReturns\n\noutput_monomials a vector of the same type as input_vector containing all generated monomials, concatenated across the requested degrees, in a deterministic order.\n\n\n\n\n\n","category":"function"},{"location":"api/models/#ReservoirComputing.resetcarry!","page":"Models","title":"ReservoirComputing.resetcarry!","text":"resetcarry!(rng, rc::ReservoirComputer, st; init_carry=nothing)\nresetcarry!(rng, rc::ReservoirComputer, ps, st; init_carry=nothing)\n\nReset (or set) the hidden-state carry of a model in the echo state network family.\n\nIf an existing carry is present in st.cell.carry, its leading dimension is used to infer the state size. Otherwise the reservoir output size is taken from rc.reservoir.cell.out_dims. When init_carry=nothing, the carry is cleared; the initializer from the struct construction will then be used. When a function is provided, it is called to create a new initial hidden state.\n\nArguments\n\nrng: Random number generator (used if a new carry is sampled/created).\nrc: A reservoir computing network model.\nst: Current model states.\nps: Optional model parameters. Returned unchanged.\n\nKeyword arguments\n\ninit_carry: Controls the initialization of the new carry.\nnothing (default): remove/clear the carry (forces the cell to reinitialize from its own init_state on next use).\nf: a function following standard from WeightInitializers.jl\n\nReturns\n\nresetcarry!(rng, rc, st; ...) -> st′: Updated states with st′.cell.carry set to nothing or (h0,).\nresetcarry!(rng, rc, ps, st; ...) -> (ps, st′): Same as above, but also returns the unchanged ps for convenience.\n\n\n\n\n\n","category":"function"},{"location":"api/models/#ReservoirComputing.RECA","page":"Models","title":"ReservoirComputing.RECA","text":"RECA(in_dims, out_dims, automaton;\n    input_encoding=RandomMapping(),\n    generations=8, state_modifiers=(),\n    readout_activation=identity)\n\nConstruct a cellular–automata reservoir model.\n\nAt each time step the input vector is randomly embedded into a Cellular Automaton (CA) lattice, the CA is evolved for generations steps, and the flattened evolution (excluding the initial row) is used as the reservoir state. A linear LinearReadout maps these features to out_dims.\n\nnote: Note\nThis constructor is only available when the CellularAutomata.jl package is loaded.\n\nArguments\n\nin_dims: Number of input features (rows of training data).\nout_dims: Number of output features (rows of target data).\nautomaton: A CA rule/object from CellularAutomata.jl (e.g. DCA(90), DCA(30), …).\n\nKeyword Arguments\n\ninput_encoding: Random embedding spec with fields permutations and expansion_size. Default is RandomMapping().\ngenerations: Number of CA generations to evolve per time step. Default is 8.\nstate_modifiers: Optional tuple/vector of additional layers applied after the CA cell and before the readout (e.g., NLAT2(), Pad(1.0), custom transforms, etc.). Functions are wrapped automatically. Default is none.\nreadout_activation: Activation applied by the readout Default is identity.\n\n\n\n\n\n","category":"function"},{"location":"api/models/#ReservoirComputing.RandomMapping","page":"Models","title":"ReservoirComputing.RandomMapping","text":"RandomMapping(permutations, expansion_size)\nRandomMapping(permutations; expansion_size=40)\nRandomMapping(;permutations=8, expansion_size=40)\n\nSpecify the random input embedding used by the Cellular Automata reservoir. Each time step, the input vector of length in_dims is randomly placed into a larger 1D lattice of length expansion_size, and this is repeated for permutations independent lattices (blocks). The concatenation of these blocks forms the CA initial condition of length: ca_size = expansion_size * permutations. The detail of this implementation can be found in (Nichele and Molund, 2017).\n\nArguments\n\npermutations: number of independent random maps (blocks). Larger values increase feature diversity and ca_size proportionally.\nexpansion_size: width of each block (the size of a single CA lattice). Larger values increase the spatial resolution and both ca_size and states_size.\n\nUsage\n\nThis is a configuration object; it does not perform the mapping by itself. Create the concrete tables with create_encoding and pass them to RECACell:\n\nusing ReservoirComputing, CellularAutomata, Random\n\nin_dims = 4\ngenerations = 8\nmapping = RandomMapping(permutations = 8, expansion_size = 40)\n\nenc = ReservoirComputing.create_encoding(mapping, in_dims, generations)  # → RandomMaps\ncell = RECACell(DCA(90), enc)\n\nrc = ReservoirChain(\n    StatefulLayer(cell),\n    LinearReadout(enc.states_size => in_dims; include_collect = true)\n)\n\nOr let RECA do this for you:\n\nrc = RECA(in_dims = 4, out_dims = 4, DCA(90);\n    input_encoding = RandomMapping(permutations = 8, expansion_size = 40),\n    generations = 8)\n\n\n\n\n\n","category":"type"},{"location":"references/#References","page":"References","title":"References","text":"Barbosa, W. A.; Griffith, A.; Rowlands, G. E.; Govia, L. C.; Ribeill, G. J.; Nguyen, M.-H.; Ohki, T. A. and Gauthier, D. J. (2021). Symmetry-aware reservoir computing. Physical Review E 104.\n\n\n\nCeni, A. and Gallicchio, C. (2025). Edge of Stability Echo State Network. IEEE Transactions on Neural Networks and Learning Systems 36, 7555–7564.\n\n\n\nChattopadhyay, A.; Hassanzadeh, P. and Subramanian, D. (2020). Data-driven predictions of a multiscale Lorenz 96 chaotic system using machine-learning methods: reservoir computing,  artificial neural network,  and long short-term memory network. Nonlinear Processes in Geophysics 27, 373–389.\n\n\n\nElsarraj, D.; Qisi, M. A.; Rodan, A.; Obeid, N.; Sharieh, A. and Faris, H. (2019). Demystifying echo state network with deterministic simple topologies. International Journal of Computational Science and Engineering 19, 407–417.\n\n\n\nFleddermann, L.; Herzog, S. and Parlitz, U. (2025). Enhancing reservoir predictions of chaotic time series by incorporating delayed values of input and reservoir variables. Chaos: An Interdisciplinary Journal of Nonlinear Science 35.\n\n\n\nFu, J.; Li, G.; Tang, J.; Xia, L.; Wang, L. and Duan, S. (2023). A double-cycle echo state network topology for time series prediction. Chaos: An Interdisciplinary Journal of Nonlinear Science 33.\n\n\n\nGallicchio, C. and Micheli, A. (2017). Deep echo state network (deepesn): A brief survey, arXiv preprint arXiv:1712.04323.\n\n\n\nGauthier, D. J.; Bollt, E.; Griffith, A. and Barbosa, W. A. (2021). Next generation reservoir computing. Nature Communications 12.\n\n\n\nGriffith, A.; Pomerance, A. and Gauthier, D. J. (2019). Forecasting chaotic systems with very low connectivity reservoir computers. Chaos: An Interdisciplinary Journal of Nonlinear Science 29.\n\n\n\nHerteux, J. and Räth, C. (2020). Breaking symmetries of the reservoir equations in echo state networks. Chaos: An Interdisciplinary Journal of Nonlinear Science 30.\n\n\n\nLu, Z.; Pathak, J.; Hunt, B.; Girvan, M.; Brockett, R. and Ott, E. (2017). Reservoir observers: Model-free inference of unmeasured variables in chaotic systems. Chaos: An Interdisciplinary Journal of Nonlinear Science 27.\n\n\n\nLukoševičius, M. (2012). A Practical Guide to Applying Echo State Networks. In: Neural Networks: Tricks of the Trade (Springer Berlin Heidelberg); pp. 659–686.\n\n\n\nMa, H.; Prosperino, D.; Haluszczynski, A. and Räth, C. (2023). Efficient forecasting of chaotic systems with block-diagonal and binary reservoir computing. Chaos: An Interdisciplinary Journal of Nonlinear Science 33.\n\n\n\nMargem, M. and Yilmaz, O. (2017). An experimental study on cellular automata reservoir in pathological sequence learning tasks.\n\n\n\nNichele, S. and Molund, A. (2017). Deep reservoir computing using cellular automata, arXiv preprint arXiv:1703.02806.\n\n\n\nPathak, J.; Lu, Z.; Hunt, B. R.; Girvan, M. and Ott, E. (2017). Using machine learning to replicate chaotic attractors and calculate Lyapunov exponents from data. Chaos: An Interdisciplinary Journal of Nonlinear Science 27.\n\n\n\nPathak, J.; Wikner, A.; Fussell, R.; Chandra, S.; Hunt, B. R.; Girvan, M. and Ott, E. (2018). Hybrid forecasting of chaotic processes: Using machine learning in conjunction with a knowledge-based model. Chaos: An Interdisciplinary Journal of Nonlinear Science 28.\n\n\n\nRodan, A. and Tino, P. (2011). Minimum Complexity Echo State Network. IEEE Transactions on Neural Networks 22, 131–144.\n\n\n\nRodan, A. and Tiňo, P. (2012). Simple Deterministically Constructed Cycle Reservoirs with Regular Jumps. Neural Computation 24, 1822–1852.\n\n\n\nViehweg, J.; Poll, C. and Mäder, P. (2025). Deterministic Reservoir Computing for Chaotic Time Series Prediction, arXiv preprint arXiv:2501.15615.\n\n\n\nWang, H.; Liu, Y.; Lu, P.; Luo, Y.; Wang, D. and Xu, X. (2022). Echo state network with logistic mapping and bias dropout for time series prediction. Neurocomputing 489, 196–210.\n\n\n\nXie, M.; Wang, Q. and Yu, S. (2024). Time Series Prediction of ESN Based on Chebyshev Mapping and Strongly Connected Topology. Neural Processing Letters 56.\n\n\n\nYang, C.; Qiao, J.; Han, H. and Wang, L. (2018). Design of polynomial echo state networks for time series prediction. Neurocomputing 290, 148–160.\n\n\n\nYilmaz, O. (2014). Reservoir computing using cellular automata, arXiv preprint arXiv:1410.0162.\n\n\n\n","category":"section"},{"location":"api/inits/delayline_backward/#delay*line*backward","page":"delaylinebackward","title":"delaylinebackward","text":"","category":"section"},{"location":"api/inits/delayline_backward/#References","page":"delaylinebackward","title":"References","text":"Rodan, A. and Tino, P. (2011). Minimum Complexity Echo State Network. IEEE Transactions on Neural Networks 22, 131–144.\n\n\n\n","category":"section"},{"location":"api/inits/delayline_backward/#ReservoirComputing.delayline_backward","page":"delaylinebackward","title":"ReservoirComputing.delayline_backward","text":"delayline_backward([rng], [T], dims...;\n    delay_weight=0.1, fb_weight=0.1,\n    delay_shift=1, fb_shift=1, return_sparse=false,\n    radius=nothing, delay_kwargs=(), fb_kwargs=())\n\nCreate a delay line backward reservoir with the specified by dims and weights. Creates a matrix with backward connections as described in (Rodan and Tino, 2011).\n\nW_ij =\nbegincases\n    r  textif  i = j + 1 j in 1 D_mathrmres - 1 4pt\n    b  textif  j = i + 1 i in 1 D_mathrmres - 1 6pt\n    0  textotherwise\nendcases\n\nArguments\n\nrng: Random number generator. Default is Utils.default_rng()from WeightInitializers.\nT: Type of the elements in the reservoir matrix. Default is Float32.\ndims: Dimensions of the reservoir matrix.\n\nKeyword arguments\n\ndelay_weight: The weight determines the absolute value of forward connections in the reservoir. This can be provided as a single value or an array. In case it is provided as an array please make sure that the length of the array matches the length of the sub-diagonal you want to populate. Default is 0.1.\nfb_weight: Determines the absolute value of backward connections in the reservoir. This can be provided as a single value or an array. In case it is provided as an array please make sure that the length of the array matches the length of the sub-diagonal you want to populate. Default is 0.1.\nfb_shift: How far the backward connection will be from the diagonal. Default is 1.\ndelay_shift: delay line shift relative to the diagonal. Default is 1.\nradius: The desired spectral radius of the reservoir. If nothing is passed, no scaling takes place. Defaults to nothing.\nreturn_sparse: flag for returning a sparse matrix. true requires SparseArrays to be loaded. Default is false.\ndelay_kwargs and fb_kwargs: named tuples that control the kwargs for the delay line weight and feedback weights respectively. The kwargs are as follows:\nsampling_type: Sampling that decides the distribution of weight negative numbers. If set to :no_sample the sign is unchanged. If set to :bernoulli_sample! then each weight can be positive with a probability set by positive_prob. If set to :irrational_sample! the weight is negative if the decimal number of the irrational number chosen is odd. If set to :regular_sample!, each weight will be assigned a negative sign after the chosen strides. strides can be a single number or an array. Default is :no_sample.\npositive_prob: probability of the weight being positive when sampling_type is set to :bernoulli_sample!. Default is 0.5.\nirrational: Irrational number whose decimals decide the sign of weight. Default is pi.\nstart: Which place after the decimal point the counting starts for the irrational sign counting. Default is 1.\nstrides: number of strides for assigning negative value to a weight. It can be an integer or an array. Default is 2.\n\nExamples\n\nDefault call:\n\njulia> res_matrix = delayline_backward(5, 5)\n5×5 Matrix{Float32}:\n 0.0  0.1  0.0  0.0  0.0\n 0.1  0.0  0.1  0.0  0.0\n 0.0  0.1  0.0  0.1  0.0\n 0.0  0.0  0.1  0.0  0.1\n 0.0  0.0  0.0  0.1  0.0\n\nChanging weights:\n\njulia> res_matrix = delayline_backward(5, 5; delay_weight = 0.99, fb_weight=-1.0)\n5×5 Matrix{Float32}:\n 0.0   -1.0    0.0    0.0    0.0\n 0.99   0.0   -1.0    0.0    0.0\n 0.0    0.99   0.0   -1.0    0.0\n 0.0    0.0    0.99   0.0   -1.0\n 0.0    0.0    0.0    0.99   0.0\n\nChanging weights to custom arrays:\n\njulia> res_matrix = delayline_backward(5, 5; delay_weight = rand(4), fb_weight=.-rand(4))\n5×5 Matrix{Float32}:\n 0.0       -0.294809   0.0        0.0        0.0\n 0.736006   0.0       -0.449479   0.0        0.0\n 0.0        0.10892    0.0       -0.60118    0.0\n 0.0        0.0        0.482435   0.0       -0.673392\n 0.0        0.0        0.0        0.177982   0.0\n\nChanging sign of the weights with different samplings:\n\njulia> res_matrix = delayline_backward(5, 5; delay_kwargs=(;sampling_type=:irrational_sample!))\n5×5 Matrix{Float32}:\n  0.0  0.1   0.0   0.0  0.0\n -0.1  0.0   0.1   0.0  0.0\n  0.0  0.1   0.0   0.1  0.0\n  0.0  0.0  -0.1   0.0  0.1\n  0.0  0.0   0.0  -0.1  0.0\n\njulia> res_matrix = delayline_backward(5, 5; fb_kwargs=(;sampling_type=:bernoulli_sample!))\n5×5 Matrix{Float32}:\n 0.0  0.1   0.0  0.0   0.0\n 0.1  0.0  -0.1  0.0   0.0\n 0.0  0.1   0.0  0.1   0.0\n 0.0  0.0   0.1  0.0  -0.1\n 0.0  0.0   0.0  0.1   0.0\n\nShifting:\n\njulia> res_matrix = delayline_backward(5, 5; delay_shift=3, fb_shift=2)\n5×5 Matrix{Float32}:\n 0.0  0.0  0.1  0.0  0.0\n 0.0  0.0  0.0  0.1  0.0\n 0.0  0.0  0.0  0.0  0.1\n 0.1  0.0  0.0  0.0  0.0\n 0.0  0.1  0.0  0.0  0.0\n\nReturning as sparse:\n\njulia> using SparseArrays\n\njulia> res_matrix = delayline_backward(5, 5; return_sparse=true)\n5×5 SparseMatrixCSC{Float32, Int64} with 8 stored entries:\n  ⋅   0.1   ⋅    ⋅    ⋅\n 0.1   ⋅   0.1   ⋅    ⋅\n  ⋅   0.1   ⋅   0.1   ⋅\n  ⋅    ⋅   0.1   ⋅   0.1\n  ⋅    ⋅    ⋅   0.1   ⋅\n\n\n\n\n\n","category":"function"},{"location":"examples/model_es2n/#Building-a-model-to-add-to-ReservoirComputing.jl","page":"Building a model to add to ReservoirComputing.jl","title":"Building a model to add to ReservoirComputing.jl","text":"This examples showcases how to build custom models such that they could be also included in ReservoirComputing.jl. In this example we will be building a edge of stability echo state network ES2N. Since the model is already available in the library, we will change the names of cells and models, to not cause problems.","category":"section"},{"location":"examples/model_es2n/#Building-an-ES2NCell","page":"Building a model to add to ReservoirComputing.jl","title":"Building an ES2NCell","text":"Building a ReservoirComputing.jl model largely follows the Lux.jl model approach.\n\nusing ReservoirComputing\nusing ConcreteStructs\nusing Static\nusing Random\n\nusing ReservoirComputing: IntegerType, BoolType, InputType, has_bias, _wrap_layers\nimport ReservoirComputing: initialparameters\n\n@concrete struct CustomES2NCell <: ReservoirComputing.AbstractEchoStateNetworkCell\n    activation\n    in_dims <: IntegerType\n    out_dims <: IntegerType\n    init_bias\n    init_reservoir\n    init_input\n    init_orthogonal\n    init_state\n    proximity\n    use_bias <: StaticBool\nend\n\nfunction CustomES2NCell((in_dims, out_dims)::Pair{<:IntegerType, <:IntegerType},\n        activation = tanh; use_bias::BoolType = False(), init_bias = zeros32,\n        init_reservoir = rand_sparse, init_input = scaled_rand,\n        init_state = randn32, init_orthogonal = orthogonal,\n        proximity::AbstractFloat = 1.0)\n    return CustomES2NCell(activation, in_dims, out_dims, init_bias, init_reservoir,\n        init_input, init_orthogonal, init_state, proximity, static(use_bias))\nend\n\nfunction initialparameters(rng::Random.AbstractRNG, esn::CustomES2NCell)\n    ps = (input_matrix = esn.init_input(rng, esn.out_dims, esn.in_dims),\n        reservoir_matrix = esn.init_reservoir(rng, esn.out_dims, esn.out_dims),\n        orthogonal_matrix = esn.init_orthogonal(rng, esn.out_dims, esn.out_dims))\n    if has_bias(esn)\n        ps = merge(ps, (bias = esn.init_bias(rng, esn.out_dims),))\n    end\n    return ps\nend\n\nfunction (esn::CustomES2NCell)((inp, (hidden_state,))::InputType, ps, st::NamedTuple)\n    T = eltype(inp)\n    if has_bias(esn)\n        candidate_h = esn.activation.(ps.input_matrix * inp .+\n                                      ps.reservoir_matrix * hidden_state .+ ps.bias)\n    else\n        candidate_h = esn.activation.(ps.input_matrix * inp .+\n                                      ps.reservoir_matrix * hidden_state)\n    end\n    h_new = (T(1.0) - esn.proximity) .* ps.orthogonal_matrix * hidden_state .+\n            esn.proximity .* candidate_h\n    return (h_new, (h_new,)), st\nend\n\nYou will notice that some definitions are missing. For instance, we did not dispatch over initialstates. This is because the AbstractEchoStateNetworkCell subtyping takes care of a lot of these smaller functions already.","category":"section"},{"location":"examples/model_es2n/#Building-the-full-ES2N-model","page":"Building a model to add to ReservoirComputing.jl","title":"Building the full ES2N model","text":"Now you can build a full model in two different ways:\n\nLeveraging ReservoirComputer\nBuilding from scratch with a proper CustomES2N struct\n\nfunction CustomES2NApproach1(in_dims, res_dims,\n      out_dims, activation = tanh;\n      readout_activation = identity,\n      state_modifiers = (),\n      kwargs...)\n  return ReservoirComputer(StatefulLayer(CustomES2NCell(in_dims => res_dims, activation; kwargs...)),\n      state_modifiers, LinearReadout(res_dims => out_dims, readout_activation))\nend\n\n@concrete struct CustomES2NApproach2 <:\n                 ReservoirComputing.AbstractEchoStateNetwork{(:reservoir, :states_modifiers, :readout)}\n    reservoir\n    states_modifiers\n    readout\nend\n\nfunction CustomES2NApproach2(in_dims::Int, res_dims::Int,\n        out_dims::Int, activation = tanh;\n        readout_activation = identity,\n        state_modifiers = (),\n        kwargs...)\n    cell = StatefulLayer(CustomES2NCell(in_dims => res_dims, activation; kwargs...))\n    mods_tuple = state_modifiers isa Tuple || state_modifiers isa AbstractVector ?\n                 Tuple(state_modifiers) : (state_modifiers,)\n    mods = _wrap_layers(mods_tuple)\n    ro = LinearReadout(res_dims => out_dims, readout_activation)\n    return CustomES2NApproach2(cell, mods, ro)\nend\n\nNow we can use the model like any other in ReservoirComputing.jl. Following the example in the getting started page:\n\nusing OrdinaryDiffEq\nusing Plots\n\nRandom.seed!(42)\nrng = MersenneTwister(17)\n\nfunction lorenz(du, u, p, t)\n    du[1] = p[1] * (u[2] - u[1])\n    du[2] = u[1] * (p[2] - u[3]) - u[2]\n    du[3] = u[1] * u[2] - p[3] * u[3]\nend\n\nprob = ODEProblem(lorenz, [1.0f0, 0.0f0, 0.0f0], (0.0, 200.0), [10.0f0, 28.0f0, 8/3])\ndata = Array(solve(prob, ABM54(); dt=0.02))\nshift = 300\ntrain_len = 5000\npredict_len = 1250\n\ninput_data = data[:, shift:(shift + train_len - 1)]\ntarget_data = data[:, (shift + 1):(shift + train_len)]\ntest = data[:, (shift + train_len):(shift + train_len + predict_len - 1)]\n\nesn = CustomES2NApproach2(3, 300, 3; init_reservoir=rand_sparse(; radius=1.2, sparsity=6/300),\n    state_modifiers=NLAT2)\n\nps, st = setup(rng, esn)\nps, st = train!(esn, input_data, target_data, ps, st)\noutput, st = predict(esn, predict_len, ps, st; initialdata=test[:, 1])\n\nplot(transpose(output)[:, 1], transpose(output)[:, 2], transpose(output)[:, 3];\n    label=\"predicted\")\nplot!(transpose(test)[:, 1], transpose(test)[:, 2], transpose(test)[:, 3];\n    label=\"actual\")","category":"section"},{"location":"api/inits/simple_cycle!/#simple_cycle!","page":"simple_cycle!","title":"simple_cycle!","text":"","category":"section"},{"location":"api/inits/simple_cycle!/#ReservoirComputing.simple_cycle!","page":"simple_cycle!","title":"ReservoirComputing.simple_cycle!","text":"simple_cycle!([rng], reservoir_matrix, weight;\n    sampling_type=:no_sample, irrational=pi, start=1,\n    p=0.5)\n\nAdds a simple cycle in the reservoir_matrix, with given weight. The weight can be a single number or an array.\n\nArguments\n\nrng: Random number generator. Default is Utils.default_rng() from WeightInitializers.\nreservoir_matrix: matrix to be changed.\nweight: weight to add as a simple cycle. Can be either a single number or an array.\n\nKeyword arguments\n\nsampling_type: Sampling that decides the distribution of weight negative numbers. If set to :no_sample the sign is unchanged. If set to :bernoulli_sample! then each weight can be positive with a probability set by positive_prob. If set to :irrational_sample! the weight is negative if the decimal number of the irrational number chosen is odd. If set to :regular_sample!, each weight will be assigned a negative sign after the chosen strides. strides can be a single number or an array. Default is :no_sample.\npositive_prob: probability of the weight being positive when sampling_type is set to :bernoulli_sample!. Default is 0.5.\nirrational: Irrational number whose decimals decide the sign of weight. Default is pi.\nstart: Which place after the decimal point the counting starts for the irrational sign counting. Default is 1.\nstrides: number of strides for assigning negative value to a weight. It can be an integer or an array. Default is 2.\n\nExamples\n\njulia> matrix = zeros(Float32, 5, 5)\n5×5 Matrix{Float32}:\n 0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n\njulia> simple_cycle!(matrix, 1.0; sampling_type = :irrational_sample!)\n5×5 Matrix{Float32}:\n  0.0  0.0   0.0   0.0  -1.0\n -1.0  0.0   0.0   0.0   0.0\n  0.0  1.0   0.0   0.0   0.0\n  0.0  0.0  -1.0   0.0   0.0\n  0.0  0.0   0.0  -1.0   0.0\n\n\n\n\n\n","category":"function"},{"location":"api/predict/#Predict","page":"Predict","title":"Predict","text":"","category":"section"},{"location":"api/predict/#ReservoirComputing.predict","page":"Predict","title":"ReservoirComputing.predict","text":"predict(rc, steps::Integer, ps, st; initialdata=nothing)\npredict(rc, data::AbstractMatrix, ps, st)\n\nRun the model either in (1) closed-loop (auto-regressive) mode for a fixed number of steps, or in (2) teacher-forced (point-by-point) mode over a given input sequence.\n\n1) Auto-regressive rollout\n\nBehavior\n\nRolls the model forward for steps time steps.\nAt each step, the model’s output becomes the next input.\n\nArguments\n\nrc: The reservoir chain / model.\nsteps: Number of time steps to generate.\nps: Model parameters.\nst: Model states.\n\nKeyword Arguments\n\ninitialdata=nothing: Column vector used as the first input. Has to be provided.\n\nReturns\n\noutput: Generated outputs of shape (out_dims, steps).\nst: Final model state after steps steps.\n\n2) Teacher-forced / point-by-point\n\nFeeds each column of data as input; the model state is threaded across time, and an output is produced for each input column.\n\nArguments\n\nrc: The reservoir chain / model.\ndata: Input sequence of shape (in_dims, T) (columns are time).\nps: Model parameters.\nst: Model states.\n\nReturns\n\noutput: Outputs for each input column, shape (out_dims, T).\nst: Updated minal model states.\n\n\n\n\n\n","category":"function"},{"location":"api/inits/cycle_jumps/#cycle_jumps","page":"cycle_jumps","title":"cycle_jumps","text":"","category":"section"},{"location":"api/inits/cycle_jumps/#References","page":"cycle_jumps","title":"References","text":"Rodan, A. and Tiňo, P. (2012). Simple Deterministically Constructed Cycle Reservoirs with Regular Jumps. Neural Computation 24, 1822–1852.\n\n\n\n","category":"section"},{"location":"api/inits/cycle_jumps/#ReservoirComputing.cycle_jumps","page":"cycle_jumps","title":"ReservoirComputing.cycle_jumps","text":"cycle_jumps([rng], [T], dims...;\n    cycle_weight=0.1, jump_weight=0.1, jump_size=3, return_sparse=false,\n    radius=nothing, cycle_kwargs=(), jump_kwargs=())\n\nCreate a cycle reservoir with jumps (Rodan and Tiňo, 2012).\n\nW_ij =\nbegincases\n    r    textif  i = j + 1 j in 1 D_mathrmres - 1 4pt\n    r    textif  i = 1 j = D_mathrmres 8pt\n    r_j  textif  i = j + ell 4pt\n    r_j  textif  j = i + ell 4pt\n    r_j  textif  (ij) = (1+ell 1) 4pt\n    r_j  textif  (ij) = (1 D_mathrmres+1-ell) 8pt\n    0  textotherwise\nendcases\n\nArguments\n\nrng: Random number generator. Default is Utils.default_rng()from WeightInitializers.\nT: Type of the elements in the reservoir matrix. Default is Float32.\ndims: Dimensions of the reservoir matrix.\n\nKeyword arguments\n\ncycle_weight:  The weight of cycle connections. This can be provided as a single value or an array. In case it is provided as an array please make sure that the length of the array matches the length of the cycle you want to populate. Default is 0.1.\njump_weight: The weight of jump connections. This can be provided as a single value or an array. In case it is provided as an array please make sure that the length of the array matches the length of the jumps you want to populate. Default is 0.1.\njump_size:  The number of steps between jump connections. Default is 3.\nradius: The desired spectral radius of the reservoir. If nothing is passed, no scaling takes place. Defaults to nothing.\nreturn_sparse: flag for returning a sparse matrix. true requires SparseArrays to be loaded. Default is false.\ncycle_kwargs and jump_kwargs: named tuples that control the kwargs for the cycle and jump weights respectively. The kwargs are as follows:\nsampling_type: Sampling that decides the distribution of weight negative numbers. If set to :no_sample the sign is unchanged. If set to :bernoulli_sample! then each weight can be positive with a probability set by positive_prob. If set to :irrational_sample! the weight is negative if the decimal number of the irrational number chosen is odd. If set to :regular_sample!, each weight will be assigned a negative sign after the chosen strides. strides can be a single number or an array. Default is :no_sample.\npositive_prob: probability of the weight being positive when sampling_type is set to :bernoulli_sample!. Default is 0.5.\nirrational: Irrational number whose decimals decide the sign of weight. Default is pi.\nstart: Which place after the decimal point the counting starts for the irrational sign counting. Default is 1.\nstrides: number of strides for assigning negative value to a weight. It can be an integer or an array. Default is 2.\n\nExamples\n\nDefault call:\n\njulia> res_matrix = cycle_jumps(5, 5)\n5×5 Matrix{Float32}:\n 0.0  0.0  0.0  0.1  0.1\n 0.1  0.0  0.0  0.0  0.0\n 0.0  0.1  0.0  0.0  0.0\n 0.1  0.0  0.1  0.0  0.0\n 0.0  0.0  0.0  0.1  0.0\n\nChanging weights:\n\njulia> res_matrix = cycle_jumps(5, 5; jump_weight = 2, cycle_weight=-1)\n5×5 Matrix{Float32}:\n 0.0   0.0   0.0   2.0  -1.0\n-1.0   0.0   0.0   0.0   0.0\n 0.0  -1.0   0.0   0.0   0.0\n 2.0   0.0  -1.0   0.0   0.0\n 0.0   0.0   0.0  -1.0   0.0\n\nChanging weights to custom arrays:\n\njulia> res_matrix = cycle_jumps(5, 5; jump_weight = .-rand(3), cycle_weight=rand(5))\n5×5 Matrix{Float32}:\n  0.0       0.0       0.0        -0.453905  0.443731\n  0.434804  0.0       0.0         0.0       0.0\n  0.0       0.520551  0.0         0.0       0.0\n -0.453905  0.0       0.0665751   0.0       0.0\n  0.0       0.0       0.0         0.57811   0.0\n\nChanging sign of the weights with different samplings:\n\njulia> res_matrix = cycle_jumps(5, 5; cycle_kwargs = (;sampling_type=:bernoulli_sample!))\n5×5 Matrix{Float32}:\n 0.0   0.0  0.0   0.1  0.1\n 0.1   0.0  0.0   0.0  0.0\n 0.0  -0.1  0.0   0.0  0.0\n 0.1   0.0  0.1   0.0  0.0\n 0.0   0.0  0.0  -0.1  0.0\n\njulia> res_matrix = cycle_jumps(5, 5; jump_kwargs = (;sampling_type=:irrational_sample!))\n5×5 Matrix{Float32}:\n  0.0  0.0  0.0  -0.1  0.1\n  0.1  0.0  0.0   0.0  0.0\n  0.0  0.1  0.0   0.0  0.0\n -0.1  0.0  0.1   0.0  0.0\n  0.0  0.0  0.0   0.1  0.0\n\nChanging cycle jumps length:\n\njulia> res_matrix = cycle_jumps(5, 5; jump_size = 2)\n5×5 Matrix{Float32}:\n 0.0  0.0  0.1  0.0  0.1\n 0.1  0.0  0.0  0.0  0.0\n 0.1  0.1  0.0  0.0  0.1\n 0.0  0.0  0.1  0.0  0.0\n 0.0  0.0  0.1  0.1  0.0\n\njulia> res_matrix = cycle_jumps(5, 5; jump_size = 4)\n5×5 Matrix{Float32}:\n 0.0  0.0  0.0  0.0  0.1\n 0.1  0.0  0.0  0.0  0.0\n 0.0  0.1  0.0  0.0  0.0\n 0.0  0.0  0.1  0.0  0.0\n 0.1  0.0  0.0  0.1  0.0\n\nReturn as a sparse matrix:\n\njulia> using SparseArrays\n\njulia> res_matrix = cycle_jumps(5, 5; return_sparse=true)\n5×5 SparseMatrixCSC{Float32, Int64} with 7 stored entries:\n  ⋅    ⋅    ⋅   0.1  0.1\n 0.1   ⋅    ⋅    ⋅    ⋅\n  ⋅   0.1   ⋅    ⋅    ⋅\n 0.1   ⋅   0.1   ⋅    ⋅\n  ⋅    ⋅    ⋅   0.1   ⋅\n\n\n\n\n\n","category":"function"},{"location":"api/inits/scaled_rand/#scaled_rand","page":"scaled_rand","title":"scaled_rand","text":"","category":"section"},{"location":"api/inits/scaled_rand/#ReservoirComputing.scaled_rand","page":"scaled_rand","title":"ReservoirComputing.scaled_rand","text":"scaled_rand([rng], [T], dims...;\n    scaling=0.1)\n\nCreate and return a matrix with random values, uniformly distributed within a range defined by scaling.\n\nArguments\n\nrng: Random number generator. Default is Utils.default_rng() from WeightInitializers.\nT: Type of the elements in the reservoir matrix. Default is Float32.\ndims: Dimensions of the matrix. Should follow res_size x in_size.\n\nKeyword arguments\n\nscaling: A scaling factor to define the range of the uniform distribution. The factor can be passed in three different ways:\nA single number. In this case, the matrix elements will be randomly chosen from the range [-scaling, scaling]. Default option, with a the scaling value set to 0.1.\nA tuple (lower, upper). The values define the range of the distribution. the matrix elements will be randomly created and scaled the range [lower, upper].\nA vector of length = in_size. In this case, the columns will be scaled individually by the entries of the vector. The entries can be numbers or tuples, which will mirror the behavior described above.\n\nExamples\n\nStandard behavior with scaling given by a scalar:\n\njulia> res_input = scaled_rand(8, 3)\n8×3 Matrix{Float32}:\n -0.0669356  -0.0292692  -0.0188943\n  0.0159724   0.004071   -0.0737949\n  0.026355   -0.0191563   0.0714962\n -0.0177412   0.0279123   0.0892906\n -0.0184405   0.0567368   0.0190222\n  0.0944272   0.0679244   0.0148647\n -0.0799005  -0.0891089  -0.0444782\n -0.0970182   0.0934286   0.03553\n\nScaling with a tuple, providing lower and upper bound of the uniform distribution from which the weights will be sampled:\n\njulia> res_input = scaled_rand(8, 3, scaling = (0.1, 0.15))\n8×3 Matrix{Float32}:\n 0.108266  0.117683  0.120276\n 0.128993  0.126018  0.106551\n 0.131589  0.120211  0.142874\n 0.120565  0.131978  0.147323\n 0.12039   0.139184  0.129756\n 0.148607  0.141981  0.128716\n 0.105025  0.102723  0.11388\n 0.100745  0.148357  0.133882\n\nScaling with a vector of scalars, where each provides the upper bound and its negative provides the lower bound. Each column is scaled in order: first element provides bounds for the first column, and so on:\n\njulia> res_input = scaled_rand(8, 3, scaling = [0.1, 0.2, 0.3])\n8×3 Matrix{Float32}:\n -0.0669356  -0.0585384   -0.0566828\n  0.0159724   0.00814199  -0.221385\n  0.026355   -0.0383126    0.214489\n -0.0177412   0.0558246    0.267872\n -0.0184405   0.113474     0.0570667\n  0.0944272   0.135849     0.0445941\n -0.0799005  -0.178218    -0.133435\n -0.0970182   0.186857     0.10659\n\nScaling with a vector of tuples, each providing both upper and lower bound. Each column is scaled in order: first element provides bounds for the first column, and so on:\n\njulia> res_input = scaled_rand(8, 3, scaling = [(0.1, 0.2), (-0.2, -0.1), (0.3, 0.5)])\n8×3 Matrix{Float32}:\n 0.116532  -0.164635  0.381106\n 0.157986  -0.147965  0.326205\n 0.163177  -0.159578  0.471496\n 0.141129  -0.136044  0.489291\n 0.14078   -0.121632  0.419022\n 0.197214  -0.116038  0.414865\n 0.11005   -0.194554  0.355522\n 0.101491  -0.103286  0.43553\n\n\n\n\n\n","category":"function"},{"location":"tutorials/ngrc/#Next-Generation-Reservoir-Computing","page":"Fitting a Next Generation Reservoir Computer","title":"Next Generation Reservoir Computing","text":"This tutorial shows how to use next generation reservoir computing NGRC in ReservoirComputing.jl to model the chaotic Lorenz system.\n\nNGRC works differently compared to traditional reservoir computing. In NGRC the reservoir is replaced with:\n\nA delay embedding of the input\nA nonlinear feature map\n\nThe model is finally trained through ridge regression, like a normal RC.\n\nIn this tutorial we will :\n\nsimulate the Lorenz system,\nbuild an NGRC model with delayed inputs and polynomial features, following the original paper,\ntrain it on one-step increments,\nroll it out generatively and compare with the true trajectory.","category":"section"},{"location":"tutorials/ngrc/#1.-Setup-and-imports","page":"Fitting a Next Generation Reservoir Computer","title":"1. Setup and imports","text":"First we need to load the necessary packages. We are going to use the following:\n\nusing OrdinaryDiffEq\nusing Random\nusing ReservoirComputing\nusing Plots\nusing Statistics","category":"section"},{"location":"tutorials/ngrc/#2.-Define-Lorenz-system-and-generate-data","page":"Fitting a Next Generation Reservoir Computer","title":"2. Define Lorenz system and generate data","text":"We define the Lorenz system and integrate it to generate a long trajectory:\n\nfunction lorenz!(du, u, p, t)\n    σ, ρ, β = p\n    du[1] = σ * (u[2] - u[1])\n    du[2] = u[1] * (ρ - u[3]) - u[2]\n    du[3] = u[1] * u[2] - β * u[3]\nend\n\nprob = ODEProblem(\n    lorenz!,\n    Float32[1.0, 0.0, 0.0],\n    (0.0, 200.0),\n    (10.0f0, 28.0f0, 8/3f0),\n)\n\ndata = Array(solve(prob, ABM54(); dt = 0.025))  # size: (3, T)\n\nWe then split the time series into training and testing segments:\n\nshift = 300\ntrain_len = 500\npredict_len = 900\n\ninput_data = data[:, shift:(shift + train_len - 1)]\ntarget_data = data[:, (shift + 1):(shift + train_len)]\ntest_data = data[:, (shift + train_len):(shift + train_len + predict_len - 1)]","category":"section"},{"location":"tutorials/ngrc/#3.-Normalization","page":"Fitting a Next Generation Reservoir Computer","title":"3. Normalization","text":"It is good practice to normalize the data, especially for polynomial features:\n\nin_mean = mean(input_data; dims = 2)\nin_std = std(input_data;  dims = 2)\n\ntrain_norm_x = (input_data  .- in_mean) ./ in_std\ntrain_norm_y = (target_data .- in_mean) ./ in_std\ntest_norm_x  = (test_data   .- in_mean) ./ in_std\n\n# We train an increment (residual) model: Δy = y_{t+1} − y_t\ntrain_delta_y = train_norm_y .- train_norm_x","category":"section"},{"location":"tutorials/ngrc/#4.-Build-the-NGRC-model","page":"Fitting a Next Generation Reservoir Computer","title":"4. Build the NGRC model","text":"Now that we have the data we can start building the model. Following the approach of the paper we first define two feature functions:\n\na constant feature\na second order polynomial monomial \n\nconst_feature = x -> Float32[1.0]\npoly_feature  = x -> polynomial_monomials(x; degrees = 1:2)\n\nFinally, we can construct the NGRC model.\n\nWe set the following:\n\nin_dims = 3\nout_dims = 3\nnum_delays = 1\n\nWith in_dims=3 and num_delays=1 the delayed input length is 6. Adding the polinomial of degrees 1 and 2 will put give us 21 more. Finally, the constant term adds 1 more feature. In total we have 28 features. \n\nWe can pass the number of features to ro_dims to initialize the LinearReadout with the correct dimensions. However, unless one is planning to fry run the model without training, the train function will take care to adjust the dimensions.\n\nNow we build the NGRC:\n\nrng = MersenneTwister(0)\n\nngrc = NGRC(in_dims, out_dims; num_delays = num_delays, stride = 1, features = (const_feature, poly_feature),\n    include_input = false,  # we already encode everything in the features\n    ro_dims = 28,\n    readout_activation = identity)\n\nps, st = setup(rng, ngrc)\n\nAt this point, ngrc is a fully specified model with:\n\na DelayLayer that builds a 6-dimensional delayed vector from the 3D input,\na NonlinearFeaturesLayer that maps that vector to 28 polynomial features,\na LinearReadout (28 => 3).","category":"section"},{"location":"tutorials/ngrc/#5.-Training-the-NGRC-readout","page":"Fitting a Next Generation Reservoir Computer","title":"5. Training the NGRC readout","text":"We now train the linear readout using ridge regression on the increment train_delta_y:\n\nps, st = train!(ngrc, train_norm_x, train_delta_y, ps, st;\n    train_method = StandardRidge(2.5e-6))\n\nwhere StandardRidge is the ridge regression provided natively by ReservoirComputing.jl.","category":"section"},{"location":"tutorials/ngrc/#6.-Generative-prediction","page":"Fitting a Next Generation Reservoir Computer","title":"6. Generative prediction","text":"We now perform generative prediction on the increments to obtain the predicted time series:\n\nsingle_step = copy(test_norm_x[:, 1]) # normalized initial condition\ntraj_norm = similar(test_norm_x, 3, predict_len)\n\nfor step in 1:predict_len\n    global st\n    delta_step, st = ngrc(single_step, ps, st)\n    single_step .= single_step .+ delta_step # increment update in normalized space\n    traj_norm[:, step] .= single_step\nend\n\nFinally, we unscale back to the original coordinates:\n\ntraj = traj_norm .* in_std .+ in_mean # size: (3, predict_len)","category":"section"},{"location":"tutorials/ngrc/#7.-Visualization","page":"Fitting a Next Generation Reservoir Computer","title":"7. Visualization","text":"We can now compare the predicted trajectory with the true Lorenz data on the test segment:\n\nplot(transpose(test_data)[:, 1], transpose(test_data)[:, 2], transpose(test_data)[:, 3]; label=\"actual\");\nplot!(transpose(traj)[:, 1], transpose(traj)[:, 2], transpose(traj)[:, 3]; label=\"predicted\")","category":"section"},{"location":"api/inits/rand_sparse/#rand_sparse","page":"rand_sparse","title":"rand_sparse","text":"","category":"section"},{"location":"api/inits/rand_sparse/#ReservoirComputing.rand_sparse","page":"rand_sparse","title":"ReservoirComputing.rand_sparse","text":"rand_sparse([rng], [T], dims...;\n    radius=1.0, sparsity=0.1, std=1.0, return_sparse=false)\n\nCreate and return a random sparse reservoir matrix. The matrix will be of size specified by dims, with specified sparsity and scaled spectral radius according to radius.\n\nArguments\n\nrng: Random number generator. Default is Utils.default_rng()from WeightInitializers.\nT: Type of the elements in the reservoir matrix. Default is Float32.\ndims: Dimensions of the reservoir matrix.\n\nKeyword arguments\n\nradius: The desired spectral radius of the reservoir. Defaults to 1.0.\nsparsity: The sparsity level of the reservoir matrix, controlling the fraction of zero elements. Defaults to 0.1.\nreturn_sparse: flag for returning a sparse matrix. true requires SparseArrays to be loaded. Default is false.\n\nExamples\n\nChanging the sparsity:\n\njulia> res_matrix = rand_sparse(5, 5; sparsity = 0.5)\n5×5 Matrix{Float32}:\n 0.0        0.0        0.0        0.0      0.0\n 0.0        0.794565   0.0        0.26164  0.0\n 0.0        0.0       -0.931294   0.0      0.553706\n 0.723235  -0.524727   0.0        0.0      0.0\n 1.23723    0.0        0.181824  -1.5478   0.465328\n\njulia> res_matrix = rand_sparse(5, 5; sparsity = 0.2)\n5×5 Matrix{Float32}:\n 0.0       0.0        0.0   0.0      0.0\n 0.0       0.853184   0.0   0.0      0.0\n 0.0       0.0       -1.0   0.0      0.0\n 0.776591  0.0        0.0   0.0      0.0\n 0.0       0.0        0.0  -1.66199  0.499657\n\njulia> res_matrix = rand_sparse(5, 5; sparsity = 0.8)\n5×5 Matrix{Float32}:\n 0.0        0.229011   0.625026    -0.660061  -1.39078\n -0.295761   0.32544    0.0          0.107163   0.0\n 0.766352   1.44836   -0.381442    -0.435473   0.226788\n 0.296224  -0.214919   0.00956791   0.0        0.210393\n 0.506746   0.0        0.0744718   -0.633951   0.19059\n\nReturning a sparse matrix:\n\njulia> using SparseArrays\n\njulia> res_matrix = rand_sparse(5, 5; sparsity = 0.4, return_sparse = true)\n5×5 SparseMatrixCSC{Float32, Int64} with 10 stored entries:\n  ⋅          ⋅          ⋅          ⋅        ⋅\n  ⋅         0.794565    ⋅         0.26164   ⋅\n  ⋅          ⋅        -0.931294    ⋅       0.553706\n 0.723235  -0.524727    ⋅          ⋅        ⋅\n 1.23723     ⋅         0.181824  -1.5478   0.465328\n\n\n\n\n\n","category":"function"},{"location":"api/inits/chebyshev_mapping/#chebyshev_mapping","page":"chebyshev_mapping","title":"chebyshev_mapping","text":"","category":"section"},{"location":"api/inits/chebyshev_mapping/#References","page":"chebyshev_mapping","title":"References","text":"Xie, M.; Wang, Q. and Yu, S. (2024). Time Series Prediction of ESN Based on Chebyshev Mapping and Strongly Connected Topology. Neural Processing Letters 56.\n\n\n\n","category":"section"},{"location":"api/inits/chebyshev_mapping/#ReservoirComputing.chebyshev_mapping","page":"chebyshev_mapping","title":"ReservoirComputing.chebyshev_mapping","text":"chebyshev_mapping([rng], [T], dims...;\n    amplitude=one(T), sine_divisor=one(T),\n    chebyshev_parameter=one(T), return_sparse=false)\n\nGenerate a Chebyshev-mapped matrix (Xie et al., 2024). The first row is initialized using a sine function and subsequent rows are iteratively generated via the Chebyshev mapping. The first row is defined as:\n\n    W1 j = textamplitude cdot sin(j cdot pi  (textsine_divisor\n        cdot textn_cols))\n\nfor j = 1, 2, …, ncols (with ncols typically equal to K+1, where K is the number of input layer neurons). Subsequent rows are generated by applying:\n\n    Wi+1 j = cos( textchebyshev_parameter cdot acos(Wpi j))\n\nArguments\n\nrng: Random number generator. Default is Utils.default_rng() from WeightInitializers.\nT: Type of the elements in the reservoir matrix. Default is Float32.\ndims: Dimensions of the matrix. Should follow res_size x in_size. res_size is assumed to be K+1.\n\nKeyword arguments\n\namplitude: Scaling factor used to initialize the first row. This parameter adjusts the amplitude of the sine function. Default value is one.\nsine_divisor: Divisor applied in the sine function's phase. Default value is one.\nchebyshev_parameter: Control parameter for the Chebyshev mapping in subsequent rows. This parameter influences the distribution of the matrix elements. Default is one.\nreturn_sparse: If true, the function returns the matrix as a sparse matrix. Default is false.\n\nExamples\n\njulia> input_matrix = chebyshev_mapping(10, 3)\n10×3 Matrix{Float32}:\n 0.866025  0.866025   1.22465f-16\n 0.866025  0.866025  -4.37114f-8\n 0.866025  0.866025  -4.37114f-8\n 0.866025  0.866025  -4.37114f-8\n 0.866025  0.866025  -4.37114f-8\n 0.866025  0.866025  -4.37114f-8\n 0.866025  0.866025  -4.37114f-8\n 0.866025  0.866025  -4.37114f-8\n 0.866025  0.866025  -4.37114f-8\n 0.866025  0.866025  -4.37114f-8\n\n\n\n\n\n","category":"function"},{"location":"api/inits/pseudo_svd/#pseudo_svd","page":"pseudo_svd","title":"pseudo_svd","text":"","category":"section"},{"location":"api/inits/pseudo_svd/#References","page":"pseudo_svd","title":"References","text":"Yang, C.; Qiao, J.; Han, H. and Wang, L. (2018). Design of polynomial echo state networks for time series prediction. Neurocomputing 290, 148–160.\n\n\n\n","category":"section"},{"location":"api/inits/pseudo_svd/#ReservoirComputing.pseudo_svd","page":"pseudo_svd","title":"ReservoirComputing.pseudo_svd","text":"pseudo_svd([rng], [T], dims...;\n    max_value=1.0, sparsity=0.1, sorted=true, reverse_sort=false,\n    return_sparse=false)\n\nReturns an initializer to build a sparse reservoir matrix with the given sparsity by using a pseudo-SVD approach as described in (Yang et al., 2018).\n\nArguments\n\nrng: Random number generator. Default is Utils.default_rng()from WeightInitializers.\nT: Type of the elements in the reservoir matrix. Default is Float32.\ndims: Dimensions of the reservoir matrix.\n\nKeyword arguments\n\nmax_value: The maximum absolute value of elements in the matrix. Default is 1.0\nsparsity: The desired sparsity level of the reservoir matrix. Default is 0.1\nsorted: A boolean indicating whether to sort the singular values before creating the diagonal matrix. Default is true.\nreverse_sort: A boolean indicating whether to reverse the sorted singular values. Default is false.\nreturn_sparse: flag for returning a sparse matrix. true requires SparseArrays to be loaded. Default is false.\nreturn_diag: flag for returning a Diagonal matrix. If both return_diag and return_sparse are set to true priority is given to return_diag. Default is false.\n\nExamples\n\nDefault call:\n\njulia> res_matrix = pseudo_svd(5, 5)\n5×5 Matrix{Float32}:\n 0.306998  0.0       0.0       0.0       0.0\n 0.0       0.325977  0.0       0.0       0.0\n 0.0       0.0       0.549051  0.0       0.0\n 0.0       0.0       0.0       0.726199  0.0\n 0.0       0.0       0.0       0.0       1.0\n\nWith reversed sorting:\n\njulia> pseudo_svd(5, 5; reverse_sort = true)\n5×5 Matrix{Float32}:\n 1.0  0.0       0.0       0.0       0.0\n 0.0  0.726199  0.0       0.0       0.0\n 0.0  0.0       0.549051  0.0       0.0\n 0.0  0.0       0.0       0.325977  0.0\n 0.0  0.0       0.0       0.0       0.306998\n\nWith no sorting\n\njulia> pseudo_svd(5, 5; sorted = false)\n5×5 Matrix{Float32}:\n 0.726199  0.0       0.0       0.0       0.0\n 0.0       0.325977  0.0       0.0       0.0\n 0.0       0.0       0.306998  0.0       0.0\n 0.0       0.0       0.0       0.549051  0.0\n 0.0       0.0       0.0       0.0       0.788919\n\nReturning as a Diagonal or a sparse matrix:\n\njulia> pseudo_svd(5, 5; return_diag = true)\n5×5 LinearAlgebra.Diagonal{Float32, Vector{Float32}}:\n 0.306998   ⋅         ⋅         ⋅         ⋅\n  ⋅        0.325977   ⋅         ⋅         ⋅\n  ⋅         ⋅        0.549051   ⋅         ⋅\n  ⋅         ⋅         ⋅        0.726199   ⋅\n  ⋅         ⋅         ⋅         ⋅        1.0\n\njulia> using SparseArrays\n\njulia> pseudo_svd(5, 5; return_sparse = true)\n5×5 SparseMatrixCSC{Float32, Int64} with 5 stored entries:\n 0.306998   ⋅         ⋅         ⋅         ⋅\n  ⋅        0.325977   ⋅         ⋅         ⋅\n  ⋅         ⋅        0.549051   ⋅         ⋅\n  ⋅         ⋅         ⋅        0.726199   ⋅\n  ⋅         ⋅         ⋅         ⋅        1.0\n\n\n\n\n\n","category":"function"},{"location":"api/inits/selfloop_delayline_backward/#selfloop*delayline*backward","page":"selfloopdelaylinebackward","title":"selfloopdelaylinebackward","text":"","category":"section"},{"location":"api/inits/selfloop_delayline_backward/#References","page":"selfloopdelaylinebackward","title":"References","text":"Elsarraj, D.; Qisi, M. A.; Rodan, A.; Obeid, N.; Sharieh, A. and Faris, H. (2019). Demystifying echo state network with deterministic simple topologies. International Journal of Computational Science and Engineering 19, 407–417.\n\n\n\n","category":"section"},{"location":"api/inits/selfloop_delayline_backward/#ReservoirComputing.selfloop_delayline_backward","page":"selfloopdelaylinebackward","title":"ReservoirComputing.selfloop_delayline_backward","text":"selfloop_delayline_backward([rng], [T], dims...;\n    delay_weight=0.1, selfloop_weight=0.1, fb_weight=0.1,\n    fb_shift=2, delya_shift=1, radius=nothing, return_sparse=false,\n    fb_kwargs=(), selfloop_kwargs=(), delay_kwargs=())\n\nCreates a reservoir based on a delay line with the addition of self loops and backward connections shifted by one (Elsarraj et al., 2019).\n\nThis architecture is referred to as TP3 in the original paper.\n\nW_ij =\nbegincases\n    ll  textif  i = j text for  i = 1 dots N \n    r  textif  j = i - 1 text for  i = 2 dots N \n    r  textif  j = i - 2 text for  i = 3 dots N \n    0  textotherwise\nendcases\n\nArguments\n\nrng: Random number generator. Default is Utils.default_rng()from WeightInitializers.\nT: Type of the elements in the reservoir matrix. Default is Float32.\ndims: Dimensions of the reservoir matrix.\n\nKeyword arguments\n\ndelay_weight: Weight of the delay line connections in the reservoir matrix. This can be provided as a single value or an array. In case it is provided as an array please make sure that the length of the array matches the length of the cycle you want to populate. Default is 0.1.\nselfloop_weight: Weight of the self loops in the reservoir matrix. This can be provided as a single value or an array. In case it is provided as an array please make sure that the length of the array matches the length of the diagonal you want to populate. Default is 0.1.\nfb_weight: Weight of the feedback in the reservoir matrix. This can be provided as a single value or an array. In case it is provided as an array please make sure that the length of the array matches the length of the diagonal you want to populate. Default is 0.1.\nfb_shift: How far the backward connection will be from the diagonal. Default is 1.\ndelay_shift: delay line shift relative to the diagonal. Default is 1.\nradius: The desired spectral radius of the reservoir. If nothing is passed, no scaling takes place. Defaults to nothing.\nreturn_sparse: flag for returning a sparse matrix. true requires SparseArrays to be loaded. Default is false.\ndelay_kwargs, selfloop_kwargs, and fb_kwargs: named tuples that control the kwargs for the weights generation. The kwargs are as follows:\nsampling_type: Sampling that decides the distribution of weight negative numbers.   If set to :no_sample the sign is unchanged. If set to :bernoulli_sample! then each   weight can be positive with a probability set by positive_prob. If set to   :irrational_sample! the weight is negative if the decimal number of the   irrational number chosen is odd. If set to :regular_sample!, each weight will be   assigned a negative sign after the chosen strides. strides can be a single   number or an array. Default is :no_sample.\npositive_prob: probability of the weight being positive when sampling_type is set to :bernoulli_sample!. Default is 0.5.\nirrational: Irrational number whose decimals decide the sign of weight. Default is pi.\nstart: Which place after the decimal point the counting starts for the irrational sign counting. Default is 1.\nstrides: number of strides for assigning negative value to a weight. It can be an integer or an array. Default is 2.\n\nExamples\n\nDefault call:\n\njulia> res_matrix = selfloop_delayline_backward(5, 5)\n5×5 Matrix{Float32}:\n 0.1  0.0  0.1  0.0  0.0\n 0.1  0.1  0.0  0.1  0.0\n 0.0  0.1  0.1  0.0  0.1\n 0.0  0.0  0.1  0.1  0.0\n 0.0  0.0  0.0  0.1  0.1\n\nChanging weights:\n\njulia> res_matrix = selfloop_delayline_backward(5, 5; selfloop_weight=0.3, fb_weight=0.99, delay_weight=-0.5)\n5×5 Matrix{Float32}:\n  0.3   0.0   0.99   0.0   0.0\n -0.5   0.3   0.0    0.99  0.0\n  0.0  -0.5   0.3    0.0   0.99\n  0.0   0.0  -0.5    0.3   0.0\n  0.0   0.0   0.0   -0.5   0.3\n\nChanging weights to custom arrays:\n\njulia> res_matrix = selfloop_delayline_backward(5, 5; selfloop_weight=randn(5), fb_weight=rand(5), delay_weight=-rand(5))\n5×5 Matrix{Float32}:\n -1.22847    0.0       0.384073   0.0        0.0\n -0.699175   2.63937   0.0        0.345408   0.0\n  0.0       -0.5171   -0.452312   0.0        0.0205082\n  0.0        0.0      -0.193893   1.45921    0.0\n  0.0        0.0       0.0       -0.453015  -1.43402\n\nChanging sign of the weights with different samplings:\n\njulia> res_matrix = selfloop_delayline_backward(5, 5; selfloop_kwargs=(;sampling_type=:irrational_sample!))\n5×5 Matrix{Float32}:\n -0.1  0.0   0.1   0.0   0.0\n  0.1  0.1   0.0   0.1   0.0\n  0.0  0.1  -0.1   0.0   0.1\n  0.0  0.0   0.1  -0.1   0.0\n  0.0  0.0   0.0   0.1  -0.1\n\njulia> res_matrix = selfloop_delayline_backward(5, 5; delay_kwargs=(;sampling_type=:bernoulli_sample!))\n5×5 Matrix{Float32}:\n 0.1   0.0  0.1   0.0  0.0\n 0.1   0.1  0.0   0.1  0.0\n 0.0  -0.1  0.1   0.0  0.1\n 0.0   0.0  0.1   0.1  0.0\n 0.0   0.0  0.0  -0.1  0.1\n\njulia> res_matrix = selfloop_delayline_backward(5, 5; fb_kwargs=(;sampling_type=:regular_sample!))\n5×5 Matrix{Float32}:\n 0.1  0.0  0.1   0.0  0.0\n 0.1  0.1  0.0  -0.1  0.0\n 0.0  0.1  0.1   0.0  0.1\n 0.0  0.0  0.1   0.1  0.0\n 0.0  0.0  0.0   0.1  0.1\n\nShifting the delay and the backward line:\n\njulia> res_matrix = selfloop_delayline_backward(5, 5; delay_shift=3, fb_shift=2)\n5×5 Matrix{Float32}:\n 0.1  0.0  0.1  0.0  0.0\n 0.0  0.1  0.0  0.1  0.0\n 0.0  0.0  0.1  0.0  0.1\n 0.1  0.0  0.0  0.1  0.0\n 0.0  0.1  0.0  0.0  0.1\n\nReturning as sparse:\n\njulia> using SparseArrays\n\njulia> res_matrix = selfloop_delayline_backward(5, 5; return_sparse=true)\n5×5 SparseMatrixCSC{Float32, Int64} with 12 stored entries:\n 0.1   ⋅   0.1   ⋅    ⋅\n 0.1  0.1   ⋅   0.1   ⋅\n  ⋅   0.1  0.1   ⋅   0.1\n  ⋅    ⋅   0.1  0.1   ⋅\n  ⋅    ⋅    ⋅   0.1  0.1\n\n\n\n\n\n","category":"function"},{"location":"api/inits/weighted_minimal/#weighted_minimal","page":"weighted_minimal","title":"weighted_minimal","text":"","category":"section"},{"location":"api/inits/weighted_minimal/#References","page":"weighted_minimal","title":"References","text":"Lu, Z.; Pathak, J.; Hunt, B.; Girvan, M.; Brockett, R. and Ott, E. (2017). Reservoir observers: Model-free inference of unmeasured variables in chaotic systems. Chaos: An Interdisciplinary Journal of Nonlinear Science 27.\n\n\n\n","category":"section"},{"location":"api/inits/weighted_minimal/#ReservoirComputing.weighted_minimal","page":"weighted_minimal","title":"ReservoirComputing.weighted_minimal","text":"weighted_minimal([rng], [T], dims...;\n    weight=0.1, return_sparse=false,\n    sampling_type=:no_sample)\n\nCreate and return a minimal weighted input layer matrix. This initializer generates a weighted input matrix with equal, deterministic elements in the same construction as [weighted_minimal](@ref), inspired by (Lu et al., 2017).\n\nPlease note that this initializer computes its own reservoir size! If the computed reservoir size is different than the provided one it will raise a warning.\n\nArguments\n\nrng: Random number generator. Default is Utils.default_rng() from WeightInitializers.\nT: Type of the elements in the reservoir matrix. Default is Float32.\ndims: Dimensions of the matrix. Should follow res_size x in_size.\n\nKeyword arguments\n\nweight: The value for all the weights in the input matrix. Defaults to 0.1.\nreturn_sparse: flag for returning a sparse matrix. Default is false.\nsampling_type: Sampling that decides the distribution of weight negative numbers. If set to :no_sample the sign is unchanged. If set to :bernoulli_sample! then each weight can be positive with a probability set by positive_prob. If set to :irrational_sample! the weight is negative if the decimal number of the irrational number chosen is odd. If set to :regular_sample!, each weight will be assigned a negative sign after the chosen strides. strides can be a single number or an array. Default is :no_sample.\npositive_prob: probability of the weight being positive when sampling_type is set to :bernoulli_sample!. Default is 0.5.\nirrational: Irrational number whose decimals decide the sign of weight. Default is pi.\nstart: Which place after the decimal point the counting starts for the irrational sign counting. Default is 1.\nstrides: number of strides for assigning negative value to a weight. It can be an integer or an array. Default is 2.\n\nExamples\n\nStandard call, changing the init weight:\n\njulia> res_input = weighted_minimal(9, 3; weight = 0.99)\n9×3 Matrix{Float32}:\n 0.99  0.0   0.0\n 0.99  0.0   0.0\n 0.99  0.0   0.0\n 0.0   0.99  0.0\n 0.0   0.99  0.0\n 0.0   0.99  0.0\n 0.0   0.0   0.99\n 0.0   0.0   0.99\n 0.0   0.0   0.99\n\nRandom sign for each weight, drawn from a bernoulli distribution:\n\njulia> res_input = weighted_minimal(9, 3; sampling_type = :bernoulli_sample!)\n9×3 Matrix{Float32}:\n 0.1  -0.0  -0.0\n-0.1  -0.0  -0.0\n 0.1  -0.0   0.0\n-0.0   0.1   0.0\n 0.0   0.1  -0.0\n 0.0   0.1   0.0\n-0.0  -0.0  -0.1\n-0.0  -0.0   0.1\n 0.0  -0.0   0.1\n\nExample of different reservoir size for the initializer:\n\njulia> res_input = weighted_minimal(8, 3)\n┌ Warning: Reservoir size has changed!\n│\n│     Computed reservoir size (6) does not equal the provided reservoir size (8).\n│\n│     Using computed value (6). Make sure to modify the reservoir initializer accordingly.\n│\n└ @ ReservoirComputing ~/.julia/dev/ReservoirComputing/src/esn/esn_inits.jl:159\n6×3 Matrix{Float32}:\n 0.1  0.0  0.0\n 0.1  0.0  0.0\n 0.0  0.1  0.0\n 0.0  0.1  0.0\n 0.0  0.0  0.1\n 0.0  0.0  0.1\n\n\n\n\n\n","category":"function"},{"location":"api/inits/modified_lm/#modified_lm","page":"modified_lm","title":"modified_lm","text":"","category":"section"},{"location":"api/inits/modified_lm/#References","page":"modified_lm","title":"References","text":"Viehweg, J.; Poll, C. and Mäder, P. (2025). Deterministic Reservoir Computing for Chaotic Time Series Prediction, arXiv preprint arXiv:2501.15615.\n\n\n\n","category":"section"},{"location":"api/inits/modified_lm/#ReservoirComputing.modified_lm","page":"modified_lm","title":"ReservoirComputing.modified_lm","text":"modified_lm([rng], [T], dims...;\n    factor, amplitude=0.3, sine_divisor=5.9, logistic_parameter=2.35,\n    return_sparse=false)\n\nGenerate a input weight matrix based on the logistic mapping (Viehweg et al., 2025). Thematrix is built so that each input is transformed into a high-dimensional feature space via a recursive logistic map. For each input, a chain of weights is generated as follows:\n\nThe first element of the chain is initialized using a sine function:\n\n      W1j = textamplitude cdot sin( (j cdot pi) \n          (textfactor cdot textn cdot textsine_divisor) )\n\nwhere j is the index corresponding to the input and n is the number of inputs.\n\nSubsequent elements are recursively computed using the logistic mapping:\n\n      Wi+1j = textlogistic_parameter cdot Wij cdot (1 - Wij)\n\nThe resulting matrix has dimensions (factor * in_size) x in_size, where in_size corresponds to the number of columns provided in dims. If the provided number of rows does not match factor * in_size the number of rows is overridden.\n\nArguments\n\nrng: Random number generator. Default is Utils.default_rng() from WeightInitializers.\nT: Type of the elements in the reservoir matrix. Default is Float32.\ndims: Dimensions of the matrix. Should follow res_size x in_size.\n\nKeyword arguments\n\nfactor: The number of logistic map iterations (chain length) per input, determining the number of rows per input.\namplitude: Scaling parameter A for the sine-based initialization of the first element in each logistic chain. Default is 0.3.\nsine_divisor: Parameter B used to adjust the phase in the sine initialization. Default is 5.9.\nlogistic_parameter: The parameter r in the logistic recurrence that governs the chain dynamics. Default is 2.35.\nreturn_sparse: If true, returns the resulting matrix as a sparse matrix. Default is false.\n\nExamples\n\njulia> modified_lm(20, 10; factor=2)\n20×10 SparseArrays.SparseMatrixCSC{Float32, Int64} with 18 stored entries:\n⎡⢠⠀⠀⠀⠀⎤\n⎢⠀⢣⠀⠀⠀⎥\n⎢⠀⠀⢣⠀⠀⎥\n⎢⠀⠀⠀⢣⠀⎥\n⎣⠀⠀⠀⠀⢣⎦\n\njulia> modified_lm(12, 4; factor=3)\n12×4 SparseArrays.SparseMatrixCSC{Float32, Int64} with 9 stored entries:\n  ⋅    ⋅          ⋅          ⋅\n  ⋅    ⋅          ⋅          ⋅\n  ⋅    ⋅          ⋅          ⋅\n  ⋅   0.0133075   ⋅          ⋅\n  ⋅   0.0308564   ⋅          ⋅\n  ⋅   0.070275    ⋅          ⋅\n  ⋅    ⋅         0.0265887   ⋅\n  ⋅    ⋅         0.0608222   ⋅\n  ⋅    ⋅         0.134239    ⋅\n  ⋅    ⋅          ⋅         0.0398177\n  ⋅    ⋅          ⋅         0.0898457\n  ⋅    ⋅          ⋅         0.192168\n\n\n\n\n\n\n","category":"function"},{"location":"api/utils/#Utilities","page":"Utilities","title":"Utilities","text":"","category":"section"},{"location":"api/utils/#ReservoirComputing.collectstates","page":"Utilities","title":"ReservoirComputing.collectstates","text":"collectstates(rc, data, ps, st)\n\nRun the sequence data once through the reservoir chain rc, advancing the model state over time, and collect feature vectors at every Collect layer. If more than one Collect is encountered in a step, their vectors are concatenated with vcat in order of appearance. If no Collect is seen in a step, the feature defaults to the final vector exiting the chain for that time step.\n\nnote: Note\nIf your LinearReadout layer was created with include_collect=true (default behaviour), a collection point is placed immediately before the readout, so the collected features are the inputs to the readout.\n\nArguments\n\nrc: A ReservoirChain (or compatible AbstractLuxLayer with .layers).\ndata: Input sequence of shape (in_dims, T), where columns are time steps.\nps, st: Current parameters and state for rc.\n\nReturns\n\nstates: Reservoir states, i.e. a feature matrix with one column per time step. The feature dimension n_features equals the vertical concatenation of all vectors captured at Collect layers in that step.\nst: Updated model states.\n\n\n\n\n\n","category":"function"},{"location":"api/inits/selfloop_cycle/#selfloop_cycle","page":"selfloop_cycle","title":"selfloop_cycle","text":"","category":"section"},{"location":"api/inits/selfloop_cycle/#References","page":"selfloop_cycle","title":"References","text":"Elsarraj, D.; Qisi, M. A.; Rodan, A.; Obeid, N.; Sharieh, A. and Faris, H. (2019). Demystifying echo state network with deterministic simple topologies. International Journal of Computational Science and Engineering 19, 407–417.\n\n\n\n","category":"section"},{"location":"api/inits/selfloop_cycle/#ReservoirComputing.selfloop_cycle","page":"selfloop_cycle","title":"ReservoirComputing.selfloop_cycle","text":"selfloop_cycle([rng], [T], dims...;\n    cycle_weight=0.1, selfloop_weight=0.1,\n    radius=nothing, return_sparse=false, kwargs...)\n\nCreates a simple cycle reservoir with the addition of self loops (Elsarraj et al., 2019).\n\nThis architecture is referred to as TP1 in the original paper.\n\nW_ij =\nbegincases\n    ll  textif  i = j \n    r  textif  j = i - 1 text for  i = 2 dots N \n    r  textif  i = 1 j = N \n    0  textotherwise\nendcases\n\nArguments\n\nrng: Random number generator. Default is Utils.default_rng()from WeightInitializers.\nT: Type of the elements in the reservoir matrix. Default is Float32.\ndims: Dimensions of the reservoir matrix.\n\nKeyword arguments\n\ncycle_weight: Weight of the cycle connections in the reservoir matrix. This can be provided as a single value or an array. In case it is provided as an array please make sure that the length of the array matches the length of the cycle you want to populate. Default is 0.1.\nselfloop_weight: Weight of the self loops in the reservoir matrix. This can be provided as a single value or an array. In case it is provided as an array please make sure that the length of the array matches the length of the diagonal you want to populate. Default is 0.1.\nradius: The desired spectral radius of the reservoir. If nothing is passed, no scaling takes place. Defaults to nothing.\nreturn_sparse: flag for returning a sparse matrix. true requires SparseArrays to be loaded. Default is false.\ncycle_kwargs and jump_kwargs: named tuples that control the kwargs for the cycle and jump weights respectively. The kwargs are as follows:\nsampling_type: Sampling that decides the distribution of weight negative numbers. If set to :no_sample the sign is unchanged. If set to :bernoulli_sample! then each weight can be positive with a probability set by positive_prob. If set to :irrational_sample! the weight is negative if the decimal number of the irrational number chosen is odd. If set to :regular_sample!, each weight will be assigned a negative sign after the chosen strides. strides can be a single number or an array. Default is :no_sample.\npositive_prob: probability of the weight being positive when sampling_type is set to :bernoulli_sample!. Default is 0.5.\nirrational: Irrational number whose decimals decide the sign of weight. Default is pi.\nstart: Which place after the decimal point the counting starts for the irrational sign counting. Default is 1.\nstrides: number of strides for assigning negative value to a weight. It can be an integer or an array. Default is 2.\n\nExamples\n\nDefault call:\n\njulia> res_matrix = selfloop_cycle(5, 5)\n5×5 Matrix{Float32}:\n 0.1  0.0  0.0  0.0  0.1\n 0.1  0.1  0.0  0.0  0.0\n 0.0  0.1  0.1  0.0  0.0\n 0.0  0.0  0.1  0.1  0.0\n 0.0  0.0  0.0  0.1  0.1\n\njldoctest slcycle\n\nChanging weights:\n\njulia> res_matrix = selfloop_cycle(5, 5; cycle_weight=-0.2, selfloop_weight=0.5)\n5×5 Matrix{Float32}:\n  0.5   0.0   0.0   0.0  -0.2\n -0.2   0.5   0.0   0.0   0.0\n  0.0  -0.2   0.5   0.0   0.0\n  0.0   0.0  -0.2   0.5   0.0\n  0.0   0.0   0.0  -0.2   0.5\n\nChanging weights to custom arrays:\n\njulia> res_matrix = selfloop_cycle(5, 5; cycle_weight=rand(5), selfloop_weight=.-rand(5))\n5×5 Matrix{Float32}:\n -0.902546   0.0          0.0        0.0          0.0987988\n  0.911585  -0.968998     0.0        0.0          0.0\n  0.0        0.00149246  -0.613033   0.0          0.0\n  0.0        0.0          0.777804  -0.727024     0.0\n  0.0        0.0          0.0        0.00441047  -0.310635\n\nChanging sign of the weights with different samplings:\n\njulia> res_matrix = selfloop_cycle(5, 5; cycle_kwargs=(;sampling_type=:irrational_sample!))\n5×5 Matrix{Float32}:\n  0.1  0.0   0.0   0.0  -0.1\n -0.1  0.1   0.0   0.0   0.0\n  0.0  0.1   0.1   0.0   0.0\n  0.0  0.0  -0.1   0.1   0.0\n  0.0  0.0   0.0  -0.1   0.1\n\njulia> res_matrix = selfloop_cycle(5, 5; selfloop_kwargs=(;sampling_type=:bernoulli_sample!))\n5×5 Matrix{Float32}:\n 0.1   0.0  0.0   0.0  0.1\n 0.1  -0.1  0.0   0.0  0.0\n 0.0   0.1  0.1   0.0  0.0\n 0.0   0.0  0.1  -0.1  0.0\n 0.0   0.0  0.0   0.1  0.1\n\nReturning as sparse:\n\njulia> using SparseArrays\n\njulia> res_matrix = selfloop_cycle(5, 5; return_sparse=true)\n5×5 SparseMatrixCSC{Float32, Int64} with 10 stored entries:\n 0.1   ⋅    ⋅    ⋅   0.1\n 0.1  0.1   ⋅    ⋅    ⋅\n  ⋅   0.1  0.1   ⋅    ⋅\n  ⋅    ⋅   0.1  0.1   ⋅\n  ⋅    ⋅    ⋅   0.1  0.1\n\n\n\n\n\n","category":"function"},{"location":"tutorials/hybrid/#Hybrid-Echo-State-Networks","page":"Hybrid Echo State Networks","title":"Hybrid Echo State Networks","text":"","category":"section"},{"location":"api/inits/informed_init/#informed_init","page":"informed_init","title":"informed_init","text":"","category":"section"},{"location":"api/inits/informed_init/#References","page":"informed_init","title":"References","text":"Pathak, J.; Wikner, A.; Fussell, R.; Chandra, S.; Hunt, B. R.; Girvan, M. and Ott, E. (2018). Hybrid forecasting of chaotic processes: Using machine learning in conjunction with a knowledge-based model. Chaos: An Interdisciplinary Journal of Nonlinear Science 28.\n\n\n\n","category":"section"},{"location":"api/inits/informed_init/#ReservoirComputing.informed_init","page":"informed_init","title":"ReservoirComputing.informed_init","text":"informed_init([rng], [T], dims...;\n    scaling=0.1, model_in_size, gamma=0.5)\n\nCreate an input layer for informed echo state networks (Pathak et al., 2018).\n\nArguments\n\nrng: Random number generator. Default is Utils.default_rng() from WeightInitializers.\nT: Type of the elements in the reservoir matrix. Default is Float32.\ndims: Dimensions of the matrix. Should follow res_size x in_size.\n\nKeyword arguments\n\nscaling: The scaling factor for the input matrix. Default is 0.1.\nmodel_in_size: The size of the input model.\ngamma: The gamma value. Default is 0.5.\n\nExamples\n\n\n\n\n\n","category":"function"},{"location":"api/inits/self_loop!/#self_loop!","page":"self_loop!","title":"self_loop!","text":"","category":"section"},{"location":"api/inits/self_loop!/#ReservoirComputing.self_loop!","page":"self_loop!","title":"ReservoirComputing.self_loop!","text":"self_loop!([rng], reservoir_matrix, weight, jump_size;\n    sampling_type=:no_sample, irrational=pi, start=1,\n    positive_prob=0.5)\n\nAdds jumps to a given reservoir_matrix with chosen weight and determined jump_size. weight can be either a number or an array.\n\nArguments\n\nrng: Random number generator. Default is Utils.default_rng() from WeightInitializers.\nreservoir_matrix: matrix to be changed.\nweight: weight to add as a self loop. Can be either a single number or an array.\n\nKeyword arguments\n\nsampling_type: Sampling that decides the distribution of weight negative numbers. If set to :no_sample the sign is unchanged. If set to :bernoulli_sample! then each weight can be positive with a probability set by positive_prob. If set to :irrational_sample! the weight is negative if the decimal number of the irrational number chosen is odd. If set to :regular_sample!, each weight will be assigned a negative sign after the chosen strides. strides can be a single number or an array. Default is :no_sample.\npositive_prob: probability of the weight being positive when sampling_type is set to :bernoulli_sample!. Default is 0.5.\nirrational: Irrational number whose decimals decide the sign of weight. Default is pi.\nstart: Which place after the decimal point the counting starts for the irrational sign counting. Default is 1.\nstrides: number of strides for assigning negative value to a weight. It can be an integer or an array. Default is 2.\n\nExamples\n\njulia> matrix = zeros(Float32, 5, 5)\n5×5 Matrix{Float32}:\n 0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n\njulia> self_loop!(matrix, 1.0)\n5×5 Matrix{Float32}:\n  1.0  0.0   0.0   0.0   0.0\n  0.0  1.0   0.0   0.0   0.0\n  0.0  0.0   1.0   0.0   0.0\n  0.0  0.0   0.0   1.0   0.0\n  0.0  0.0   0.0   0.0   1.0\n\n\n\n\n\n","category":"function"},{"location":"api/inits/add_jumps!/#add_jumps!","page":"add_jumps!","title":"add_jumps!","text":"","category":"section"},{"location":"api/inits/add_jumps!/#ReservoirComputing.add_jumps!","page":"add_jumps!","title":"ReservoirComputing.add_jumps!","text":"add_jumps!([rng], reservoir_matrix, weight, jump_size;\n    sampling_type=:no_sample, irrational=pi, start=1,\n    positive_prob=0.5)\n\nAdds jumps to a given reservoir_matrix with chosen weight and determined jump_size. weight can be either a number or an array.\n\nArguments\n\nrng: Random number generator. Default is Utils.default_rng() from WeightInitializers.\nreservoir_matrix: matrix to be changed.\nweight: weight to add as a simple cycle. Can be either a single number or an array.\njump_size: size of the jump's distance.\n\nKeyword arguments\n\nsampling_type: Sampling that decides the distribution of weight negative numbers. If set to :no_sample the sign is unchanged. If set to :bernoulli_sample! then each weight can be positive with a probability set by positive_prob. If set to :irrational_sample! the weight is negative if the decimal number of the irrational number chosen is odd. If set to :regular_sample!, each weight will be assigned a negative sign after the chosen strides. strides can be a single number or an array. Default is :no_sample.\npositive_prob: probability of the weight being positive when sampling_type is set to :bernoulli_sample!. Default is 0.5.\nirrational: Irrational number whose decimals decide the sign of weight. Default is pi.\nstart: Which place after the decimal point the counting starts for the irrational sign counting. Default is 1.\nstrides: number of strides for assigning negative value to a weight. It can be an integer or an array. Default is 2.\n\nExamples\n\njulia> matrix = zeros(Float32, 5, 5)\n5×5 Matrix{Float32}:\n 0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n\njulia> add_jumps!(matrix, 1.0)\n5×5 Matrix{Float32}:\n  0.0  0.0   1.0   0.0   0.0\n  0.0  0.0   0.0   0.0   0.0\n  1.0  0.0   0.0   0.0   0.0\n  0.0  0.0   0.0   0.0   1.0\n  0.0  0.0   1.0   0.0   0.0\n\n\n\n\n\n","category":"function"},{"location":"api/inits/logistic_mapping/#logistic_mapping","page":"logistic_mapping","title":"logistic_mapping","text":"","category":"section"},{"location":"api/inits/logistic_mapping/#References","page":"logistic_mapping","title":"References","text":"Wang, H.; Liu, Y.; Lu, P.; Luo, Y.; Wang, D. and Xu, X. (2022). Echo state network with logistic mapping and bias dropout for time series prediction. Neurocomputing 489, 196–210.\n\n\n\n","category":"section"},{"location":"api/inits/logistic_mapping/#ReservoirComputing.logistic_mapping","page":"logistic_mapping","title":"ReservoirComputing.logistic_mapping","text":"logistic_mapping([rng], [T], dims...;\n    amplitude=0.3, sine_divisor=5.9, logistic_parameter=3.7,\n    return_sparse=false)\n\nGenerate an input weight matrix using a logistic mapping (Wang et al., 2022) The first row is initialized using a sine function:\n\n    W1 j = textamplitude cdot sin(j cdot pi \n        (textsine_divisor cdot in_size))\n\nfor each input index j, with in_size being the number of columns provided in dims. Subsequent rows are generated recursively using the logistic map recurrence:\n\n    Wi+1 j = textlogistic_parameter cdot W(i j) cdot (1 - Wi j)\n\nArguments\n\nrng: Random number generator. Default is Utils.default_rng() from WeightInitializers.\nT: Type of the elements in the reservoir matrix. Default is Float32.\ndims: Dimensions of the matrix. Should follow res_size x in_size.\n\nKeyword arguments\n\namplitude: Scaling parameter used in the sine initialization of the first row. Default is 0.3.\nsine_divisor: Parameter used to adjust the phase in the sine initialization. Default is 5.9.\nlogistic_parameter: The parameter in the logistic mapping recurrence that governs the dynamics. Default is 3.7.\nreturn_sparse: If true, returns the resulting matrix as a sparse matrix. Default is false.\n\nExamples\n\njulia> logistic_mapping(8, 3)\n8×3 Matrix{Float32}:\n 0.0529682  0.104272  0.1523\n 0.185602   0.345578  0.477687\n 0.559268   0.836769  0.923158\n 0.912003   0.50537   0.262468\n 0.296938   0.924893  0.716241\n 0.772434   0.257023  0.751987\n 0.650385   0.70656   0.69006\n 0.841322   0.767132  0.791346\n\n\n\n\n\n\n","category":"function"},{"location":"api/states/#States-Modifications","page":"States","title":"States Modifications","text":"","category":"section"},{"location":"api/states/#References","page":"States","title":"References","text":"Barbosa, W. A.; Griffith, A.; Rowlands, G. E.; Govia, L. C.; Ribeill, G. J.; Nguyen, M.-H.; Ohki, T. A. and Gauthier, D. J. (2021). Symmetry-aware reservoir computing. Physical Review E 104.\n\n\n\nChattopadhyay, A.; Hassanzadeh, P. and Subramanian, D. (2020). Data-driven predictions of a multiscale Lorenz 96 chaotic system using machine-learning methods: reservoir computing,  artificial neural network,  and long short-term memory network. Nonlinear Processes in Geophysics 27, 373–389.\n\n\n\nHerteux, J. and Räth, C. (2020). Breaking symmetries of the reservoir equations in echo state networks. Chaos: An Interdisciplinary Journal of Nonlinear Science 30.\n\n\n\nPathak, J.; Lu, Z.; Hunt, B. R.; Girvan, M. and Ott, E. (2017). Using machine learning to replicate chaotic attractors and calculate Lyapunov exponents from data. Chaos: An Interdisciplinary Journal of Nonlinear Science 27.\n\n\n\n","category":"section"},{"location":"api/states/#ReservoirComputing.Pad","page":"States","title":"ReservoirComputing.Pad","text":"Pad(padding=1.0)\n\nPadding layer that appends a constant value to the state (and hence to the layer output).\n\ntildex = beginbmatrix x  textpadding endbmatrix\n\nArguments\n\npadding: value to append. Default is 1.0.\n\nForward\n\npad(state)\n\nArguments\n\nstate: The reservoir computing state.\n\nReturns\n\nA vector or matrix with chosen padding added, thus increasing the size by 1.\n\nExamples\n\njulia> pad = Pad(1.0)\n(::Pad{Float64}) (generic function with 2 methods)\n\njulia> x_old = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n10-element Vector{Int64}:\n 0\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n\njulia> x_new = pad(x_old)\n11-element Vector{Int64}:\n 0\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n 1\n\njulia> mat_old = [1  2  3;\n                   4  5  6;\n                   7  8  9;\n                  10 11 12;\n                  13 14 15;\n                  16 17 18;\n                  19 20 21]\n7×3 Matrix{Int64}:\n  1   2   3\n  4   5   6\n  7   8   9\n 10  11  12\n 13  14  15\n 16  17  18\n 19  20  21\n\n\n julia> mat_new = pad(mat_old)\n 8×3 Matrix{Int64}:\n   1   2   3\n   4   5   6\n   7   8   9\n  10  11  12\n  13  14  15\n  16  17  18\n  19  20  21\n   1   1   1\n\n\n\n\n\n","category":"type"},{"location":"api/states/#ReservoirComputing.Extend","page":"States","title":"ReservoirComputing.Extend","text":"Extend(op)\n\nWrapper layer that concatenates the reservoir state produced by op with the input that Extend receives.\n\nFor an input vector or matrix x and a wrapped layer producing state s, Extend computes:\n\nbeginbmatrix\nx \ns\nendbmatrix\n\nArguments\n\nop: the wrapped layer whose output state will be concatenated with the input.\n\nExamples\n\nesn = ReservoirChain(\n    Extend(\n        StatefulLayer(\n        ESNCell(\n        3 => 300; init_reservoir = rand_sparse(; radius = 1.2, sparsity = 6 / 300))\n    )\n    ),\n    NLAT2(),\n    LinearReadout(300 + 3 => 3)\n)\n\nIn this esample the input to Extend is the initial value fed to ReservoirChain. After Extend, the value in the chain will be the state returned by the StatefulLayer, vcated with the input.\n\n\n\n\n\n","category":"type"},{"location":"api/states/#ReservoirComputing.NLAT1","page":"States","title":"ReservoirComputing.NLAT1","text":"NLAT1()\n\nNLAT1 implements the T₁ transformation algorithm introduced in (Chattopadhyay et al., 2020) and (Pathak et al., 2017). The T₁ algorithm squares elements of the input array, targeting every second row.\n\ntilder_ij =\nbegincases\n    r_ij times r_ij  textif  j text is odd \n    r_ij  textif  j text is even\nendcases\n\nArguments\n\nNone\n\nForward\n\nnlat1(state)\n\nArguments\n\nstate: The reservoir computing state.\n\nReturns\n\nA vector or matrix with transformed elements according to NLAT1, with same dimensionality as the original.\n\nExample\n\njulia> nlat1 = NLAT1()\nNLAT1 (generic function with 3 methods)\n\njulia> x_old = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n10-element Vector{Int64}:\n 0\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n\njulia> n_new = nlat1(x_old)\n10-element Vector{Int64}:\n  0\n  1\n  4\n  3\n 16\n  5\n 36\n  7\n 64\n  9\n\njulia> mat_old = [1  2  3;\n                   4  5  6;\n                   7  8  9;\n                  10 11 12;\n                  13 14 15;\n                  16 17 18;\n                  19 20 21]\n7×3 Matrix{Int64}:\n  1   2   3\n  4   5   6\n  7   8   9\n 10  11  12\n 13  14  15\n 16  17  18\n 19  20  21\n\njulia> mat_new = nlat1(mat_old)\n7×3 Matrix{Int64}:\n   1    4    9\n   4    5    6\n  49   64   81\n  10   11   12\n 169  196  225\n  16   17   18\n 361  400  441\n\n\n\n\n\n\n","category":"function"},{"location":"api/states/#ReservoirComputing.NLAT2","page":"States","title":"ReservoirComputing.NLAT2","text":"NLAT2()\n\nNLAT2 implements the T₂ transformation algorithm as defined in (Chattopadhyay et al., 2020). This transformation algorithm modifies the reservoir states by multiplying each odd-indexed row (starting from the second row) with the product of its two preceding rows.\n\ntilder_ij =\nbegincases\n    r_ij-1 times r_ij-2  textif  j  1 text is odd \n    r_ij  textif  j text is 1 or even\nendcases\n\nArguments\n\nNone\n\nForward\n\nnlat2(state)\n\nArguments\n\nstate: The reservoir computing state.\n\nReturns\n\nA vector or matrix with transformed elements according to NLAT2, with same dimensionality as the original.\n\nExample\n\njulia> nlat2 = NLAT2()\nNLAT2 (generic function with 3 methods)\n\njulia> x_old = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n10-element Vector{Int64}:\n 0\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n\njulia> n_new = nlat2(x_old)\n10-element Vector{Int64}:\n  0\n  1\n  0\n  3\n  6\n  5\n 20\n  7\n 42\n  9\n\n\njulia> mat_old = [1  2  3;\n                   4  5  6;\n                   7  8  9;\n                  10 11 12;\n                  13 14 15;\n                  16 17 18;\n                  19 20 21]\n7×3 Matrix{Int64}:\n  1   2   3\n  4   5   6\n  7   8   9\n 10  11  12\n 13  14  15\n 16  17  18\n 19  20  21\n\njulia> mat_new = nlat2(mat_old)\n7×3 Matrix{Int64}:\n  1   2    3\n  4   5    6\n  4  10   18\n 10  11   12\n 70  88  108\n 16  17   18\n 19  20   21\n\n\n\n\n\n\n","category":"function"},{"location":"api/states/#ReservoirComputing.NLAT3","page":"States","title":"ReservoirComputing.NLAT3","text":"NLAT3(x)\n\nImplements the T₃ transformation algorithm as detailed in (Chattopadhyay et al., 2020). This algorithm modifies the reservoir's states by multiplying each odd-indexed row (beginning from the second row) with the product of the immediately preceding and the immediately following rows.\n\ntilder_ij =\nbegincases\nr_ij-1 times r_ij+1  textif  j  1 text is odd \nr_ij  textif  j = 1 text or even\nendcases\n\nArguments\n\nNone\n\nForward\n\nnlat3(state)\n\nArguments\n\nstate: The reservoir computing state.\n\nReturns\n\nA vector or matrix with transformed elements according to NLAT3, with same dimensionality as the original.\n\nExample\n\njulia> nlat2 = NLAT3()\nNLAT3 (generic function with 3 methods)\n\njulia> x_old = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n10-element Vector{Int64}:\n 0\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n\njulia> n_new = nlat2(x_old)\n10-element Vector{Int64}:\n  0\n  1\n  3\n  3\n 15\n  5\n 35\n  7\n 63\n  9\n\n\njulia> mat_old = [1  2  3;\n                   4  5  6;\n                   7  8  9;\n                  10 11 12;\n                  13 14 15;\n                  16 17 18;\n                  19 20 21]\n7×3 Matrix{Int64}:\n  1   2   3\n  4   5   6\n  7   8   9\n 10  11  12\n 13  14  15\n 16  17  18\n 19  20  21\n\njulia> mat_new = nlat2(mat_old)\n7×3 Matrix{Int64}:\n   1    2    3\n   4    5    6\n  40   55   72\n  10   11   12\n 160  187  216\n  16   17   18\n  19   20   21\n\n\n\n\n\n\n","category":"function"},{"location":"api/states/#ReservoirComputing.PartialSquare","page":"States","title":"ReservoirComputing.PartialSquare","text":"PartialSquare(eta)\n\nImplement a partial squaring of the states as described in (Barbosa et al., 2021).\n\n    beginequation\n    g(r_i) =\n    begincases\n        r_i^2  textif  i leq eta_r N \n        r_i  textif  i  eta_r N\n    endcases\n    endequation\n\nArguments\n\neta: Percentage of elements of the input vector to be squared.\n\nForward\n\npartialsq(state)\n\nArguments\n\nstate: The reservoir computing state.\n\nReturns\n\nA vector or matrix with partial square components, with same dimensionality as the original.\n\nExample\n\njulia> partialsq = PartialSquare(0.6)\nPartialSquare(0.6)\n\njulia> x_old = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n10-element Vector{Int64}:\n 0\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n\njulia> x_new = partialsq(x_old)\n10-element Vector{Int64}:\n  0\n  1\n  4\n  9\n 16\n 25\n  6\n  7\n  8\n  9\n\n\n\n\n\n","category":"type"},{"location":"api/states/#ReservoirComputing.ExtendedSquare","page":"States","title":"ReservoirComputing.ExtendedSquare","text":"ExtendedSquare()\n\nExtension of the Lu initialization proposed in (Herteux and Räth, 2020). The state vector is extended with the squared elements of the initial state.\n\nbeginequation\n    vecx = x_1 x_2 dots x_N x_1^2 x_2^2 dots x_N^2\nendequation\n\nArguments\n\nNone\n\nForward\n\nextendedsq(state)\n\nArguments\n\nstate: The reservoir computing state.\n\nReturns\n\nA vector or matrix with the original elements concatenated with the squared elements. Dimensionality is double of the original.\n\nExample\n\njulia> extendedsq = ExtendedSquare()\nExtendedSquare()\n\njulia> x_old = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n9-element Vector{Int64}:\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n\njulia> x_new = extendedsq(x_old)\n18-element Vector{Int64}:\n  1\n  2\n  3\n  4\n  5\n  6\n  7\n  8\n  9\n  1\n  4\n  9\n 16\n 25\n 36\n 49\n 64\n 81\n\n\n\n\n\n\n","category":"function"},{"location":"api/inits/delay_line/#delay_line","page":"delay_line","title":"delay_line","text":"","category":"section"},{"location":"api/inits/delay_line/#References","page":"delay_line","title":"References","text":"Rodan, A. and Tino, P. (2011). Minimum Complexity Echo State Network. IEEE Transactions on Neural Networks 22, 131–144.\n\n\n\n","category":"section"},{"location":"api/inits/delay_line/#ReservoirComputing.delay_line","page":"delay_line","title":"ReservoirComputing.delay_line","text":"delay_line([rng], [T], dims...;\n    delay_weight=0.1, delay_shift=1,\n    return_sparse=false, radius=nothing, kwargs...)\n\nCreate and return a delay line reservoir matrix (Rodan and Tino, 2011).\n\nW_ij =\nbegincases\n    r  textif  i = j + 1 j in 1 D_mathrmres - 1 6pt\n    0  textotherwise\nendcases\n\nArguments\n\nrng: Random number generator. Default is Utils.default_rng()from WeightInitializers.\nT: Type of the elements in the reservoir matrix. Default is Float32.\ndims: Dimensions of the reservoir matrix.\n\nKeyword arguments\n\ndelay_weight: Determines the value of all connections in the reservoir. This can be provided as a single value or an array. In case it is provided as an array please make sure that the length of the array matches the length of the sub-diagonal you want to populate. Default is 0.1.\ndelay_shift: delay line shift. Default is 1.\nradius: The desired spectral radius of the reservoir. If nothing is passed, no scaling takes place. Defaults to nothing.\nreturn_sparse: flag for returning a sparse matrix. true requires SparseArrays to be loaded. Default is false.\nsampling_type: Sampling that decides the distribution of weight negative numbers. If set to :no_sample the sign is unchanged. If set to :bernoulli_sample! then each weight can be positive with a probability set by positive_prob. If set to :irrational_sample! the weight is negative if the decimal number of the irrational number chosen is odd. If set to :regular_sample!, each weight will be assigned a negative sign after the chosen strides. strides can be a single number or an array. Default is :no_sample.\npositive_prob: probability of the weight being positive when sampling_type is set to :bernoulli_sample!. Default is 0.5.\nirrational: Irrational number whose decimals decide the sign of weight. Default is pi.\nstart: Which place after the decimal point the counting starts for the irrational sign counting. Default is 1.\nstrides: number of strides for assigning negative value to a weight. It can be an integer or an array. Default is 2.\n\nExamples\n\nDefault call:\n\njulia> res_matrix = delay_line(5, 5)\n5×5 Matrix{Float32}:\n 0.0  0.0  0.0  0.0  0.0\n 0.1  0.0  0.0  0.0  0.0\n 0.0  0.1  0.0  0.0  0.0\n 0.0  0.0  0.1  0.0  0.0\n 0.0  0.0  0.0  0.1  0.0\n\nChanging weights:\n\njulia> res_matrix = delay_line(5, 5; delay_weight = 1)\n5×5 Matrix{Float32}:\n 0.0  0.0  0.0  0.0  0.0\n 1.0  0.0  0.0  0.0  0.0\n 0.0  1.0  0.0  0.0  0.0\n 0.0  0.0  1.0  0.0  0.0\n 0.0  0.0  0.0  1.0  0.0\n\nChanging weights to a custom array:\n\njulia> res_matrix = delay_line(5, 5; delay_weight = rand(Float32, 4))\n5×5 Matrix{Float32}:\n 0.0       0.0       0.0      0.0        0.0\n 0.398408  0.0       0.0      0.0        0.0\n 0.0       0.624473  0.0      0.0        0.0\n 0.0       0.0       0.66302  0.0        0.0\n 0.0       0.0       0.0      0.0780818  0.0\n\nChanging sign of the weights with different samplings:\n\njulia> res_matrix = delay_line(5, 5; sampling_type=:irrational_sample!)\n5×5 Matrix{Float32}:\n 0.0  0.0   0.0   0.0  0.0\n -0.1  0.0   0.0   0.0  0.0\n 0.0  0.1   0.0   0.0  0.0\n 0.0  0.0  -0.1   0.0  0.0\n 0.0  0.0   0.0  -0.1  0.0\n\njulia> res_matrix = delay_line(5, 5; sampling_type=:bernoulli_sample!)\n5×5 Matrix{Float32}:\n 0.0   0.0  0.0   0.0  0.0\n 0.1   0.0  0.0   0.0  0.0\n 0.0  -0.1  0.0   0.0  0.0\n 0.0   0.0  0.1   0.0  0.0\n 0.0   0.0  0.0  -0.1  0.0\n\nShifting the delay line:\n\njulia> res_matrix = delay_line(5, 5; delay_shift = 3)\n5×5 Matrix{Float32}:\n 0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n 0.1  0.0  0.0  0.0  0.0\n 0.0  0.1  0.0  0.0  0.0\n\nReturning as sparse:\n\njulia> using SparseArrays\n\njulia> res_matrix = delay_line(5, 5; return_sparse=true)\n5×5 SparseMatrixCSC{Float32, Int64} with 4 stored entries:\n  ⋅    ⋅    ⋅    ⋅    ⋅\n 0.1   ⋅    ⋅    ⋅    ⋅\n  ⋅   0.1   ⋅    ⋅    ⋅\n  ⋅    ⋅   0.1   ⋅    ⋅\n  ⋅    ⋅    ⋅   0.1   ⋅\n\n\n\n\n\n","category":"function"},{"location":"getting_started/#Getting-Started-with-ReservoirComputing.jl","page":"Getting Started with ReservoirComputing.jl","title":"Getting Started with ReservoirComputing.jl","text":"This is an introductory tutorial for ReservoirComputing.jl. We will showcase a typical usecase by creating an Echo State Network (ESN) and training it to reproduce the dynamics of the chaotic Lorenz system.","category":"section"},{"location":"getting_started/#Installing-ReservoirComputing","page":"Getting Started with ReservoirComputing.jl","title":"Installing ReservoirComputing","text":"ReservoirComputing.jl is registered in general registry, so it can be installed through the Julia package manager:\n\nusing Pkg\nPkg.add(\"ReservoirComputing\")","category":"section"},{"location":"getting_started/#Copy-Pastable-Simplified-Example","page":"Getting Started with ReservoirComputing.jl","title":"Copy-Pastable Simplified Example","text":"If you wish to just get some code running to get started, the following code block provides an end=to-end simplified runnable example. The rest of this page will delve into more details, expanding on various aspects of the example.\n\nusing OrdinaryDiffEq\nusing Plots\nusing Random\nusing ReservoirComputing\n\nRandom.seed!(42)\nrng = MersenneTwister(17)\n\nfunction lorenz(du, u, p, t)\n    du[1] = p[1] * (u[2] - u[1])\n    du[2] = u[1] * (p[2] - u[3]) - u[2]\n    du[3] = u[1] * u[2] - p[3] * u[3]\nend\n\nprob = ODEProblem(lorenz, [1.0f0, 0.0f0, 0.0f0], (0.0, 200.0), [10.0f0, 28.0f0, 8/3])\ndata = Array(solve(prob, ABM54(); dt=0.02))\nshift = 300\ntrain_len = 5000\npredict_len = 1250\n\ninput_data = data[:, shift:(shift + train_len - 1)]\ntarget_data = data[:, (shift + 1):(shift + train_len)]\ntest = data[:, (shift + train_len):(shift + train_len + predict_len - 1)]\n\nesn = ESN(3, 300, 3; init_reservoir=rand_sparse(; radius=1.2, sparsity=6/300),\n    state_modifiers=NLAT2)\n\nps, st = setup(rng, esn)\nps, st = train!(esn, input_data, target_data, ps, st)\noutput, st = predict(esn, predict_len, ps, st; initialdata=test[:, 1])\n\nplot(transpose(output)[:, 1], transpose(output)[:, 2], transpose(output)[:, 3];\n    label=\"predicted\")\nplot!(transpose(test)[:, 1], transpose(test)[:, 2], transpose(test)[:, 3];\n    label=\"actual\")\n\nCongrats, you trained your first ESN! Now let's go into more detail.","category":"section"},{"location":"getting_started/#Generating-the-data","page":"Getting Started with ReservoirComputing.jl","title":"Generating the data","text":"Starting off the workflow, the first step is to obtain the data. We use OrdinaryDiffEq to derive the Lorenz system data. The data is passed to the model as a matrix, where the rows are the features and the columns represent the time steps.\n\nusing OrdinaryDiffEq\n\n#define lorenz system\nfunction lorenz!(du, u, p, t)\n    du[1] = 10.0 * (u[2] - u[1])\n    du[2] = u[1] * (28.0 - u[3]) - u[2]\n    du[3] = u[1] * u[2] - (8 / 3) * u[3]\nend\n\n#solve and take data\nprob = ODEProblem(lorenz!, [1.0, 0.0, 0.0], (0.0, 200.0))\ndata = solve(prob, ABM54(); dt=0.02)\ndata = reduce(hcat, data.u)\n\nNow we split the data in training and testing. To do an autoregressive forecast we want the model to be trained on the next step, so we are going to shift the target data by one. Additionally, we discard the transient period.\n\n#determine shift length, training length and prediction length\nshift = 300\ntrain_len = 5000\npredict_len = 1250\n\n#split the data accordingly\ninput_data = data[:, shift:(shift + train_len - 1)]\ntarget_data = data[:, (shift + 1):(shift + train_len)]\ntest_data = data[:, (shift + train_len + 1):(shift + train_len + predict_len)]\n\nIt is important to notice that the data needs to be formatted in a matrix with the features as rows and time steps as columns as in this example. This is needed even if the time series consists of single values (ie, the data can be represented by a vector).","category":"section"},{"location":"getting_started/#Building-the-Echo-State-Network","page":"Getting Started with ReservoirComputing.jl","title":"Building the Echo State Network","text":"Once the data is ready, it is possible to define the parameters for the ESN and the ESN struct itself. In this example, the values from (Pathak et al., 2017) are loosely followed as general guidelines.\n\nusing ReservoirComputing\n\n#define ESN parameters\nres_size = 300\nin_size = 3\nres_radius = 1.2\nres_sparsity = 6 / 300\ninput_scaling = 0.1\n\n#build ESN struct\nesn = ESN(in_size, res_size, in_size; #autoregressive so in_size = out_size\n    init_reservoir = rand_sparse(; radius = res_radius, sparsity = res_sparsity),\n    init_input = weighted_init(; scaling = input_scaling),\n    state_modifiers = NLAT2,\n    leak_coefficient=1.0, # default value\n    init_state = randn32, # default value\n    use_bias=false, # default value\n    init_bias = zeros32, # default value, not used since use_bias=false\n    readout_activation=identity, # default value\n)\n\nIn this case, a size of 300 has been chosen, so the reservoir matrix will be 300 x 300. However, this is not always the case, since some input layer constructions can modify the dimensions of the reservoir. Please make sure to read the API documentation of the initializer you intend to use if you think that is cause of errors.\n\nThe res_radius determines the scaling of the spectral radius of the reservoir matrix; a proper scaling is necessary to assure the echo state property. The default value in the rand_sparse method is 1.0 in accordance with the most commonly followed guidelines found in the literature (see (Lukoševičius, 2012) and references therein).\n\nThe value of input_scaling determines the upper and lower bounds of the uniform distribution of the weights in the weighted_init. The value of 0.1 represents the default. The default input layer is the scaled_rand, a dense matrix. The details of the weighted version can be found in (Lu et al., 2017). For this example, this version returns the best results.","category":"section"},{"location":"getting_started/#Training-and-Prediction","page":"Getting Started with ReservoirComputing.jl","title":"Training and Prediction","text":"Training for ESNs usually means solving a linear regression. The library supports solvers from 'MLILinearModels.jl', in addition to a custom implementation of ridge regression StandardRidge. In this example we will use the latter.\n\nSince ReservoirComputing.jl builds on LuxCore.jl we first need to setup the state and the parameters\n\nusing Random\nRandom.seed!(42)\nrng = MersenneTwister(17)\n\nps, st = setup(rng, esn)\n\nNow we can proceed with training the ESN model. Usually an initial transient is discarded, to account for the dynamics of the ESN to settle. This can be done by passing the washout keyword argument to train.\n\n#define training method\ntraining_method = StandardRidge(0.0)\n\nps, st = train!(esn, input_data, target_data, ps, st, training_method;\n    washout = 0 # we use no washout\n)\n\nps now contains the trained parameters for the ESN.\n\ninfo: Returning training states\nThe ESN states are internally used the training, however they are not returned by default. To inspect the states, it is necessary to set the boolean keyword argument return_states as true in the train! call.(ps, st), states = train!(esn, input_data, target_data, ps, st, training_method;\n    return_states = true\n)\n\nReservoirComputing.jl provides additional utilities functions for autoregressive forecasting:\n\noutput, st = predict(esn, predict_len, ps, st; initialdata=test_data[:, 1])\n\nTo inspect the results, they can easily be plotted using an external library. In this case, we will use Plots.jl:\n\nusing Plots, Plots.PlotMeasures\n\nts = 0.0:0.02:200.0\nlorenz_maxlyap = 0.9056\npredict_ts = ts[(shift + train_len + 1):(shift + train_len + predict_len)]\nlyap_time = (predict_ts .- predict_ts[1]) * (1 / lorenz_maxlyap)\n\np1 = plot(lyap_time, [test_data[1, :] output[1, :]]; label=[\"actual\" \"predicted\"],\n    ylabel=\"x(t)\", linewidth=2.5, xticks=false, yticks=-15:15:15);\np2 = plot(lyap_time, [test_data[2, :] output[2, :]]; label=[\"actual\" \"predicted\"],\n    ylabel=\"y(t)\", linewidth=2.5, xticks=false, yticks=-20:20:20);\np3 = plot(lyap_time, [test_data[3, :] output[3, :]]; label=[\"actual\" \"predicted\"],\n    ylabel=\"z(t)\", linewidth=2.5, xlabel=\"max(λ)*t\", yticks=10:15:40);\n\nplot(p1, p2, p3; plot_title=\"Lorenz System Coordinates\",\n    layout=(3, 1), xtickfontsize=12, ytickfontsize=12, xguidefontsize=15,\n    yguidefontsize=15,\n    legendfontsize=12, titlefontsize=20)","category":"section"},{"location":"getting_started/#References","page":"Getting Started with ReservoirComputing.jl","title":"References","text":"Lu, Z.; Pathak, J.; Hunt, B.; Girvan, M.; Brockett, R. and Ott, E. (2017). Reservoir observers: Model-free inference of unmeasured variables in chaotic systems. Chaos: An Interdisciplinary Journal of Nonlinear Science 27.\n\n\n\nLukoševičius, M. (2012). A Practical Guide to Applying Echo State Networks. In: Neural Networks: Tricks of the Trade (Springer Berlin Heidelberg); pp. 659–686.\n\n\n\nPathak, J.; Lu, Z.; Hunt, B. R.; Girvan, M. and Ott, E. (2017). Using machine learning to replicate chaotic attractors and calculate Lyapunov exponents from data. Chaos: An Interdisciplinary Journal of Nonlinear Science 27.\n\n\n\n","category":"section"},{"location":"api/inits/scale_radius!/#scale_radius!","page":"scale_radius!","title":"scale_radius!","text":"","category":"section"},{"location":"api/inits/scale_radius!/#ReservoirComputing.scale_radius!","page":"scale_radius!","title":"ReservoirComputing.scale_radius!","text":"scale_radius!(matrix, radius)\n\nScale the spectral radius of the given matrix to be equal to the given radius\n\nArguments\n\nmatrix: Matrix to be scaled.\nradius: desidered radius to scale the given matrix to\n\n\n\n\n\n","category":"function"}]
}
