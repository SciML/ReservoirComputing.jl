var documenterSearchIndex = {"docs":
[{"location":"general/predictive_generative/#Generative-vs-Predictive","page":"Generative vs Predictive","title":"Generative vs Predictive","text":"","category":"section"},{"location":"general/predictive_generative/","page":"Generative vs Predictive","title":"Generative vs Predictive","text":"The library provides two different methods for prediction, denoted as Predictive() and Generative(). These methods correspond to the two major applications of Reservoir Computing models found in the literature. This section aims to clarify the differences between these two methods before providing further details on their usage in the library.","category":"page"},{"location":"general/predictive_generative/#Predictive","page":"Generative vs Predictive","title":"Predictive","text":"","category":"section"},{"location":"general/predictive_generative/","page":"Generative vs Predictive","title":"Generative vs Predictive","text":"In the first method, users can utilize Reservoir Computing models in a manner similar to standard Machine Learning models. This involves using a set of features as input and a set of labels as outputs. In this case, both the feature and label sets can consist of vectors of different dimensions. Specifically, let's denote the feature set as X=x_1x_n where x_i in mathbbR^N, and the label set as Y=y_1y_n where y_i in mathbbR^M.","category":"page"},{"location":"general/predictive_generative/","page":"Generative vs Predictive","title":"Generative vs Predictive","text":"To make predictions using this method, you need to provide the feature set that you want to predict the labels for. For example, you can call Predictive(X) using the feature set X as input. This method allows for both one-step-ahead and multi-step-ahead predictions.","category":"page"},{"location":"general/predictive_generative/#Generative","page":"Generative vs Predictive","title":"Generative","text":"","category":"section"},{"location":"general/predictive_generative/","page":"Generative vs Predictive","title":"Generative vs Predictive","text":"The generative method provides a different approach to forecasting with Reservoir Computing models. It enables you to extend the forecasting capabilities of the model by allowing predicted results to be fed back into the model to generate the next prediction. This autonomy allows the model to make predictions without the need for a feature dataset as input.","category":"page"},{"location":"general/predictive_generative/","page":"Generative vs Predictive","title":"Generative vs Predictive","text":"To use the generative method, you only need to specify the number of time steps that you intend to forecast. For instance, you can call Generative(100) to generate predictions for the next one hundred time steps.","category":"page"},{"location":"general/predictive_generative/","page":"Generative vs Predictive","title":"Generative vs Predictive","text":"The key distinction between these methods lies in how predictions are made. The predictive method relies on input feature sets to make predictions, while the generative method allows for autonomous forecasting by feeding predicted results back into the model.","category":"page"},{"location":"reca_tutorials/reca/#Reservoir-Computing-using-Cellular-Automata","page":"Reservoir Computing with Cellular Automata","title":"Reservoir Computing using Cellular Automata","text":"","category":"section"},{"location":"reca_tutorials/reca/","page":"Reservoir Computing with Cellular Automata","title":"Reservoir Computing with Cellular Automata","text":"Reservoir Computing based on Elementary Cellular Automata (ECA) has been recently introduced. Dubbed as ReCA [1][2] it proposed the advantage of storing the reservoir states as binary data. Less parameter tuning represents another advantage of this model. The architecture implemented in ReservoirComputing.jl follows [3] which builds on top of the original implementation, improving the results. It is strongly suggested to go through the paper to get a solid understanding of the model before delving into experimentation with the code.","category":"page"},{"location":"reca_tutorials/reca/","page":"Reservoir Computing with Cellular Automata","title":"Reservoir Computing with Cellular Automata","text":"To showcase how to use these models, this page illustrates the performance of ReCA in the 5 bit memory task [4]. The script for the example and companion data can be found here.","category":"page"},{"location":"reca_tutorials/reca/#5-bit-memory-task","page":"Reservoir Computing with Cellular Automata","title":"5 bit memory task","text":"","category":"section"},{"location":"reca_tutorials/reca/","page":"Reservoir Computing with Cellular Automata","title":"Reservoir Computing with Cellular Automata","text":"The data can be read as follows:","category":"page"},{"location":"reca_tutorials/reca/","page":"Reservoir Computing with Cellular Automata","title":"Reservoir Computing with Cellular Automata","text":"using DelimitedFiles\n\ninput = readdlm(\"./5bitinput.txt\", ',', Float64)\noutput = readdlm(\"./5bitoutput.txt\", ',', Float64)","category":"page"},{"location":"reca_tutorials/reca/","page":"Reservoir Computing with Cellular Automata","title":"Reservoir Computing with Cellular Automata","text":"To use a ReCA model, it is necessary to define the rule one intends to use. To do so, ReservoirComputing.jl leverages CellularAutomata.jl that needs to be called as well to define the RECA struct:","category":"page"},{"location":"reca_tutorials/reca/","page":"Reservoir Computing with Cellular Automata","title":"Reservoir Computing with Cellular Automata","text":"using ReservoirComputing, CellularAutomata\n\nca = DCA(90)","category":"page"},{"location":"reca_tutorials/reca/","page":"Reservoir Computing with Cellular Automata","title":"Reservoir Computing with Cellular Automata","text":"To define the ReCA model, it suffices to call:","category":"page"},{"location":"reca_tutorials/reca/","page":"Reservoir Computing with Cellular Automata","title":"Reservoir Computing with Cellular Automata","text":"reca = RECA(input, ca;\n    generations=16,\n    input_encoding=RandomMapping(16, 40))","category":"page"},{"location":"reca_tutorials/reca/","page":"Reservoir Computing with Cellular Automata","title":"Reservoir Computing with Cellular Automata","text":"After this, the training can be performed with the chosen method.","category":"page"},{"location":"reca_tutorials/reca/","page":"Reservoir Computing with Cellular Automata","title":"Reservoir Computing with Cellular Automata","text":"output_layer = train(reca, output, StandardRidge(0.00001))","category":"page"},{"location":"reca_tutorials/reca/","page":"Reservoir Computing with Cellular Automata","title":"Reservoir Computing with Cellular Automata","text":"The prediction in this case will be a Predictive() with the input data equal to the training data. In addition, to test the 5 bit memory task, a conversion from Float to Bool is necessary (at the moment, we are aware of a bug that doesn't allow boolean input data to the RECA models):","category":"page"},{"location":"reca_tutorials/reca/","page":"Reservoir Computing with Cellular Automata","title":"Reservoir Computing with Cellular Automata","text":"prediction = reca(Predictive(input), output_layer)\nfinal_pred = convert(AbstractArray{Float32}, prediction .> 0.5)\n\nfinal_pred == output","category":"page"},{"location":"reca_tutorials/reca/","page":"Reservoir Computing with Cellular Automata","title":"Reservoir Computing with Cellular Automata","text":"[1]: Yilmaz, Ozgur. \"Reservoir computing using cellular automata.\" arXiv preprint arXiv:1410.0162 (2014).","category":"page"},{"location":"reca_tutorials/reca/","page":"Reservoir Computing with Cellular Automata","title":"Reservoir Computing with Cellular Automata","text":"[2]: Margem, Mrwan, and Ozgür Yilmaz. \"An experimental study on cellular automata reservoir in pathological sequence learning tasks.\" (2017).","category":"page"},{"location":"reca_tutorials/reca/","page":"Reservoir Computing with Cellular Automata","title":"Reservoir Computing with Cellular Automata","text":"[3]: Nichele, Stefano, and Andreas Molund. \"Deep reservoir computing using cellular automata.\" arXiv preprint arXiv:1703.02806 (2017).","category":"page"},{"location":"reca_tutorials/reca/","page":"Reservoir Computing with Cellular Automata","title":"Reservoir Computing with Cellular Automata","text":"[4]: Hochreiter, Sepp, and Jürgen Schmidhuber. \"Long short-term memory.\" Neural computation 9.8 (1997): 1735-1780.","category":"page"},{"location":"esn_tutorials/different_drivers/#Using-Different-Reservoir-Drivers","page":"Using Different Reservoir Drivers","title":"Using Different Reservoir Drivers","text":"","category":"section"},{"location":"esn_tutorials/different_drivers/","page":"Using Different Reservoir Drivers","title":"Using Different Reservoir Drivers","text":"While the original implementation of the Echo State Network implemented the model using the equations of Recurrent Neural Networks to obtain non-linearity in the reservoir, other variations have been proposed in recent years. More specifically, the different drivers implemented in ReservoirComputing.jl are the multiple activation function RNN MRNN() and the Gated Recurrent Unit GRU(). To change them, it suffices to give the chosen method to the ESN keyword argument reservoir_driver. In this section, some examples, of their usage will be given, as well as a brief introduction to their equations.","category":"page"},{"location":"esn_tutorials/different_drivers/#Multiple-Activation-Function-RNN","page":"Using Different Reservoir Drivers","title":"Multiple Activation Function RNN","text":"","category":"section"},{"location":"esn_tutorials/different_drivers/","page":"Using Different Reservoir Drivers","title":"Using Different Reservoir Drivers","text":"Based on the double activation function ESN (DAFESN) proposed in [1], the Multiple Activation Function ESN expands the idea and allows a custom number of activation functions to be used in the reservoir dynamics. This can be thought of as a linear combination of multiple activation functions with corresponding parameters.","category":"page"},{"location":"esn_tutorials/different_drivers/","page":"Using Different Reservoir Drivers","title":"Using Different Reservoir Drivers","text":"mathbfx(t+1) = (1-alpha)mathbfx(t) + lambda_1 f_1(mathbfWmathbfx(t)+mathbfW_inmathbfu(t)) + dots + lambda_D f_D(mathbfWmathbfx(t)+mathbfW_inmathbfu(t))","category":"page"},{"location":"esn_tutorials/different_drivers/","page":"Using Different Reservoir Drivers","title":"Using Different Reservoir Drivers","text":"where D is the number of activation functions and respective parameters chosen.","category":"page"},{"location":"esn_tutorials/different_drivers/","page":"Using Different Reservoir Drivers","title":"Using Different Reservoir Drivers","text":"The method to call to use the multiple activation function ESN is MRNN(activation_function, leaky_coefficient, scaling_factor). The arguments can be used as both args and kwargs. activation_function and scaling_factor have to be vectors (or tuples) containing the chosen activation functions and respective scaling factors (f_1f_D and lambda_1lambda_D following the nomenclature introduced above). The leaky_coefficient represents alpha and it is a single value.","category":"page"},{"location":"esn_tutorials/different_drivers/","page":"Using Different Reservoir Drivers","title":"Using Different Reservoir Drivers","text":"Starting with the example, the data used is based on the following function based on the DAFESN paper [1].","category":"page"},{"location":"esn_tutorials/different_drivers/","page":"Using Different Reservoir Drivers","title":"Using Different Reservoir Drivers","text":"u(t) = sin(t) + sin(0.51 * t) + sin(0.22 * t) + sin(0.1002 * t) + sin(0.05343 * t)","category":"page"},{"location":"esn_tutorials/different_drivers/","page":"Using Different Reservoir Drivers","title":"Using Different Reservoir Drivers","text":"For this example, the type of prediction will be one step ahead. The metric used to assure a good prediction will be the normalized root-mean-square deviation rmsd from StatsBase. Like in the other examples, first it is needed to gather the data:","category":"page"},{"location":"esn_tutorials/different_drivers/","page":"Using Different Reservoir Drivers","title":"Using Different Reservoir Drivers","text":"train_len = 3000\npredict_len = 2000\nshift = 1\n\ndata = u.(collect(0.0:0.01:500))\ntraining_input = reduce(hcat, data[shift:(shift + train_len - 1)])\ntraining_target = reduce(hcat, data[(shift + 1):(shift + train_len)])\ntesting_input = reduce(hcat,\n    data[(shift + train_len):(shift + train_len + predict_len - 1)])\ntesting_target = reduce(hcat,\n    data[(shift + train_len + 1):(shift + train_len + predict_len)])","category":"page"},{"location":"esn_tutorials/different_drivers/","page":"Using Different Reservoir Drivers","title":"Using Different Reservoir Drivers","text":"To follow the paper more closely, it is necessary to define a couple of activation functions. The numbering of them follows the ones in the paper. Of course, one can also use any custom-defined function, available in the base language or any activation function from NNlib.","category":"page"},{"location":"esn_tutorials/different_drivers/","page":"Using Different Reservoir Drivers","title":"Using Different Reservoir Drivers","text":"f2(x) = (1 - exp(-x)) / (2 * (1 + exp(-x)))\nf3(x) = (2 / pi) * atan((pi / 2) * x)\nf4(x) = x / sqrt(1 + x * x)","category":"page"},{"location":"esn_tutorials/different_drivers/","page":"Using Different Reservoir Drivers","title":"Using Different Reservoir Drivers","text":"It is now possible to build different drivers, using the parameters suggested by the paper. Also, in this instance, the numbering follows the test cases of the paper. In the end, a simple for loop is implemented to compare the different drivers and activation functions.","category":"page"},{"location":"esn_tutorials/different_drivers/","page":"Using Different Reservoir Drivers","title":"Using Different Reservoir Drivers","text":"using ReservoirComputing, Random, StatsBase\n\n#fix seed for reproducibility\nRandom.seed!(42)\n\n#baseline case with RNN() driver. Parameter given as args\nbase_case = RNN(tanh, 0.85)\n\n#MRNN() test cases\n#Parameter given as kwargs\ncase3 = MRNN(; activation_function=[tanh, f2],\n    leaky_coefficient=0.85,\n    scaling_factor=[0.5, 0.3])\n\n#Parameter given as kwargs\ncase4 = MRNN(; activation_function=[tanh, f3],\n    leaky_coefficient=0.9,\n    scaling_factor=[0.45, 0.35])\n\n#Parameter given as args\ncase5 = MRNN([tanh, f4], 0.9, [0.43, 0.13])\n\n#tests\ntest_cases = [base_case, case3, case4, case5]\nfor case in test_cases\n    esn = ESN(training_input, 1, 100;\n        input_layer=weighted_init(; scaling=0.3),\n        reservoir=rand_sparse(; radius=0.4),\n        reservoir_driver=case,\n        states_type=ExtendedStates())\n    wout = train(esn, training_target, StandardRidge(10e-6))\n    output = esn(Predictive(testing_input), wout)\n    println(rmsd(testing_target, output; normalize=true))\nend","category":"page"},{"location":"esn_tutorials/different_drivers/","page":"Using Different Reservoir Drivers","title":"Using Different Reservoir Drivers","text":"In this example, it is also possible to observe the input of parameters to the methods RNN() MRNN(), both by argument and by keyword argument.","category":"page"},{"location":"esn_tutorials/different_drivers/#Gated-Recurrent-Unit","page":"Using Different Reservoir Drivers","title":"Gated Recurrent Unit","text":"","category":"section"},{"location":"esn_tutorials/different_drivers/","page":"Using Different Reservoir Drivers","title":"Using Different Reservoir Drivers","text":"Gated Recurrent Units (GRUs) [2] have been proposed in more recent years with the intent of limiting notable problems of RNNs, like the vanishing gradient. This change in the underlying equations can be easily transported into the Reservoir Computing paradigm, by switching the RNN equations in the reservoir with the GRU equations. This approach has been explored in [3] and [4]. Different variations of GRU have been proposed [5][6]; this section is subdivided into different sections that go into detail about the governing equations and the implementation of them into ReservoirComputing.jl. Like before, to access the GRU reservoir driver, it suffices to change the reservoir_diver keyword argument for ESN with GRU(). All the variations that will be presented can be used in this package by leveraging the keyword argument variant in the method GRU() and specifying the chosen variant: FullyGated() or Minimal(). Other variations are possible by modifying the inner layers and reservoirs. The default is set to the standard version FullyGated(). The first section will go into more detail about the default of the GRU() method, and the following ones will refer to it to minimize repetitions. This example was run on Julia v1.7.2.","category":"page"},{"location":"esn_tutorials/different_drivers/#Standard-GRU","page":"Using Different Reservoir Drivers","title":"Standard GRU","text":"","category":"section"},{"location":"esn_tutorials/different_drivers/","page":"Using Different Reservoir Drivers","title":"Using Different Reservoir Drivers","text":"The equations for the standard GRU are as follows:","category":"page"},{"location":"esn_tutorials/different_drivers/","page":"Using Different Reservoir Drivers","title":"Using Different Reservoir Drivers","text":"mathbfr(t) = sigma (mathbfW^r_textinmathbfu(t)+mathbfW^rmathbfx(t-1)+mathbfb_r) \nmathbfz(t) = sigma (mathbfW^z_textinmathbfu(t)+mathbfW^zmathbfx(t-1)+mathbfb_z) \ntildemathbfx(t) = texttanh(mathbfW_inmathbfu(t)+mathbfW(mathbfr(t) odot mathbfx(t-1))+mathbfb) \nmathbfx(t) = mathbfz(t) odot mathbfx(t-1)+(1-mathbfz(t)) odot tildemathbfx(t)","category":"page"},{"location":"esn_tutorials/different_drivers/","page":"Using Different Reservoir Drivers","title":"Using Different Reservoir Drivers","text":"Going over the GRU keyword argument, it will be explained how to feed the desired input to the model.","category":"page"},{"location":"esn_tutorials/different_drivers/","page":"Using Different Reservoir Drivers","title":"Using Different Reservoir Drivers","text":"activation_function is a vector with default values [NNlib.sigmoid, NNlib.sigmoid, tanh]. This argument controls the activation functions of the GRU, going from top to bottom. Changing the first element corresponds to changing the activation function for mathbfr(t) and so on.\ninner_layer is a vector with default values fill(DenseLayer(), 2). This keyword argument controls the mathbfW_textins going from top to bottom like before.\nreservoir is a vector with default value fill(RandSparseReservoir(), 2). In a similar fashion to inner_layer, this keyword argument controls the reservoir matrix construction in a top to bottom order.\nbias is again a vector with default value fill(DenseLayer(), 2). It is meant to control the mathbfbs, going as usual from top to bottom.\nvariant controls the GRU variant. The default value is set to FullyGated().","category":"page"},{"location":"esn_tutorials/different_drivers/","page":"Using Different Reservoir Drivers","title":"Using Different Reservoir Drivers","text":"It is important to notice that inner_layer and reservoir control every layer except mathbfW_in and mathbfW and mathbfb. These arguments are given as input to the ESN() call as input_layer, reservoir and bias.","category":"page"},{"location":"esn_tutorials/different_drivers/","page":"Using Different Reservoir Drivers","title":"Using Different Reservoir Drivers","text":"The following sections are going to illustrate the variations of the GRU architecture and how to obtain them in ReservoirComputing.jl","category":"page"},{"location":"esn_tutorials/different_drivers/#Type-1","page":"Using Different Reservoir Drivers","title":"Type 1","text":"","category":"section"},{"location":"esn_tutorials/different_drivers/","page":"Using Different Reservoir Drivers","title":"Using Different Reservoir Drivers","text":"The first variation of the GRU is dependent only on the previous hidden state and the bias:","category":"page"},{"location":"esn_tutorials/different_drivers/","page":"Using Different Reservoir Drivers","title":"Using Different Reservoir Drivers","text":"mathbfr(t) = sigma (mathbfW^rmathbfx(t-1)+mathbfb_r) \nmathbfz(t) = sigma (mathbfW^zmathbfx(t-1)+mathbfb_z) ","category":"page"},{"location":"esn_tutorials/different_drivers/","page":"Using Different Reservoir Drivers","title":"Using Different Reservoir Drivers","text":"To obtain this variation, it will suffice to set inner_layer = fill(NullLayer(), 2) and leaving the variant = FullyGated().","category":"page"},{"location":"esn_tutorials/different_drivers/#Type-2","page":"Using Different Reservoir Drivers","title":"Type 2","text":"","category":"section"},{"location":"esn_tutorials/different_drivers/","page":"Using Different Reservoir Drivers","title":"Using Different Reservoir Drivers","text":"The second variation only depends on the previous hidden state:","category":"page"},{"location":"esn_tutorials/different_drivers/","page":"Using Different Reservoir Drivers","title":"Using Different Reservoir Drivers","text":"mathbfr(t) = sigma (mathbfW^rmathbfx(t-1)) \nmathbfz(t) = sigma (mathbfW^zmathbfx(t-1)) ","category":"page"},{"location":"esn_tutorials/different_drivers/","page":"Using Different Reservoir Drivers","title":"Using Different Reservoir Drivers","text":"Similarly to before, to obtain this variation, it is only required to set inner_layer = fill(NullLayer(), 2) and bias = fill(NullLayer(), 2) while keeping variant = FullyGated().","category":"page"},{"location":"esn_tutorials/different_drivers/#Type-3","page":"Using Different Reservoir Drivers","title":"Type 3","text":"","category":"section"},{"location":"esn_tutorials/different_drivers/","page":"Using Different Reservoir Drivers","title":"Using Different Reservoir Drivers","text":"The final variation, before the minimal one, depends only on the biases","category":"page"},{"location":"esn_tutorials/different_drivers/","page":"Using Different Reservoir Drivers","title":"Using Different Reservoir Drivers","text":"mathbfr(t) = sigma (mathbfb_r) \nmathbfz(t) = sigma (mathbfb_z) ","category":"page"},{"location":"esn_tutorials/different_drivers/","page":"Using Different Reservoir Drivers","title":"Using Different Reservoir Drivers","text":"This means that it is only needed to set inner_layer = fill(NullLayer(), 2) and reservoir = fill(NullReservoir(), 2) while keeping variant = FullyGated().","category":"page"},{"location":"esn_tutorials/different_drivers/#Minimal","page":"Using Different Reservoir Drivers","title":"Minimal","text":"","category":"section"},{"location":"esn_tutorials/different_drivers/","page":"Using Different Reservoir Drivers","title":"Using Different Reservoir Drivers","text":"The minimal GRU variation merges two gates into one:","category":"page"},{"location":"esn_tutorials/different_drivers/","page":"Using Different Reservoir Drivers","title":"Using Different Reservoir Drivers","text":"mathbff(t) = sigma (mathbfW^f_textinmathbfu(t)+mathbfW^fmathbfx(t-1)+mathbfb_f) \ntildemathbfx(t) = texttanh(mathbfW_inmathbfu(t)+mathbfW(mathbff(t) odot mathbfx(t-1))+mathbfb) \nmathbfx(t) = (1-mathbff(t)) odot mathbfx(t-1) + mathbff(t) odot tildemathbfx(t)","category":"page"},{"location":"esn_tutorials/different_drivers/","page":"Using Different Reservoir Drivers","title":"Using Different Reservoir Drivers","text":"This variation can be obtained by setting variation=Minimal(). The inner_layer, reservoir and bias kwargs this time are not vectors, but must be defined like, for example inner_layer = DenseLayer() or reservoir = SparseDenseReservoir().","category":"page"},{"location":"esn_tutorials/different_drivers/#Examples","page":"Using Different Reservoir Drivers","title":"Examples","text":"","category":"section"},{"location":"esn_tutorials/different_drivers/","page":"Using Different Reservoir Drivers","title":"Using Different Reservoir Drivers","text":"To showcase the use of the GRU() method, this section will only illustrate the standard FullyGated() version. The full script for this example with the data can be found here.","category":"page"},{"location":"esn_tutorials/different_drivers/","page":"Using Different Reservoir Drivers","title":"Using Different Reservoir Drivers","text":"The data used for this example is the Santa Fe laser dataset [7] retrieved from here. The data is split to account for a next step prediction.","category":"page"},{"location":"esn_tutorials/different_drivers/","page":"Using Different Reservoir Drivers","title":"Using Different Reservoir Drivers","text":"using DelimitedFiles\n\ndata = reduce(hcat, readdlm(\"./data/santafe_laser.txt\"))\n\ntrain_len = 5000\npredict_len = 2000\n\ntraining_input = data[:, 1:train_len]\ntraining_target = data[:, 2:(train_len + 1)]\ntesting_input = data[:, (train_len + 1):(train_len + predict_len)]\ntesting_target = data[:, (train_len + 2):(train_len + predict_len + 1)]","category":"page"},{"location":"esn_tutorials/different_drivers/","page":"Using Different Reservoir Drivers","title":"Using Different Reservoir Drivers","text":"The construction of the ESN proceeds as usual.","category":"page"},{"location":"esn_tutorials/different_drivers/","page":"Using Different Reservoir Drivers","title":"Using Different Reservoir Drivers","text":"using ReservoirComputing, Random\n\nres_size = 300\nres_radius = 1.4\n\nRandom.seed!(42)\nesn = ESN(training_input, 1, res_size;\n    reservoir=rand_sparse(; radius=res_radius),\n    reservoir_driver=GRU())","category":"page"},{"location":"esn_tutorials/different_drivers/","page":"Using Different Reservoir Drivers","title":"Using Different Reservoir Drivers","text":"The default inner reservoir and input layer for the GRU are the same defaults for the reservoir and input_layer of the ESN. One can use the explicit call if they choose to.","category":"page"},{"location":"esn_tutorials/different_drivers/","page":"Using Different Reservoir Drivers","title":"Using Different Reservoir Drivers","text":"gru = GRU(; reservoir=[rand_sparse,\n        rand_sparse],\n    inner_layer=[scaled_rand, scaled_rand])\nesn = ESN(training_input, 1, res_size;\n    reservoir=rand_sparse(; radius=res_radius),\n    reservoir_driver=gru)","category":"page"},{"location":"esn_tutorials/different_drivers/","page":"Using Different Reservoir Drivers","title":"Using Different Reservoir Drivers","text":"The training and prediction can proceed as usual:","category":"page"},{"location":"esn_tutorials/different_drivers/","page":"Using Different Reservoir Drivers","title":"Using Different Reservoir Drivers","text":"training_method = StandardRidge(0.0)\noutput_layer = train(esn, training_target, training_method)\noutput = esn(Predictive(testing_input), output_layer)","category":"page"},{"location":"esn_tutorials/different_drivers/","page":"Using Different Reservoir Drivers","title":"Using Different Reservoir Drivers","text":"The results can be plotted using Plots.jl","category":"page"},{"location":"esn_tutorials/different_drivers/","page":"Using Different Reservoir Drivers","title":"Using Different Reservoir Drivers","text":"using Plots\n\nplot([testing_target' output']; label=[\"actual\" \"predicted\"],\n    plot_title=\"Santa Fe Laser\",\n    titlefontsize=20,\n    legendfontsize=12,\n    linewidth=2.5,\n    xtickfontsize=12,\n    ytickfontsize=12,\n    size=(1080, 720))","category":"page"},{"location":"esn_tutorials/different_drivers/","page":"Using Different Reservoir Drivers","title":"Using Different Reservoir Drivers","text":"It is interesting to see a comparison of the GRU driven ESN and the standard RNN driven ESN. Using the same parameters defined before it is possible to do the following","category":"page"},{"location":"esn_tutorials/different_drivers/","page":"Using Different Reservoir Drivers","title":"Using Different Reservoir Drivers","text":"using StatsBase\n\nesn_rnn = ESN(training_input, 1, res_size;\n    reservoir=rand_sparse(; radius=res_radius),\n    reservoir_driver=RNN())\n\noutput_layer = train(esn_rnn, training_target, training_method)\noutput_rnn = esn_rnn(Predictive(testing_input), output_layer)\n\nprintln(msd(testing_target, output))\nprintln(msd(testing_target, output_rnn))","category":"page"},{"location":"esn_tutorials/different_drivers/","page":"Using Different Reservoir Drivers","title":"Using Different Reservoir Drivers","text":"[1]: Lun, Shu-Xian, et al. \"A novel model of leaky integrator echo state network for time-series prediction.\" Neurocomputing 159 (2015): 58-66.","category":"page"},{"location":"esn_tutorials/different_drivers/","page":"Using Different Reservoir Drivers","title":"Using Different Reservoir Drivers","text":"[2]: Cho, Kyunghyun, et al. “Learning phrase representations using RNN encoder-decoder for statistical machine translation.” arXiv preprint arXiv:1406.1078 (2014).","category":"page"},{"location":"esn_tutorials/different_drivers/","page":"Using Different Reservoir Drivers","title":"Using Different Reservoir Drivers","text":"[3]: Wang, Xinjie, Yaochu Jin, and Kuangrong Hao. \"A Gated Recurrent Unit based Echo State Network.\" 2020 International Joint Conference on Neural Networks (IJCNN). IEEE, 2020.","category":"page"},{"location":"esn_tutorials/different_drivers/","page":"Using Different Reservoir Drivers","title":"Using Different Reservoir Drivers","text":"[4]: Di Sarli, Daniele, Claudio Gallicchio, and Alessio Micheli. \"Gated Echo State Networks: a preliminary study.\" 2020 International Conference on INnovations in Intelligent SysTems and Applications (INISTA). IEEE, 2020.","category":"page"},{"location":"esn_tutorials/different_drivers/","page":"Using Different Reservoir Drivers","title":"Using Different Reservoir Drivers","text":"[5]: Dey, Rahul, and Fathi M. Salem. \"Gate-variants of gated recurrent unit (GRU) neural networks.\" 2017 IEEE 60th international midwest symposium on circuits and systems (MWSCAS). IEEE, 2017.","category":"page"},{"location":"esn_tutorials/different_drivers/","page":"Using Different Reservoir Drivers","title":"Using Different Reservoir Drivers","text":"[6]: Zhou, Guo-Bing, et al. \"Minimal gated unit for recurrent neural networks.\" International Journal of Automation and Computing 13.3 (2016): 226-234.","category":"page"},{"location":"esn_tutorials/different_drivers/","page":"Using Different Reservoir Drivers","title":"Using Different Reservoir Drivers","text":"[7]: Hübner, Uwe, Nimmi B. Abraham, and Carlos O. Weiss. \"Dimensions and entropies of chaotic intensity pulsations in a single-mode far-infrared NH 3 laser.\" Physical Review A 40.11 (1989): 6354.","category":"page"},{"location":"esn_tutorials/lorenz_basic/#Lorenz-System-Forecasting","page":"Lorenz System Forecasting","title":"Lorenz System Forecasting","text":"","category":"section"},{"location":"esn_tutorials/lorenz_basic/","page":"Lorenz System Forecasting","title":"Lorenz System Forecasting","text":"This example expands on the readme Lorenz system forecasting to better showcase how to use methods and functions provided in the library for Echo State Networks. Here the prediction method used is Generative, for a more detailed explanation of the differences between Generative and Predictive please refer to the other examples given in the documentation.","category":"page"},{"location":"esn_tutorials/lorenz_basic/#Generating-the-data","page":"Lorenz System Forecasting","title":"Generating the data","text":"","category":"section"},{"location":"esn_tutorials/lorenz_basic/","page":"Lorenz System Forecasting","title":"Lorenz System Forecasting","text":"Starting off the workflow, the first step is to obtain the data. Leveraging OrdinaryDiffEq it is possible to derive the Lorenz system data in the following way:","category":"page"},{"location":"esn_tutorials/lorenz_basic/","page":"Lorenz System Forecasting","title":"Lorenz System Forecasting","text":"using OrdinaryDiffEq\n\n#define lorenz system\nfunction lorenz!(du, u, p, t)\n    du[1] = 10.0 * (u[2] - u[1])\n    du[2] = u[1] * (28.0 - u[3]) - u[2]\n    du[3] = u[1] * u[2] - (8 / 3) * u[3]\nend\n\n#solve and take data\nprob = ODEProblem(lorenz!, [1.0, 0.0, 0.0], (0.0, 200.0))\ndata = solve(prob, ABM54(); dt=0.02)\ndata = reduce(hcat, data.u)","category":"page"},{"location":"esn_tutorials/lorenz_basic/","page":"Lorenz System Forecasting","title":"Lorenz System Forecasting","text":"After obtaining the data, it is necessary to determine the kind of prediction for the model. Since this example will use the Generative prediction type, this means that the target data will be the next step of the input data. In addition, it is important to notice that the Lorenz system just obtained presents a transient period that is not representative of the general behavior of the system. This can easily be discarded by setting a shift parameter.","category":"page"},{"location":"esn_tutorials/lorenz_basic/","page":"Lorenz System Forecasting","title":"Lorenz System Forecasting","text":"#determine shift length, training length and prediction length\nshift = 300\ntrain_len = 5000\npredict_len = 1250\n\n#split the data accordingly\ninput_data = data[:, shift:(shift + train_len - 1)]\ntarget_data = data[:, (shift + 1):(shift + train_len)]\ntest_data = data[:, (shift + train_len + 1):(shift + train_len + predict_len)]","category":"page"},{"location":"esn_tutorials/lorenz_basic/","page":"Lorenz System Forecasting","title":"Lorenz System Forecasting","text":"It is important to notice that the data needs to be formatted in a matrix with the features as rows and time steps as columns as in this example. This is needed even if the time series consists of single values.","category":"page"},{"location":"esn_tutorials/lorenz_basic/#Building-the-Echo-State-Network","page":"Lorenz System Forecasting","title":"Building the Echo State Network","text":"","category":"section"},{"location":"esn_tutorials/lorenz_basic/","page":"Lorenz System Forecasting","title":"Lorenz System Forecasting","text":"Once the data is ready, it is possible to define the parameters for the ESN and the ESN struct itself. In this example, the values from [1] are loosely followed as general guidelines.","category":"page"},{"location":"esn_tutorials/lorenz_basic/","page":"Lorenz System Forecasting","title":"Lorenz System Forecasting","text":"using ReservoirComputing\n\n#define ESN parameters\nres_size = 300\nin_size = 3\nres_radius = 1.2\nres_sparsity = 6 / 300\ninput_scaling = 0.1\n\n#build ESN struct\nesn = ESN(input_data, in_size, res_size;\n    reservoir=rand_sparse(; radius=res_radius, sparsity=res_sparsity),\n    input_layer=weighted_init(; scaling=input_scaling),\n    reservoir_driver=RNN(),\n    nla_type=NLADefault(),\n    states_type=StandardStates())","category":"page"},{"location":"esn_tutorials/lorenz_basic/","page":"Lorenz System Forecasting","title":"Lorenz System Forecasting","text":"Most of the parameters chosen here mirror the default ones, so a direct call is not necessary. The readme example is identical to this one, except for the explicit call. Going line by line to see what is happening, starting from res_size: this value determines the dimensions of the reservoir matrix. In this case, a size of 300 has been chosen, so the reservoir matrix will be 300 x 300. This is not always the case, since some input layer constructions can modify the dimensions of the reservoir, but in that case, everything is taken care of internally.","category":"page"},{"location":"esn_tutorials/lorenz_basic/","page":"Lorenz System Forecasting","title":"Lorenz System Forecasting","text":"The res_radius determines the scaling of the spectral radius of the reservoir matrix; a proper scaling is necessary to assure the Echo State Property. The default value in the rand_sparse method is 1.0 in accordance with the most commonly followed guidelines found in the literature (see [2] and references therein). The sparsity of the reservoir matrix in this case is obtained by choosing a degree of connections and dividing that by the reservoir size. Of course, it is also possible to simply choose any value between 0.0 and 1.0 to test behaviors for different sparsity values.","category":"page"},{"location":"esn_tutorials/lorenz_basic/","page":"Lorenz System Forecasting","title":"Lorenz System Forecasting","text":"The value of input_scaling determines the upper and lower bounds of the uniform distribution of the weights in the weighted_init. The value of 0.1 represents the default. The default input layer is the scaled_rand, a dense matrix. The details of the weighted version can be found in [3], for this example, this version returns the best results.","category":"page"},{"location":"esn_tutorials/lorenz_basic/","page":"Lorenz System Forecasting","title":"Lorenz System Forecasting","text":"The reservoir driver represents the dynamics of the reservoir. In the standard ESN definition, these dynamics are obtained through a Recurrent Neural Network (RNN), and this is reflected by calling the RNN driver for the ESN struct. This option is set as the default, and unless there is the need to change parameters, it is not needed. The full equation is the following:","category":"page"},{"location":"esn_tutorials/lorenz_basic/","page":"Lorenz System Forecasting","title":"Lorenz System Forecasting","text":"textbfx(t+1) = (1-alpha)textbfx(t) + alpha cdot texttanh(textbfWtextbfx(t)+textbfW_textintextbfu(t))","category":"page"},{"location":"esn_tutorials/lorenz_basic/","page":"Lorenz System Forecasting","title":"Lorenz System Forecasting","text":"where α represents the leaky coefficient, and tanh can be any activation function. Also, textbfx represents the state vector, textbfu the input data, and textbfW textbfW_textin are the reservoir matrix and input matrix, respectively. The default call to the RNN in the library is the following RNN(;activation_function=tanh, leaky_coefficient=1.0), where the meaning of the parameters is clear from the equation above. Instead of the hyperbolic tangent, any activation function can be used, either leveraging external libraries such as NNlib or creating a custom one.","category":"page"},{"location":"esn_tutorials/lorenz_basic/","page":"Lorenz System Forecasting","title":"Lorenz System Forecasting","text":"The final calls are modifications to the states in training or prediction. The default calls, depicted in the example, do not make any modifications to the states. This is the safest bet if one is not sure how these work. The nla_type applies a non-linear algorithm to the states, while the states_type can expand them by concatenating them with the input data, or padding them by concatenating a constant value to all the states. More in depth descriptions of these parameters are given in other examples in the documentation.","category":"page"},{"location":"esn_tutorials/lorenz_basic/#Training-and-Prediction","page":"Lorenz System Forecasting","title":"Training and Prediction","text":"","category":"section"},{"location":"esn_tutorials/lorenz_basic/","page":"Lorenz System Forecasting","title":"Lorenz System Forecasting","text":"Now that the ESN has been created and all the parameters have been explained, it is time to proceed with the training. The full call of the readme example follows this general idea:","category":"page"},{"location":"esn_tutorials/lorenz_basic/","page":"Lorenz System Forecasting","title":"Lorenz System Forecasting","text":"#define training method\ntraining_method = StandardRidge(0.0)\n\n#obtain output layer\noutput_layer = train(esn, target_data, training_method)","category":"page"},{"location":"esn_tutorials/lorenz_basic/","page":"Lorenz System Forecasting","title":"Lorenz System Forecasting","text":"The training returns an OutputLayer struct containing the trained output matrix and other  needed for the prediction. The necessary elements in the train() call are the ESN struct created in the previous step and the target_data, which in this case is the one step ahead evolution of the Lorenz system. The training method chosen in this example is the standard one, so an equivalent way of calling the train function here is output_layer = train(esn, target_data) like the readme basic version. Likewise, the default value for the ridge regression parameter is set to zero, so the actual default training is Ordinary Least Squares regression. Other training methods are available and will be explained in the following examples.","category":"page"},{"location":"esn_tutorials/lorenz_basic/","page":"Lorenz System Forecasting","title":"Lorenz System Forecasting","text":"Once the OutputLayer has been obtained, the prediction can be done following this procedure:","category":"page"},{"location":"esn_tutorials/lorenz_basic/","page":"Lorenz System Forecasting","title":"Lorenz System Forecasting","text":"output = esn(Generative(predict_len), output_layer)","category":"page"},{"location":"esn_tutorials/lorenz_basic/","page":"Lorenz System Forecasting","title":"Lorenz System Forecasting","text":"both the training method and the output layer are needed in this call. The number of steps for the prediction must be specified in the Generative method. The output results are given in a matrix.","category":"page"},{"location":"esn_tutorials/lorenz_basic/","page":"Lorenz System Forecasting","title":"Lorenz System Forecasting","text":"info: Saving the states during prediction\nWhile the states are saved in the ESN struct for the training, for the prediction they are not saved by default. To inspect the states, it is necessary to pass the boolean keyword argument save_states to the prediction call, in this example using esn(... ; save_states=true). This returns a tuple (output, states) where size(states) = res_size, prediction_len","category":"page"},{"location":"esn_tutorials/lorenz_basic/","page":"Lorenz System Forecasting","title":"Lorenz System Forecasting","text":"To inspect the results, they can easily be plotted using an external library. In this case, Plots is adopted:","category":"page"},{"location":"esn_tutorials/lorenz_basic/","page":"Lorenz System Forecasting","title":"Lorenz System Forecasting","text":"using Plots, Plots.PlotMeasures\n\nts = 0.0:0.02:200.0\nlorenz_maxlyap = 0.9056\npredict_ts = ts[(shift + train_len + 1):(shift + train_len + predict_len)]\nlyap_time = (predict_ts .- predict_ts[1]) * (1 / lorenz_maxlyap)\n\np1 = plot(lyap_time, [test_data[1, :] output[1, :]]; label=[\"actual\" \"predicted\"],\n    ylabel=\"x(t)\", linewidth=2.5, xticks=false, yticks=-15:15:15);\np2 = plot(lyap_time, [test_data[2, :] output[2, :]]; label=[\"actual\" \"predicted\"],\n    ylabel=\"y(t)\", linewidth=2.5, xticks=false, yticks=-20:20:20);\np3 = plot(lyap_time, [test_data[3, :] output[3, :]]; label=[\"actual\" \"predicted\"],\n    ylabel=\"z(t)\", linewidth=2.5, xlabel=\"max(λ)*t\", yticks=10:15:40);\n\nplot(p1, p2, p3; plot_title=\"Lorenz System Coordinates\",\n    layout=(3, 1), xtickfontsize=12, ytickfontsize=12, xguidefontsize=15,\n    yguidefontsize=15,\n    legendfontsize=12, titlefontsize=20)","category":"page"},{"location":"esn_tutorials/lorenz_basic/#Bibliography","page":"Lorenz System Forecasting","title":"Bibliography","text":"","category":"section"},{"location":"esn_tutorials/lorenz_basic/","page":"Lorenz System Forecasting","title":"Lorenz System Forecasting","text":"[1]: Pathak, Jaideep, et al. \"Using machine learning to replicate chaotic attractors and calculate Lyapunov exponents from data.\" Chaos: An Interdisciplinary Journal of Nonlinear Science 27.12 (2017): 121102.","category":"page"},{"location":"esn_tutorials/lorenz_basic/","page":"Lorenz System Forecasting","title":"Lorenz System Forecasting","text":"[2]: Lukoševičius, Mantas. \"A practical guide to applying echo state networks.\" Neural networks: Tricks of the trade. Springer, Berlin, Heidelberg, 2012. 659-686.","category":"page"},{"location":"esn_tutorials/lorenz_basic/","page":"Lorenz System Forecasting","title":"Lorenz System Forecasting","text":"[3]: Lu, Zhixin, et al. \"Reservoir observers: Model-free inference of unmeasured variables in chaotic systems.\" Chaos: An Interdisciplinary Journal of Nonlinear Science 27.4 (2017): 041102.","category":"page"},{"location":"esn_tutorials/hybrid/#Hybrid-Echo-State-Networks","page":"Hybrid Echo State Networks","title":"Hybrid Echo State Networks","text":"","category":"section"},{"location":"esn_tutorials/hybrid/","page":"Hybrid Echo State Networks","title":"Hybrid Echo State Networks","text":"Following the idea of giving physical information to machine learning models, the hybrid echo state networks [1] try to achieve this results by feeding model data into the ESN. In this example, it is explained how to create and leverage such models in ReservoirComputing.jl.","category":"page"},{"location":"esn_tutorials/hybrid/#Generating-the-data","page":"Hybrid Echo State Networks","title":"Generating the data","text":"","category":"section"},{"location":"esn_tutorials/hybrid/","page":"Hybrid Echo State Networks","title":"Hybrid Echo State Networks","text":"For this example, we are going to forecast the Lorenz system. As usual, the data is generated leveraging DifferentialEquations.jl:","category":"page"},{"location":"esn_tutorials/hybrid/","page":"Hybrid Echo State Networks","title":"Hybrid Echo State Networks","text":"using DifferentialEquations\n\nu0 = [1.0, 0.0, 0.0]\ntspan = (0.0, 1000.0)\ndatasize = 100000\ntsteps = range(tspan[1], tspan[2]; length=datasize)\n\nfunction lorenz(du, u, p, t)\n    p = [10.0, 28.0, 8 / 3]\n    du[1] = p[1] * (u[2] - u[1])\n    du[2] = u[1] * (p[2] - u[3]) - u[2]\n    du[3] = u[1] * u[2] - p[3] * u[3]\nend\n\node_prob = ODEProblem(lorenz, u0, tspan)\node_sol = solve(ode_prob; saveat=tsteps)\node_data = Array(ode_sol)\n\ntrain_len = 10000\n\ninput_data = ode_data[:, 1:train_len]\ntarget_data = ode_data[:, 2:(train_len + 1)]\ntest_data = ode_data[:, (train_len + 1):end][:, 1:1000]\n\npredict_len = size(test_data, 2)\ntspan_train = (tspan[1], ode_sol.t[train_len])","category":"page"},{"location":"esn_tutorials/hybrid/#Building-the-Hybrid-Echo-State-Network","page":"Hybrid Echo State Networks","title":"Building the Hybrid Echo State Network","text":"","category":"section"},{"location":"esn_tutorials/hybrid/","page":"Hybrid Echo State Networks","title":"Hybrid Echo State Networks","text":"To feed the data to the ESN, it is necessary to create a suitable function.","category":"page"},{"location":"esn_tutorials/hybrid/","page":"Hybrid Echo State Networks","title":"Hybrid Echo State Networks","text":"function prior_model_data_generator(u0, tspan, tsteps, model=lorenz)\n    prob = ODEProblem(lorenz, u0, tspan)\n    sol = Array(solve(prob; saveat=tsteps))\n    return sol\nend","category":"page"},{"location":"esn_tutorials/hybrid/","page":"Hybrid Echo State Networks","title":"Hybrid Echo State Networks","text":"Given the initial condition, time span, and time steps, this function returns the data for the chosen model. Now, using the KnowledgeModel method, it is possible to input all this information to HybridESN.","category":"page"},{"location":"esn_tutorials/hybrid/","page":"Hybrid Echo State Networks","title":"Hybrid Echo State Networks","text":"using ReservoirComputing, Random\nRandom.seed!(42)\n\nkm = KnowledgeModel(prior_model_data_generator, u0, tspan_train, train_len)\n\nin_size = 3\nres_size = 300\nhesn = HybridESN(km,\n    input_data,\n    in_size,\n    res_size;\n    reservoir=rand_sparse)","category":"page"},{"location":"esn_tutorials/hybrid/#Training-and-Prediction","page":"Hybrid Echo State Networks","title":"Training and Prediction","text":"","category":"section"},{"location":"esn_tutorials/hybrid/","page":"Hybrid Echo State Networks","title":"Hybrid Echo State Networks","text":"The training and prediction of the Hybrid ESN can proceed as usual:","category":"page"},{"location":"esn_tutorials/hybrid/","page":"Hybrid Echo State Networks","title":"Hybrid Echo State Networks","text":"output_layer = train(hesn, target_data, StandardRidge(0.3))\noutput = hesn(Generative(predict_len), output_layer)","category":"page"},{"location":"esn_tutorials/hybrid/","page":"Hybrid Echo State Networks","title":"Hybrid Echo State Networks","text":"It is now possible to plot the results, leveraging Plots.jl:","category":"page"},{"location":"esn_tutorials/hybrid/","page":"Hybrid Echo State Networks","title":"Hybrid Echo State Networks","text":"using Plots\nlorenz_maxlyap = 0.9056\npredict_ts = tsteps[(train_len + 1):(train_len + predict_len)]\nlyap_time = (predict_ts .- predict_ts[1]) * (1 / lorenz_maxlyap)\n\np1 = plot(lyap_time, [test_data[1, :] output[1, :]]; label=[\"actual\" \"predicted\"],\n    ylabel=\"x(t)\", linewidth=2.5, xticks=false, yticks=-15:15:15);\np2 = plot(lyap_time, [test_data[2, :] output[2, :]]; label=[\"actual\" \"predicted\"],\n    ylabel=\"y(t)\", linewidth=2.5, xticks=false, yticks=-20:20:20);\np3 = plot(lyap_time, [test_data[3, :] output[3, :]]; label=[\"actual\" \"predicted\"],\n    ylabel=\"z(t)\", linewidth=2.5, xlabel=\"max(λ)*t\", yticks=10:15:40);\n\nplot(p1, p2, p3; plot_title=\"Lorenz System Coordinates\",\n    layout=(3, 1), xtickfontsize=12, ytickfontsize=12, xguidefontsize=15,\n    yguidefontsize=15,\n    legendfontsize=12, titlefontsize=20)","category":"page"},{"location":"esn_tutorials/hybrid/#Bibliography","page":"Hybrid Echo State Networks","title":"Bibliography","text":"","category":"section"},{"location":"esn_tutorials/hybrid/","page":"Hybrid Echo State Networks","title":"Hybrid Echo State Networks","text":"[1]: Pathak, Jaideep, et al. \"Hybrid forecasting of chaotic processes: Using machine learning in conjunction with a knowledge-based model.\" Chaos: An Interdisciplinary Journal of Nonlinear Science 28.4 (2018): 041101.","category":"page"},{"location":"esn_tutorials/deep_esn/#Deep-Echo-State-Networks","page":"Deep Echo State Networks","title":"Deep Echo State Networks","text":"","category":"section"},{"location":"esn_tutorials/deep_esn/","page":"Deep Echo State Networks","title":"Deep Echo State Networks","text":"Deep Echo State Network architectures started to gain some traction recently. In this guide, we illustrate how it is possible to use ReservoirComputing.jl to build a deep ESN.","category":"page"},{"location":"esn_tutorials/deep_esn/","page":"Deep Echo State Networks","title":"Deep Echo State Networks","text":"The network implemented in this library is taken from [1]. It works by stacking reservoirs on top of each other, feeding the output from one into the next. The states are obtained by merging all the inner states of the stacked reservoirs. For a more in-depth explanation, refer to the paper linked above.","category":"page"},{"location":"esn_tutorials/deep_esn/#Lorenz-Example","page":"Deep Echo State Networks","title":"Lorenz Example","text":"","category":"section"},{"location":"esn_tutorials/deep_esn/","page":"Deep Echo State Networks","title":"Deep Echo State Networks","text":"For this example, we are going to reuse the Lorenz data used in the Lorenz System Forecasting example.","category":"page"},{"location":"esn_tutorials/deep_esn/","page":"Deep Echo State Networks","title":"Deep Echo State Networks","text":"using OrdinaryDiffEq\n\n#define lorenz system\nfunction lorenz!(du, u, p, t)\n    du[1] = 10.0 * (u[2] - u[1])\n    du[2] = u[1] * (28.0 - u[3]) - u[2]\n    du[3] = u[1] * u[2] - (8 / 3) * u[3]\nend\n\n#solve and take data\nprob = ODEProblem(lorenz!, [1.0, 0.0, 0.0], (0.0, 200.0))\ndata = solve(prob, ABM54(); dt=0.02)\ndata = reduce(hcat, data.u)\n\n#determine shift length, training length and prediction length\nshift = 300\ntrain_len = 5000\npredict_len = 1250\n\n#split the data accordingly\ninput_data = data[:, shift:(shift + train_len - 1)]\ntarget_data = data[:, (shift + 1):(shift + train_len)]\ntest_data = data[:, (shift + train_len + 1):(shift + train_len + predict_len)]","category":"page"},{"location":"esn_tutorials/deep_esn/","page":"Deep Echo State Networks","title":"Deep Echo State Networks","text":"Again, it is important to notice that the data needs to be formatted in a matrix, with the features as rows and time steps as columns, as in this example. This is needed even if the time series consists of single values.","category":"page"},{"location":"esn_tutorials/deep_esn/","page":"Deep Echo State Networks","title":"Deep Echo State Networks","text":"The construction of the ESN is also really similar. The only difference is that the reservoir can be fed as an array of reservoirs.","category":"page"},{"location":"esn_tutorials/deep_esn/","page":"Deep Echo State Networks","title":"Deep Echo State Networks","text":"using ReservoirComputing\n\nreservoirs = [rand_sparse(; radius=1.1, sparsity=0.1),\n    rand_sparse(; radius=1.2, sparsity=0.1),\n    rand_sparse(; radius=1.4, sparsity=0.1)]\n\nesn = DeepESN(input_data, 3, 200;\n    reservoir=reservoirs,\n    reservoir_driver=RNN(),\n    nla_type=NLADefault(),\n    states_type=StandardStates())","category":"page"},{"location":"esn_tutorials/deep_esn/","page":"Deep Echo State Networks","title":"Deep Echo State Networks","text":"The input layer and bias can also be given as vectors, but of course, they have to be of the same size of the reservoirs vector. If they are not passed as a vector, the value passed will be used for all the layers in the deep ESN.","category":"page"},{"location":"esn_tutorials/deep_esn/","page":"Deep Echo State Networks","title":"Deep Echo State Networks","text":"In addition to using the provided functions for the construction of the layers, the user can also choose to build their own matrix, or array of matrices, and feed that into the ESN in the same way.","category":"page"},{"location":"esn_tutorials/deep_esn/","page":"Deep Echo State Networks","title":"Deep Echo State Networks","text":"The training and prediction follow the usual framework:","category":"page"},{"location":"esn_tutorials/deep_esn/","page":"Deep Echo State Networks","title":"Deep Echo State Networks","text":"training_method = StandardRidge(0.0)\noutput_layer = train(esn, target_data, training_method)\n\noutput = esn(Generative(predict_len), output_layer)","category":"page"},{"location":"esn_tutorials/deep_esn/","page":"Deep Echo State Networks","title":"Deep Echo State Networks","text":"Plotting the results:","category":"page"},{"location":"esn_tutorials/deep_esn/","page":"Deep Echo State Networks","title":"Deep Echo State Networks","text":"using Plots\n\nts = 0.0:0.02:200.0\nlorenz_maxlyap = 0.9056\npredict_ts = ts[(shift + train_len + 1):(shift + train_len + predict_len)]\nlyap_time = (predict_ts .- predict_ts[1]) * (1 / lorenz_maxlyap)\n\np1 = plot(lyap_time, [test_data[1, :] output[1, :]]; label=[\"actual\" \"predicted\"],\n    ylabel=\"x(t)\", linewidth=2.5, xticks=false, yticks=-15:15:15);\np2 = plot(lyap_time, [test_data[2, :] output[2, :]]; label=[\"actual\" \"predicted\"],\n    ylabel=\"y(t)\", linewidth=2.5, xticks=false, yticks=-20:20:20);\np3 = plot(lyap_time, [test_data[3, :] output[3, :]]; label=[\"actual\" \"predicted\"],\n    ylabel=\"z(t)\", linewidth=2.5, xlabel=\"max(λ)*t\", yticks=10:15:40);\n\nplot(p1, p2, p3; plot_title=\"Lorenz System Coordinates\",\n    layout=(3, 1), xtickfontsize=12, ytickfontsize=12, xguidefontsize=15,\n    yguidefontsize=15,\n    legendfontsize=12, titlefontsize=20)","category":"page"},{"location":"esn_tutorials/deep_esn/#Documentation","page":"Deep Echo State Networks","title":"Documentation","text":"","category":"section"},{"location":"esn_tutorials/deep_esn/","page":"Deep Echo State Networks","title":"Deep Echo State Networks","text":"[1]: Gallicchio, Claudio, and Alessio Micheli. \"Deep echo state network (deepesn): A brief survey.\" arXiv preprint arXiv:1712.04323 (2017).","category":"page"},{"location":"general/states_variation/#Altering-States","page":"Altering States","title":"Altering States","text":"","category":"section"},{"location":"general/states_variation/","page":"Altering States","title":"Altering States","text":"In ReservoirComputing models, it's possible to perform alterations on the reservoir states during the training stage. These alterations can improve prediction results or replicate results found in the literature. Alterations are categorized into two possibilities: padding or extending the states, and applying non-linear algorithms to the states.","category":"page"},{"location":"general/states_variation/#Padding-and-Extending-States","page":"Altering States","title":"Padding and Extending States","text":"","category":"section"},{"location":"general/states_variation/#Extending-States","page":"Altering States","title":"Extending States","text":"","category":"section"},{"location":"general/states_variation/","page":"Altering States","title":"Altering States","text":"Extending the states involves appending the corresponding input values to the reservoir states. If textbfx(t) represents the reservoir state at time t corresponding to the input textbfu(t), the extended state is represented as textbfx(t) textbfu(t), where  denotes vertical concatenation. This procedure is commonly used in Echo State Networks. You can extend the states in every ReservoirComputing.jl model by using the states_type keyword argument and calling the ExtendedStates() method. No additional arguments are needed.","category":"page"},{"location":"general/states_variation/#Padding-States","page":"Altering States","title":"Padding States","text":"","category":"section"},{"location":"general/states_variation/","page":"Altering States","title":"Altering States","text":"Padding the states involves appending a constant value, such as 1.0, to each state. In the notation introduced earlier, padded states can be represented as textbfx(t) 10. This approach is detailed in \"A practical guide to applying echo state networks.\" by Lukoševičius, Mantas. To pad the states, you can use the states_type keyword argument and call the PaddedStates(padding) method, where padding represents the value to be concatenated to the states. By default, the padding value is set to 1.0, so most of the time, calling PaddedStates() will suffice.","category":"page"},{"location":"general/states_variation/","page":"Altering States","title":"Altering States","text":"Additionally, you can pad the extended states by using the PaddedExtendedStates(padding) method, which also has a default padding value of 1.0.","category":"page"},{"location":"general/states_variation/","page":"Altering States","title":"Altering States","text":"You can choose not to apply any of these changes to the states by calling StandardStates(), which is the default choice for the states.","category":"page"},{"location":"general/states_variation/#Non-Linear-Algorithms","page":"Altering States","title":"Non-Linear Algorithms","text":"","category":"section"},{"location":"general/states_variation/","page":"Altering States","title":"Altering States","text":"First introduced in [1] and expanded in [2], non-linear algorithms are nonlinear combinations of the columns of the matrix states. There are three such algorithms implemented in ReservoirComputing.jl, and you can choose which one to use with the nla_type keyword argument. The default value is set to NLADefault(), which means no non-linear algorithm is applied.","category":"page"},{"location":"general/states_variation/","page":"Altering States","title":"Altering States","text":"The available non-linear algorithms are:","category":"page"},{"location":"general/states_variation/","page":"Altering States","title":"Altering States","text":"NLAT1()\nNLAT2()\nNLAT3()","category":"page"},{"location":"general/states_variation/","page":"Altering States","title":"Altering States","text":"These algorithms perform specific operations on the reservoir states. To provide a better understanding of what they do, let textbfx_i j be elements of the state matrix, with i=1T  j=1N where T is the length of the training and N is the reservoir size.","category":"page"},{"location":"general/states_variation/","page":"Altering States","title":"Altering States","text":"NLAT1","category":"page"},{"location":"general/states_variation/","page":"Altering States","title":"Altering States","text":"tildetextbfx_ij = textbfx_ij times textbfx_ij   textif textitj is odd \ntildetextbfx_ij = textbfx_ij    textif textitj is even","category":"page"},{"location":"general/states_variation/","page":"Altering States","title":"Altering States","text":"NLAT2","category":"page"},{"location":"general/states_variation/","page":"Altering States","title":"Altering States","text":"tildetextbfx_ij = textbfx_ij-1 times textbfx_ij-2   textif textitj  1 is odd \ntildetextbfx_ij = textbfx_ij    textif textitj is 1 or even","category":"page"},{"location":"general/states_variation/","page":"Altering States","title":"Altering States","text":"NLAT3","category":"page"},{"location":"general/states_variation/","page":"Altering States","title":"Altering States","text":"tildetextbfx_ij = textbfx_ij-1 times textbfx_ij+1   textif textitj  1 is odd \ntildetextbfx_ij = textbfx_ij    textif textitj is 1 or even","category":"page"},{"location":"general/states_variation/","page":"Altering States","title":"Altering States","text":"[1]: Pathak, Jaideep, et al. \"Using machine learning to replicate chaotic attractors and calculate Lyapunov exponents from data.\" Chaos: An Interdisciplinary Journal of Nonlinear Science 27.12 (2017): 121102.","category":"page"},{"location":"general/states_variation/","page":"Altering States","title":"Altering States","text":"[2]: Chattopadhyay, Ashesh, Pedram Hassanzadeh, and Devika Subramanian. \"Data-driven predictions of a multiscale Lorenz 96 chaotic system using machine-learning methods: reservoir computing, artificial neural network, and long short-term memory network.\" Nonlinear Processes in Geophysics 27.3 (2020): 373-389.","category":"page"},{"location":"esn_tutorials/different_training/#Using-Different-Training-Methods","page":"Using Different Training Methods","title":"Using Different Training Methods","text":"","category":"section"},{"location":"esn_tutorials/different_training/#Linear-Methods","page":"Using Different Training Methods","title":"Linear Methods","text":"","category":"section"},{"location":"esn_tutorials/different_training/#Echo-State-Gaussian-Processes","page":"Using Different Training Methods","title":"Echo State Gaussian Processes","text":"","category":"section"},{"location":"esn_tutorials/different_training/#Support-Vector-Echo-State-Machines","page":"Using Different Training Methods","title":"Support Vector Echo State Machines","text":"","category":"section"},{"location":"api/predict/#Prediction-Types","page":"Prediction Types","title":"Prediction Types","text":"","category":"section"},{"location":"api/predict/#ReservoirComputing.Generative","page":"Prediction Types","title":"ReservoirComputing.Generative","text":"Generative(prediction_len)\n\nA prediction strategy that enables models to generate autonomous multi-step forecasts by recursively feeding their own outputs back as inputs for subsequent prediction steps.\n\nParameters\n\nprediction_len: The number of future steps to predict.\n\nDescription\n\nThe Generative prediction method allows a model to perform multi-step forecasting by using its own previous predictions as inputs for future predictions.\n\nAt each step, the model takes the current input, generates a prediction, and then incorporates that prediction into the input for the next step. This recursive process continues until the specified number of prediction steps (prediction_len) is reached.\n\n\n\n\n\n","category":"type"},{"location":"api/predict/#ReservoirComputing.Predictive","page":"Prediction Types","title":"ReservoirComputing.Predictive","text":"Predictive(prediction_data)\n\nA prediction strategy for supervised learning tasks, where a model predicts labels based on a provided set of input features (prediction_data).\n\nParameters\n\nprediction_data: The input data used for prediction, feature x sample\n\nDescription\n\nThe Predictive prediction method uses the provided input data (prediction_data) to produce corresponding labels or outputs based on the learned relationships in the model.\n\n\n\n\n\n","category":"type"},{"location":"api/reca/#Reservoir-Computing-with-Cellular-Automata","page":"ReCA","title":"Reservoir Computing with Cellular Automata","text":"","category":"section"},{"location":"api/reca/#ReservoirComputing.RECA","page":"ReCA","title":"ReservoirComputing.RECA","text":"RECA(train_data,\n    automata;\n    generations = 8,\n    input_encoding=RandomMapping(),\n    nla_type = NLADefault(),\n    states_type = StandardStates())\n\n[1] Yilmaz, Ozgur. “Reservoir computing using cellular automata.” arXiv preprint arXiv:1410.0162 (2014).\n\n[2] Nichele, Stefano, and Andreas Molund. “Deep reservoir computing using cellular automata.” arXiv preprint arXiv:1703.02806 (2017).\n\n\n\n\n\n","category":"type"},{"location":"api/reca/","page":"ReCA","title":"ReCA","text":"The input encodings are the equivalent of the input matrices of the ESNs. These are the available encodings:","category":"page"},{"location":"api/reca/#ReservoirComputing.RandomMapping","page":"ReCA","title":"ReservoirComputing.RandomMapping","text":"RandomMapping(permutations, expansion_size)\nRandomMapping(permutations; expansion_size=40)\nRandomMapping(;permutations=8, expansion_size=40)\n\nRandom mapping of the input data directly in the reservoir. The expansion_size determines the dimension of the single reservoir, and permutations determines the number of total reservoirs that will be connected, each with a different mapping. The detail of this implementation can be found in [1].\n\n[1] Nichele, Stefano, and Andreas Molund. “Deep reservoir computing using cellular automata.” arXiv preprint arXiv:1703.02806 (2017).\n\n\n\n\n\n","category":"type"},{"location":"api/reca/","page":"ReCA","title":"ReCA","text":"The training and prediction follow the same workflow as the ESN. It is important to note that currently we were unable to find any papers using these models with a Generative approach for the prediction, so full support is given only to the Predictive method.","category":"page"},{"location":"esn_tutorials/change_layers/#Using-different-layers","page":"Using Different Layers","title":"Using different layers","text":"","category":"section"},{"location":"esn_tutorials/change_layers/","page":"Using Different Layers","title":"Using Different Layers","text":"A great deal of efforts in the ESNs field are devoted to finding an ideal construction for the reservoir matrices. ReservoirComputing.jl offers multiple implementation of reservoir and input matrices initializations found in the literature. The API is standardized, and follows  WeightInitializers.jl:","category":"page"},{"location":"esn_tutorials/change_layers/","page":"Using Different Layers","title":"Using Different Layers","text":"weights = init(rng, dims...)\n#rng is optional\nweights = init(dims...)","category":"page"},{"location":"esn_tutorials/change_layers/","page":"Using Different Layers","title":"Using Different Layers","text":"Additional keywords can be added when needed:","category":"page"},{"location":"esn_tutorials/change_layers/","page":"Using Different Layers","title":"Using Different Layers","text":"weights_init = init(rng; kwargs...)\nweights = weights_init(rng, dims...)\n# or\nweights_init = init(; kwargs...)\nweights = weights_init(dims...)","category":"page"},{"location":"esn_tutorials/change_layers/","page":"Using Different Layers","title":"Using Different Layers","text":"Custom layers only need to follow these APIs to be compatible with ReservoirComputing.jl.","category":"page"},{"location":"esn_tutorials/change_layers/#Example-of-minimally-complex-ESN","page":"Using Different Layers","title":"Example of minimally complex ESN","text":"","category":"section"},{"location":"esn_tutorials/change_layers/","page":"Using Different Layers","title":"Using Different Layers","text":"Using [rodan2012] and [rodan2010] as references this section will provide an example on how to change both the input layer and the reservoir for ESNs.","category":"page"},{"location":"esn_tutorials/change_layers/","page":"Using Different Layers","title":"Using Different Layers","text":"The task for this example will be the one step ahead prediction of the Henon map. To obtain the data one can leverage the package PredefinedDynamicalSystems.jl. The data is scaled to be between -1 and 1.","category":"page"},{"location":"esn_tutorials/change_layers/","page":"Using Different Layers","title":"Using Different Layers","text":"using PredefinedDynamicalSystems\ntrain_len = 3000\npredict_len = 2000\n\nds = Systems.henon()\ntraj, t = trajectory(ds, 7000)\ndata = Matrix(traj)'\ndata = (data .- 0.5) .* 2\nshift = 200\n\ntraining_input = data[:, shift:(shift + train_len - 1)]\ntraining_target = data[:, (shift + 1):(shift + train_len)]\ntesting_input = data[:, (shift + train_len):(shift + train_len + predict_len - 1)]\ntesting_target = data[:, (shift + train_len + 1):(shift + train_len + predict_len)]","category":"page"},{"location":"esn_tutorials/change_layers/","page":"Using Different Layers","title":"Using Different Layers","text":"Now it is possible to define the input layers and reservoirs we want to compare and run the comparison in a simple for loop. The accuracy will be tested using the mean squared deviation msd from StatsBase.","category":"page"},{"location":"esn_tutorials/change_layers/","page":"Using Different Layers","title":"Using Different Layers","text":"using ReservoirComputing, StatsBase\n\nres_size = 300\ninput_layer = [minimal_init(; weight=0.85, sampling_type=:irrational_sample!),\n    minimal_init(; weight=0.95, sampling_type=:irrational_sample!)]\nreservoirs = [simple_cycle(; weight=0.7),\n    cycle_jumps(; cycle_weight=0.7, jump_weight=0.2, jump_size=5)]\n\nfor i in 1:length(reservoirs)\n    esn = ESN(training_input, 2, res_size;\n        input_layer=input_layer[i],\n        reservoir=reservoirs[i])\n    wout = train(esn, training_target, StandardRidge(0.001))\n    output = esn(Predictive(testing_input), wout)\n    println(msd(testing_target, output))\nend","category":"page"},{"location":"esn_tutorials/change_layers/","page":"Using Different Layers","title":"Using Different Layers","text":"As it is possible to see, changing layers in ESN models is straightforward. Be sure to check the API documentation for a full list of reservoir and layers.","category":"page"},{"location":"esn_tutorials/change_layers/#Bibliography","page":"Using Different Layers","title":"Bibliography","text":"","category":"section"},{"location":"esn_tutorials/change_layers/","page":"Using Different Layers","title":"Using Different Layers","text":"[rodan2012]: Rodan, Ali, and Peter Tiňo. “Simple deterministically constructed cycle reservoirs with regular jumps.” Neural computation 24.7 (2012): 1822-1852.","category":"page"},{"location":"esn_tutorials/change_layers/","page":"Using Different Layers","title":"Using Different Layers","text":"[rodan2010]: Rodan, Ali, and Peter Tiňo. “Minimum complexity echo state network.” IEEE transactions on neural networks 22.1 (2010): 131-144.","category":"page"},{"location":"general/different_training/#Changing-Training-Algorithms","page":"Changing Training Algorithms","title":"Changing Training Algorithms","text":"","category":"section"},{"location":"general/different_training/","page":"Changing Training Algorithms","title":"Changing Training Algorithms","text":"Notably Echo State Networks have been trained with Ridge Regression algorithms, but the range of useful algorithms to use is much greater. In this section of the documentation, it is possible to explore how to use other training methods to obtain the readout layer. All the methods implemented in ReservoirComputing.jl can be used for all models in the library, not only ESNs. The general workflow illustrated in this section will be based on a dummy RC model my_model = MyModel(...) that needs training to obtain the readout layer. The training is done as follows:","category":"page"},{"location":"general/different_training/","page":"Changing Training Algorithms","title":"Changing Training Algorithms","text":"training_algo = TrainingAlgo()\nreadout_layer = train(my_model, train_data, training_algo)","category":"page"},{"location":"general/different_training/","page":"Changing Training Algorithms","title":"Changing Training Algorithms","text":"In this section, it is possible to explore how to properly build the training_algo and all the possible choices available. In the example section of the documentation it will be provided copy-pasteable code to better explore the training algorithms and their impact on the model.","category":"page"},{"location":"general/different_training/#Linear-Models","page":"Changing Training Algorithms","title":"Linear Models","text":"","category":"section"},{"location":"general/different_training/","page":"Changing Training Algorithms","title":"Changing Training Algorithms","text":"The library includes a standard implementation of ridge regression, callable using StandardRidge(regularization_coeff). The default regularization coefficient is set to zero. This is also the default model called when no model is specified in train(). This makes the default call for training train(my_model, train_data) use Ordinary Least Squares (OLS) for regression.","category":"page"},{"location":"general/different_training/","page":"Changing Training Algorithms","title":"Changing Training Algorithms","text":"Leveraging MLJLinearModels you can expand your choices of linear models for training. The wrappers provided follow this structure:","category":"page"},{"location":"general/different_training/","page":"Changing Training Algorithms","title":"Changing Training Algorithms","text":"struct LinearModel\n    regression::Any\n    solver::Any\n    regression_kwargs::Any\nend","category":"page"},{"location":"general/different_training/","page":"Changing Training Algorithms","title":"Changing Training Algorithms","text":"To call the ridge regression using the MLJLinearModels APIs, you can use LinearModel(;regression=LinearRegression). You can also choose a specific solver by calling, for example, LinearModel(regression=LinearRegression, solver=Analytical()). For all the available solvers, please refer to the MLJLinearModels documentation.","category":"page"},{"location":"general/different_training/","page":"Changing Training Algorithms","title":"Changing Training Algorithms","text":"To change the regularization coefficient in the ridge example, using for example lambda = 0.1, it is needed to pass it in the regression_kwargs like so LinearModel(;regression=LinearRegression, solver=Analytical(), regression_kwargs=(lambda=lambda)). The nomenclature of the coefficients must follow the MLJLinearModels APIs, using lambda, gamma for LassoRegression and delta, lambda, gamma for HuberRegression. Again, please check the relevant documentation if in doubt. When using MLJLinearModels based regressors, do remember to specify using MLJLinearModels.","category":"page"},{"location":"general/different_training/#Support-Vector-Regression","page":"Changing Training Algorithms","title":"Support Vector Regression","text":"","category":"section"},{"location":"general/different_training/","page":"Changing Training Algorithms","title":"Changing Training Algorithms","text":"Contrary to the LinearModels, no wrappers are needed for support vector regression. By using LIBSVM.jl, LIBSVM wrappers in Julia, it is possible to call both epsilonSVR() or nuSVR() directly in train(). For the full range of kernels provided and the parameters to call, we refer the user to the official documentation. Like before, if one intends to use LIBSVM regressors, it is necessary to specify using LIBSVM.","category":"page"},{"location":"api/states/#States-Modifications","page":"States Modifications","title":"States Modifications","text":"","category":"section"},{"location":"api/states/#Padding-and-Estension","page":"States Modifications","title":"Padding and Estension","text":"","category":"section"},{"location":"api/states/#ReservoirComputing.StandardStates","page":"States Modifications","title":"ReservoirComputing.StandardStates","text":"StandardStates()\n\nWhen this struct is employed, the states of the reservoir are not modified.\n\nExample\n\njulia> states = StandardStates()\nStandardStates()\n\njulia> test_vec = zeros(Float32, 5)\n5-element Vector{Float32}:\n 0.0\n 0.0\n 0.0\n 0.0\n 0.0\n\njulia> new_vec = states(test_vec)\n5-element Vector{Float32}:\n 0.0\n 0.0\n 0.0\n 0.0\n 0.0\n\njulia> test_mat = zeros(Float32, 5, 5)\n5×5 Matrix{Float32}:\n 0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n\njulia> new_mat = states(test_mat)\n5×5 Matrix{Float32}:\n 0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n\n\n\n\n\n","category":"type"},{"location":"api/states/#ReservoirComputing.ExtendedStates","page":"States Modifications","title":"ReservoirComputing.ExtendedStates","text":"ExtendedStates()\n\nThe ExtendedStates struct is used to extend the reservoir states by vertically concatenating the input data (during training) and the prediction data (during the prediction phase).\n\nExample\n\njulia> states = ExtendedStates()\nExtendedStates()\n\njulia> test_vec = zeros(Float32, 5)\n5-element Vector{Float32}:\n 0.0\n 0.0\n 0.0\n 0.0\n 0.0\n\njulia> new_vec = states(test_vec, fill(3.0f0, 3))\n8-element Vector{Float32}:\n 0.0\n 0.0\n 0.0\n 0.0\n 0.0\n 3.0\n 3.0\n 3.0\n\njulia> test_mat = zeros(Float32, 5, 5)\n5×5 Matrix{Float32}:\n 0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n\njulia> new_mat = states(test_mat, fill(3.0f0, 3))\n8×5 Matrix{Float32}:\n 0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n 3.0  3.0  3.0  3.0  3.0\n 3.0  3.0  3.0  3.0  3.0\n 3.0  3.0  3.0  3.0  3.0\n\n\n\n\n\n","category":"type"},{"location":"api/states/#ReservoirComputing.PaddedStates","page":"States Modifications","title":"ReservoirComputing.PaddedStates","text":"PaddedStates(padding)\nPaddedStates(;padding=1.0)\n\nCreates an instance of the PaddedStates struct with specified padding value (default 1.0). The states of the reservoir are padded by vertically concatenating the padding value.\n\nExample\n\njulia> states = PaddedStates(1.0)\nPaddedStates{Float64}(1.0)\n\njulia> test_vec = zeros(Float32, 5)\n5-element Vector{Float32}:\n 0.0\n 0.0\n 0.0\n 0.0\n 0.0\n\njulia> new_vec = states(test_vec)\n6-element Vector{Float32}:\n 0.0\n 0.0\n 0.0\n 0.0\n 0.0\n 1.0\n\njulia> test_mat = zeros(Float32, 5, 5)\n5×5 Matrix{Float32}:\n 0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n\njulia> new_mat = states(test_mat)\n6×5 Matrix{Float32}:\n 0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n 1.0  1.0  1.0  1.0  1.0\n\n\n\n\n\n","category":"type"},{"location":"api/states/#ReservoirComputing.PaddedExtendedStates","page":"States Modifications","title":"ReservoirComputing.PaddedExtendedStates","text":"PaddedExtendedStates(padding)\nPaddedExtendedStates(;padding=1.0)\n\nConstructs a PaddedExtendedStates struct, which first extends the reservoir states with training or prediction data,then pads them with a specified value (defaulting to 1.0).\n\nExample\n\njulia> states = PaddedExtendedStates(1.0)\nPaddedExtendedStates{Float64}(1.0)\n\njulia> test_vec = zeros(Float32, 5)\n5-element Vector{Float32}:\n 0.0\n 0.0\n 0.0\n 0.0\n 0.0\n\njulia> new_vec = states(test_vec, fill(3.0f0, 3))\n9-element Vector{Float32}:\n 0.0\n 0.0\n 0.0\n 0.0\n 0.0\n 1.0\n 3.0\n 3.0\n 3.0\n\njulia> test_mat = zeros(Float32, 5, 5)\n5×5 Matrix{Float32}:\n 0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n\njulia> new_mat = states(test_mat, fill(3.0f0, 3))\n9×5 Matrix{Float32}:\n 0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n 1.0  1.0  1.0  1.0  1.0\n 3.0  3.0  3.0  3.0  3.0\n 3.0  3.0  3.0  3.0  3.0\n 3.0  3.0  3.0  3.0  3.0\n\n\n\n\n\n","category":"type"},{"location":"api/states/#Non-Linear-Transformations","page":"States Modifications","title":"Non Linear Transformations","text":"","category":"section"},{"location":"api/states/#ReservoirComputing.NLADefault","page":"States Modifications","title":"ReservoirComputing.NLADefault","text":"NLADefault()\n\nNLADefault represents the default non-linear algorithm option. When used, it leaves the input array unchanged.\n\nExample\n\njulia> nlat = NLADefault()\nNLADefault()\n\njulia> x_old = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n10-element Vector{Int64}:\n 0\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n\njulia> n_new = nlat(x_old)\n10-element Vector{Int64}:\n 0\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n\njulia> mat_old = [1 2 3;\n                  4 5 6;\n                  7 8 9;\n                  10 11 12;\n                  13 14 15;\n                  16 17 18;\n                  19 20 21]\n7×3 Matrix{Int64}:\n  1   2   3\n  4   5   6\n  7   8   9\n 10  11  12\n 13  14  15\n 16  17  18\n 19  20  21\n\njulia> mat_new = nlat(mat_old)\n7×3 Matrix{Int64}:\n  1   2   3\n  4   5   6\n  7   8   9\n 10  11  12\n 13  14  15\n 16  17  18\n 19  20  21\n\n\n\n\n\n","category":"type"},{"location":"api/states/#ReservoirComputing.NLAT1","page":"States Modifications","title":"ReservoirComputing.NLAT1","text":"NLAT1()\n\nNLAT1 implements the T₁ transformation algorithm introduced in [Chattopadhyay] and [Pathak]. The T₁ algorithm squares elements of the input array, targeting every second row.\n\ntilder_ij =\nbegincases\n    r_ij times r_ij  textif  j text is odd \n    r_ij  textif  j text is even\nendcases\n\nExample\n\njulia> nlat = NLAT1()\nNLAT1()\n\njulia> x_old = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n10-element Vector{Int64}:\n 0\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n\njulia> n_new = nlat(x_old)\n10-element Vector{Int64}:\n  0\n  1\n  4\n  3\n 16\n  5\n 36\n  7\n 64\n  9\n\njulia> mat_old = [1  2  3;\n                   4  5  6;\n                   7  8  9;\n                  10 11 12;\n                  13 14 15;\n                  16 17 18;\n                  19 20 21]\n7×3 Matrix{Int64}:\n  1   2   3\n  4   5   6\n  7   8   9\n 10  11  12\n 13  14  15\n 16  17  18\n 19  20  21\n\njulia> mat_new = nlat(mat_old)\n7×3 Matrix{Int64}:\n   1    4    9\n   4    5    6\n  49   64   81\n  10   11   12\n 169  196  225\n  16   17   18\n 361  400  441\n\n\n[Chattopadhyay]: Chattopadhyay, Ashesh, et al. \"Data-driven prediction of a multi-scale Lorenz 96 chaotic system using a hierarchy of deep learning methods: Reservoir computing, ANN, and RNN-LSTM.\" (2019).\n\n[Pathak]: Pathak, Jaideep, et al. \"Model-free prediction of large spatiotemporally chaotic systems from data: A reservoir computing approach.\" Physical review letters 120.2 (2018): 024102.\n\n\n\n\n\n","category":"type"},{"location":"api/states/#ReservoirComputing.NLAT2","page":"States Modifications","title":"ReservoirComputing.NLAT2","text":"NLAT2()\n\nNLAT2 implements the T₂ transformation algorithm as defined in [Chattopadhyay]. This transformation algorithm modifies the reservoir states by multiplying each odd-indexed row (starting from the second row) with the product of its two preceding rows.\n\ntilder_ij =\nbegincases\n    r_ij-1 times r_ij-2  textif  j  1 text is odd \n    r_ij  textif  j text is 1 or even\nendcases\n\nExample\n\njulia> nlat = NLAT2()\nNLAT2()\n\njulia> x_old = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n10-element Vector{Int64}:\n 0\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n\njulia> n_new = nlat(x_old)\n10-element Vector{Int64}:\n  0\n  1\n  0\n  3\n  6\n  5\n 20\n  7\n 42\n  9\n\njulia> mat_old = [1  2  3;\n                   4  5  6;\n                   7  8  9;\n                  10 11 12;\n                  13 14 15;\n                  16 17 18;\n                  19 20 21]\n7×3 Matrix{Int64}:\n  1   2   3\n  4   5   6\n  7   8   9\n 10  11  12\n 13  14  15\n 16  17  18\n 19  20  21\n\njulia> mat_new = nlat(mat_old)\n7×3 Matrix{Int64}:\n  1   2    3\n  4   5    6\n  4  10   18\n 10  11   12\n 70  88  108\n 16  17   18\n 19  20   21\n\n\n[Chattopadhyay]: Chattopadhyay, Ashesh, et al. \"Data-driven prediction of a multi-scale Lorenz 96 chaotic system using a hierarchy of deep learning methods: Reservoir computing, ANN, and RNN-LSTM.\" (2019).\n\n\n\n\n\n","category":"type"},{"location":"api/states/#ReservoirComputing.NLAT3","page":"States Modifications","title":"ReservoirComputing.NLAT3","text":"NLAT3()\n\nImplements the T₃ transformation algorithm as detailed in [Chattopadhyay]. This algorithm modifies the reservoir's states by multiplying each odd-indexed row (beginning from the second row) with the product of the immediately preceding and the immediately following rows.\n\ntilder_ij =\nbegincases\nr_ij-1 times r_ij+1  textif  j  1 text is odd \nr_ij  textif  j = 1 text or even\nendcases\n\nExample\n\njulia> nlat = NLAT3()\nNLAT3()\n\njulia> x_old = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n10-element Vector{Int64}:\n 0\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n\njulia> n_new = nlat(x_old)\n10-element Vector{Int64}:\n  0\n  1\n  3\n  3\n 15\n  5\n 35\n  7\n 63\n  9\n\njulia> mat_old = [1  2  3;\n                   4  5  6;\n                   7  8  9;\n                  10 11 12;\n                  13 14 15;\n                  16 17 18;\n                  19 20 21]\n7×3 Matrix{Int64}:\n  1   2   3\n  4   5   6\n  7   8   9\n 10  11  12\n 13  14  15\n 16  17  18\n 19  20  21\n\njulia> mat_new = nlat(mat_old)\n7×3 Matrix{Int64}:\n   1    2    3\n   4    5    6\n  40   55   72\n  10   11   12\n 160  187  216\n  16   17   18\n  19   20   21\n\n\n[Chattopadhyay]: Chattopadhyay, Ashesh, et al. \"Data-driven predictions of a multiscale Lorenz 96 chaotic system using machine-learning methods: reservoir computing, artificial neural network, and long short-term memory network.\" (2019).\n\n\n\n\n\n","category":"type"},{"location":"api/states/#ReservoirComputing.PartialSquare","page":"States Modifications","title":"ReservoirComputing.PartialSquare","text":"PartialSquare(eta)\n\nImplement a partial squaring of the states as described in [barbosa2021].\n\nEquations\n\n    beginequation\n    g(r_i) =\n    begincases \n        r_i^2  textif  i leq eta_r N \n        r_i  textif  i  eta_r N\n    endcases\n    endequation\n\nExamples\n\n```jldoctest julia> ps = PartialSquare(0.6) PartialSquare(0.6)\n\njulia> x_old = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] 10-element Vector{Int64}:  0  1  2  3  4  5  6  7  8  9\n\njulia> xnew = ps(xold) 10-element Vector{Int64}:   0   1   4   9  16  25   6   7   8   9\n\n[barbosa2021]: Barbosa, Wendson AS, et al. \"Symmetry-aware reservoir computing.\" Physical Review E 104.4 (2021): 045307.\n\n\n\n\n\n","category":"type"},{"location":"api/states/#ReservoirComputing.ExtendedSquare","page":"States Modifications","title":"ReservoirComputing.ExtendedSquare","text":"ExtendedSquare()\n\nExtension of the Lu initialization proposed in [herteux2020]. The state vector is extended with the squared elements of the initial state\n\nEquations\n\nbeginequation\n    vecx = x_1 x_2 dots x_N x_1^2 x_2^2 dots x_N^2\nendequation\n\nExamples\n\njulia> x_old = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n9-element Vector{Int64}:\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n\njulia> es = ExtendedSquare()\nExtendedSquare()\n\njulia> x_new = es(x_old)\n18-element Vector{Int64}:\n  1\n  2\n  3\n  4\n  5\n  6\n  7\n  8\n  9\n  1\n  4\n  9\n 16\n 25\n 36\n 49\n 64\n 81\n\n\n[herteux2020]: Herteux, Joschka, and Christoph Räth. \"Breaking symmetries of the reservoir equations in echo state networks.\" Chaos: An Interdisciplinary Journal of Nonlinear Science 30.12 (2020).\n\n\n\n\n\n","category":"type"},{"location":"api/states/#Internals","page":"States Modifications","title":"Internals","text":"","category":"section"},{"location":"api/states/#ReservoirComputing.create_states","page":"States Modifications","title":"ReservoirComputing.create_states","text":"create_states(reservoir_driver::AbstractReservoirDriver, train_data, washout,\n    reservoir_matrix, input_matrix, bias_vector)\n\nCreate and return the trained Echo State Network (ESN) states according to the specified reservoir driver.\n\nArguments\n\nreservoir_driver: The reservoir driver that determines how the ESN states evolve over time.\ntrain_data: The training data used to train the ESN.\nwashout: The number of initial time steps to discard during training to allow the reservoir dynamics to wash out the initial conditions.\nreservoir_matrix: The reservoir matrix representing the dynamic, recurrent part of the ESN.\ninput_matrix: The input matrix that defines the connections between input features and reservoir nodes.\nbias_vector: The bias vector to be added at each time step during the reservoir update.\n\n\n\n\n\n","category":"function"},{"location":"api/esn_drivers/#ESN-Drivers","page":"ESN Drivers","title":"ESN Drivers","text":"","category":"section"},{"location":"api/esn_drivers/#ReservoirComputing.RNN","page":"ESN Drivers","title":"ReservoirComputing.RNN","text":"RNN(activation_function, leaky_coefficient)\nRNN(;activation_function=tanh, leaky_coefficient=1.0)\n\nReturns a Recurrent Neural Network (RNN) initializer for echo state networks (ESN).\n\nArguments\n\nactivation_function: The activation function used in the RNN.\nleaky_coefficient: The leaky coefficient used in the RNN.\n\nKeyword Arguments\n\nactivation_function: The activation function used in the RNN. Defaults to tanh_fast.\nleaky_coefficient: The leaky coefficient used in the RNN. Defaults to 1.0.\n\n\n\n\n\n","category":"type"},{"location":"api/esn_drivers/#ReservoirComputing.MRNN","page":"ESN Drivers","title":"ReservoirComputing.MRNN","text":"MRNN(activation_function, leaky_coefficient, scaling_factor)\nMRNN(;activation_function=[tanh, sigmoid], leaky_coefficient=1.0,\n    scaling_factor=fill(leaky_coefficient, length(activation_function)))\n\nReturns a Multiple RNN (MRNN) initializer for the Echo State Network (ESN), introduced in [Lun2015].\n\nArguments\n\nactivation_function: A vector of activation functions used in the MRNN.\nleaky_coefficient: The leaky coefficient used in the MRNN.\nscaling_factor: A vector of scaling factors for combining activation functions.\n\nKeyword Arguments\n\nactivation_function: A vector of activation functions used in the MRNN. Defaults to [tanh, sigmoid].\nleaky_coefficient: The leaky coefficient used in the MRNN. Defaults to 1.0.\nscaling_factor: A vector of scaling factors for combining activation functions. Defaults to an array of the same size as activation_function with all elements set to leaky_coefficient.\n\nThis function creates an MRNN object with the specified activation functions, leaky coefficient, and scaling factors, which can be used as a reservoir driver in the ESN.\n\n[Lun2015]: Lun, Shu-Xian, et al. \"A novel model of leaky integrator echo state network for time-series prediction.\" Neurocomputing 159 (2015): 58-66.\n\n\n\n\n\n","category":"type"},{"location":"api/esn_drivers/#ReservoirComputing.GRU","page":"ESN Drivers","title":"ReservoirComputing.GRU","text":"GRU(;activation_function=[NNlib.sigmoid, NNlib.sigmoid, tanh],\n    inner_layer = fill(DenseLayer(), 2),\n    reservoir = fill(RandSparseReservoir(), 2),\n    bias = fill(DenseLayer(), 2),\n    variant = FullyGated())\n\nReturns a Gated Recurrent Unit (GRU) reservoir driver for Echo State Network (ESN). This driver is based on the GRU architecture [Cho2014].\n\nArguments\n\nactivation_function: An array of activation functions for the GRU layers. By default, it uses sigmoid activation functions for the update gate, reset gate, and tanh for the hidden state.\ninner_layer: An array of inner layers used in the GRU architecture. By default, it uses two dense layers.\nreservoir: An array of reservoir layers. By default, it uses two random sparse reservoirs.\nbias: An array of bias layers for the GRU. By default, it uses two dense layers.\nvariant: The GRU variant to use. By default, it uses the \"FullyGated\" variant.\n\n[Cho2014]: Cho, Kyunghyun, et al. \"Learning phrase representations using RNN encoder-decoder for statistical machine translation.\" arXiv preprint arXiv:1406.1078 (2014).\n\n\n\n\n\n","category":"type"},{"location":"api/esn_drivers/","page":"ESN Drivers","title":"ESN Drivers","text":"The GRU driver also provides the user with the choice of the possible variants:","category":"page"},{"location":"api/esn_drivers/#ReservoirComputing.FullyGated","page":"ESN Drivers","title":"ReservoirComputing.FullyGated","text":"FullyGated()\n\nReturns a Fully Gated Recurrent Unit (FullyGated) initializer for the Echo State Network (ESN).\n\nReturns the standard gated recurrent unit [Cho2014] as a driver for the echo state network (ESN).\n\n[Cho2014]: Cho, Kyunghyun, et al. \"Learning phrase representations using RNN encoder-decoder for statistical machine translation.\" arXiv preprint arXiv:1406.1078 (2014).\n\n\n\n\n\n","category":"type"},{"location":"api/esn_drivers/#ReservoirComputing.Minimal","page":"ESN Drivers","title":"ReservoirComputing.Minimal","text":"Minimal()\n\nReturns a minimal GRU ESN initializer as described in [Zhou2016].\n\n[Zhou2016]: Zhou, Guo-Bing, et al. \"Minimal gated unit for recurrent neural networks.\" International Journal of Automation and Computing 13.3 (2016): 226-234.\n\n\n\n\n\n","category":"type"},{"location":"api/esn_drivers/","page":"ESN Drivers","title":"ESN Drivers","text":"Please refer to the original papers for more detail about these architectures.","category":"page"},{"location":"api/training/#Training-Algorithms","page":"Training Algorithms","title":"Training Algorithms","text":"","category":"section"},{"location":"api/training/#Linear-Models","page":"Training Algorithms","title":"Linear Models","text":"","category":"section"},{"location":"api/training/#ReservoirComputing.StandardRidge","page":"Training Algorithms","title":"ReservoirComputing.StandardRidge","text":"StandardRidge([Type], [reg])\n\nReturns a training method for train based on ridge regression. The equations for ridge regression are as follows:\n\nmathbfw = (mathbfX^top mathbfX + \nlambda mathbfI)^-1 mathbfX^top mathbfy\n\nArguments\n\nType: type of the regularization argument. Default is inferred internally, there's usually no need to tweak this\nreg: regularization coefficient. Default is set to 0.0 (linear regression).\n\n```\n\n\n\n\n\n","category":"type"},{"location":"api/training/#Gaussian-Regression","page":"Training Algorithms","title":"Gaussian Regression","text":"","category":"section"},{"location":"api/training/","page":"Training Algorithms","title":"Training Algorithms","text":"Currently, v0.10, is unavailable.","category":"page"},{"location":"api/training/#Support-Vector-Regression","page":"Training Algorithms","title":"Support Vector Regression","text":"","category":"section"},{"location":"api/training/","page":"Training Algorithms","title":"Training Algorithms","text":"Support Vector Regression is possible using a direct call to LIBSVM regression methods. Instead of a wrapper, please refer to the use of LIBSVM.AbstractSVR in the original library.","category":"page"},{"location":"api/esn_variations/#Echo-State-Networks-variations","page":"ESN Variations","title":"Echo State Networks variations","text":"","category":"section"},{"location":"api/esn_variations/#Deep-ESN","page":"ESN Variations","title":"Deep ESN","text":"","category":"section"},{"location":"api/esn_variations/#ReservoirComputing.DeepESN","page":"ESN Variations","title":"ReservoirComputing.DeepESN","text":"DeepESN(train_data, in_size, res_size; kwargs...)\n\nConstructs a Deep Echo State Network (ESN) model for processing sequential data through a layered architecture of reservoirs. This constructor allows for the creation of a deep learning model that benefits from the dynamic memory and temporal processing capabilities of ESNs, enhanced by the depth provided by multiple reservoir layers.\n\nParameters\n\ntrain_data: The training dataset used for the ESN. This should be structured as sequential data where applicable.\nin_size: The size of the input layer, i.e., the number of input units to the ESN.\nres_size: The size of each reservoir, i.e., the number of neurons in each hidden layer of the ESN.\n\nOptional Keyword Arguments\n\ndepth: The number of reservoir layers in the Deep ESN. Default is 2.\ninput_layer: A function or an array of functions to initialize the input matrices for each layer. Default is scaled_rand for each layer.\nbias: A function or an array of functions to initialize the bias vectors for each layer. Default is zeros32 for each layer.\nreservoir: A function or an array of functions to initialize the reservoir matrices for each layer. Default is rand_sparse for each layer.\nreservoir_driver: The driving system for the reservoir. Default is an RNN model.\nnla_type: The type of non-linear activation used in the reservoir. Default is NLADefault().\nstates_type: Defines the type of states used in the ESN (e.g., standard states). Default is StandardStates().\nwashout: The number of initial timesteps to be discarded in the ESN's training phase. Default is 0.\nrng: Random number generator used for initializing weights. Default is Utils.default_rng().\nmatrix_type: The type of matrix used for storing the training data. Default is inferred from train_data.\n\nExample\n\ntrain_data = rand(Float32, 3, 100)\n\n# Create a DeepESN with specific parameters\ndeepESN = DeepESN(train_data, 3, 100; depth = 3, washout = 100)\n\n\n\n\n\n","category":"type"},{"location":"api/esn_variations/#Hybrid-ESN","page":"ESN Variations","title":"Hybrid ESN","text":"","category":"section"},{"location":"api/esn_variations/#ReservoirComputing.HybridESN","page":"ESN Variations","title":"ReservoirComputing.HybridESN","text":"HybridESN(model, train_data, in_size, res_size; kwargs...)\n\nConstruct a Hybrid Echo State Network (ESN) model that integrates traditional Echo State Networks with a predefined knowledge model [Pathak2018].\n\nParameters\n\nmodel: A KnowledgeModel instance representing the knowledge-based model to be integrated with the ESN.\ntrain_data: The training dataset used for the ESN. This data can be preprocessed or raw data depending on the nature of the problem and the preprocessing steps considered.\nin_size: The size of the input layer, i.e., the number of input units to the ESN.\nres_size: The size of the reservoir, i.e., the number of neurons in the hidden layer of the ESN.\n\nOptional Keyword Arguments\n\ninput_layer: A function to initialize the input matrix. Default is scaled_rand.\nreservoir: A function to initialize the reservoir matrix. Default is rand_sparse.\nbias: A function to initialize the bias vector. Default is zeros32.\nreservoir_driver: The driving system for the reservoir. Default is an RNN model.\nnla_type: The type of non-linear activation used in the reservoir. Default is NLADefault().\nstates_type: Defines the type of states used in the ESN. Default is StandardStates().\nwashout: The number of initial timesteps to be discarded in the ESN's training phase. Default is 0.\nrng: Random number generator used for initializing weights. Default is Utils.default_rng().\nT: The data type for the matrices (e.g., Float32).\nmatrix_type: The type of matrix used for storing the training data. Default is inferred from train_data.\n\n[Pathak2018]: Jaideep Pathak et al. \"Hybrid Forecasting of Chaotic Processes: Using Machine Learning in Conjunction with a Knowledge-Based Model\" (2018).\n\n\n\n\n\n","category":"type"},{"location":"api/esn_variations/#ReservoirComputing.KnowledgeModel","page":"ESN Variations","title":"ReservoirComputing.KnowledgeModel","text":"KnowledgeModel(prior_model, u0, tspan, datasize)\n\nConstructs a Hybrid variation of Echo State Networks (ESNs) [Pathak2018] integrating a knowledge-based model (prior_model) with ESNs.\n\nParameters\n\nprior_model: A knowledge-based model function for integration with ESNs.\nu0: Initial conditions for the model.\ntspan: Time span as a tuple, indicating the duration for model operation.\ndatasize: The size of the data to be processed.\n\n[Pathak2018]: Jaideep Pathak et al. \"Hybrid Forecasting of Chaotic Processes: Using Machine Learning in Conjunction with a Knowledge-Based Model\" (2018).\n\n\n\n\n\n","category":"type"},{"location":"#ReservoirComputing.jl","page":"ReservoirComputing.jl","title":"ReservoirComputing.jl","text":"","category":"section"},{"location":"","page":"ReservoirComputing.jl","title":"ReservoirComputing.jl","text":"ReservoirComputing.jl is a versatile and user-friendly Julia package designed for the implementation of Reservoir Computing models, such as Echo State Networks (ESNs). Reservoir Computing expands the input data into a higher-dimensional space, leveraging regression techniques for effective model training. This approach can be thought as a kernel method with an explicit kernel trick.","category":"page"},{"location":"","page":"ReservoirComputing.jl","title":"ReservoirComputing.jl","text":"info: Introductory material\nThis library assumes some basic knowledge of Reservoir Computing. For a good introduction, we suggest the following papers: the first two are the seminal papers about ESN and liquid state machines, the others are in-depth review papers that should cover all the needed information. For the majority of the algorithms implemented in this library we cited in the documentation the original work introducing them. If you ever are in doubt about a method or a function just type ? function in the Julia REPL to read the relevant notes.Jaeger, Herbert: The “echo state” approach to analyzing and training   recurrent neural networks-with an erratum note.\nMaass W, Natschläger T, Markram H: Real-time computing without   stable states: a new framework for neural computation based on   perturbations.\nLukoševičius, Mantas: A practical guide to applying echo state networks.   Neural networks: Tricks of the trade.\nLukoševičius, Mantas, and Herbert Jaeger: Reservoir computing approaches   to recurrent neural network training.","category":"page"},{"location":"","page":"ReservoirComputing.jl","title":"ReservoirComputing.jl","text":"info: Performance tip\nFor faster computations on the CPU it is suggested to add using MKL to the script. For clarity's sake this library will not be indicated  under every example in the documentation.","category":"page"},{"location":"#Installation","page":"ReservoirComputing.jl","title":"Installation","text":"","category":"section"},{"location":"","page":"ReservoirComputing.jl","title":"ReservoirComputing.jl","text":"To install ReservoirComputing.jl, ensure you have Julia version 1.10 or higher. Follow these steps:","category":"page"},{"location":"","page":"ReservoirComputing.jl","title":"ReservoirComputing.jl","text":"1. Open the Julia command line.\n2. Enter the Pkg REPL mode by pressing ].\n3. Type `add ReservoirComputing` and press Enter.","category":"page"},{"location":"","page":"ReservoirComputing.jl","title":"ReservoirComputing.jl","text":"For a more customized installation or to contribute to the package, consider cloning the repository:","category":"page"},{"location":"","page":"ReservoirComputing.jl","title":"ReservoirComputing.jl","text":"using Pkg\nPkg.clone(\"https://github.com/SciML/ReservoirComputing.jl.git\")","category":"page"},{"location":"","page":"ReservoirComputing.jl","title":"ReservoirComputing.jl","text":"or dev the package.","category":"page"},{"location":"#Features-Overview","page":"ReservoirComputing.jl","title":"Features Overview","text":"","category":"section"},{"location":"","page":"ReservoirComputing.jl","title":"ReservoirComputing.jl","text":"Multiple Training Algorithms: Supports Ridge Regression, Linear Models,   and LIBSVM regression methods for Reservoir Computing models.\nDiverse Prediction Methods: Offers both generative and predictive methods   for Reservoir Computing predictions.\nModifiable Training and Prediction: Allows modifications in Reservoir   Computing states, such as state extension, padding, and combination methods.\nNon-linear Algorithm Options: Includes options for non-linear   modifications in algorithms.\nEcho State Networks (ESNs): Features various input layers, reservoirs,   and methods for driving ESN reservoir states.\nCellular Automata-Based Reservoir Computing: Introduces models based   on one-dimensional Cellular Automata for Reservoir Computing.","category":"page"},{"location":"#Contributing","page":"ReservoirComputing.jl","title":"Contributing","text":"","category":"section"},{"location":"","page":"ReservoirComputing.jl","title":"ReservoirComputing.jl","text":"Contributions to ReservoirComputing.jl are highly encouraged and appreciated. Whether it's through implementing new RC model variations, enhancing documentation, adding examples, or any improvement, your contribution is valuable. We welcome posts of relevant papers or ideas in the issues section. For deeper insights into the library's functionality, the API section in the documentation is a great resource. For any queries not suited for issues, please reach out to the lead developers via Slack or email.","category":"page"},{"location":"#Citing","page":"ReservoirComputing.jl","title":"Citing","text":"","category":"section"},{"location":"","page":"ReservoirComputing.jl","title":"ReservoirComputing.jl","text":"If you use ReservoirComputing.jl in your work, we kindly ask you to cite it. Here is the BibTeX entry for your convenience:","category":"page"},{"location":"","page":"ReservoirComputing.jl","title":"ReservoirComputing.jl","text":"@article{JMLR:v23:22-0611,\n  author  = {Francesco Martinuzzi and Chris Rackauckas and Anas Abdelrehim and Miguel D. Mahecha and Karin Mora},\n  title   = {ReservoirComputing.jl: An Efficient and Modular Library for Reservoir Computing Models},\n  journal = {Journal of Machine Learning Research},\n  year    = {2022},\n  volume  = {23},\n  number  = {288},\n  pages   = {1--8},\n  url     = {http://jmlr.org/papers/v23/22-0611.html}\n}","category":"page"},{"location":"#Reproducibility","page":"ReservoirComputing.jl","title":"Reproducibility","text":"","category":"section"},{"location":"","page":"ReservoirComputing.jl","title":"ReservoirComputing.jl","text":"<details><summary>The documentation of this SciML package was built using these direct dependencies,</summary>","category":"page"},{"location":"","page":"ReservoirComputing.jl","title":"ReservoirComputing.jl","text":"using Pkg # hide\nPkg.status() # hide","category":"page"},{"location":"","page":"ReservoirComputing.jl","title":"ReservoirComputing.jl","text":"</details>","category":"page"},{"location":"","page":"ReservoirComputing.jl","title":"ReservoirComputing.jl","text":"<details><summary>and using this machine and Julia version.</summary>","category":"page"},{"location":"","page":"ReservoirComputing.jl","title":"ReservoirComputing.jl","text":"using InteractiveUtils # hide\nversioninfo() # hide","category":"page"},{"location":"","page":"ReservoirComputing.jl","title":"ReservoirComputing.jl","text":"</details>","category":"page"},{"location":"","page":"ReservoirComputing.jl","title":"ReservoirComputing.jl","text":"<details><summary>A more complete overview of all dependencies and their versions is also provided.</summary>","category":"page"},{"location":"","page":"ReservoirComputing.jl","title":"ReservoirComputing.jl","text":"using Pkg # hide\nPkg.status(; mode=PKGMODE_MANIFEST) # hide","category":"page"},{"location":"","page":"ReservoirComputing.jl","title":"ReservoirComputing.jl","text":"</details>","category":"page"},{"location":"","page":"ReservoirComputing.jl","title":"ReservoirComputing.jl","text":"using TOML\nusing Markdown\nversion = TOML.parse(read(\"../../Project.toml\", String))[\"version\"]\nname = TOML.parse(read(\"../../Project.toml\", String))[\"name\"]\nlink_manifest = \"https://github.com/SciML/\" * name * \".jl/tree/gh-pages/v\" * version *\n                \"/assets/Manifest.toml\"\nlink_project = \"https://github.com/SciML/\" * name * \".jl/tree/gh-pages/v\" * version *\n               \"/assets/Project.toml\"\nMarkdown.parse(\"\"\"You can also download the\n[manifest]($link_manifest)\nfile and the\n[project]($link_project)\nfile.\n\"\"\")","category":"page"},{"location":"api/inits/#Echo-State-Networks-Initializers","page":"ESN Initializers","title":"Echo State Networks Initializers","text":"","category":"section"},{"location":"api/inits/#Input-layers","page":"ESN Initializers","title":"Input layers","text":"","category":"section"},{"location":"api/inits/#ReservoirComputing.scaled_rand","page":"ESN Initializers","title":"ReservoirComputing.scaled_rand","text":"scaled_rand([rng], [T], dims...;\n    scaling=0.1)\n\nCreate and return a matrix with random values, uniformly distributed within a range defined by scaling.\n\nArguments\n\nrng: Random number generator. Default is Utils.default_rng() from WeightInitializers.\nT: Type of the elements in the reservoir matrix. Default is Float32.\ndims: Dimensions of the matrix. Should follow res_size x in_size.\n\nKeyword arguments\n\nscaling: A scaling factor to define the range of the uniform distribution. The matrix elements will be randomly chosen from the range [-scaling, scaling]. Defaults to 0.1.\n\nExamples\n\njulia> res_input = scaled_rand(8, 3)\n8×3 Matrix{Float32}:\n -0.0669356  -0.0292692  -0.0188943\n  0.0159724   0.004071   -0.0737949\n  0.026355   -0.0191563   0.0714962\n -0.0177412   0.0279123   0.0892906\n -0.0184405   0.0567368   0.0190222\n  0.0944272   0.0679244   0.0148647\n -0.0799005  -0.0891089  -0.0444782\n -0.0970182   0.0934286   0.03553\n\n\n\n\n\n","category":"function"},{"location":"api/inits/#ReservoirComputing.weighted_init","page":"ESN Initializers","title":"ReservoirComputing.weighted_init","text":"weighted_init([rng], [T], dims...;\n    scaling=0.1, return_sparse=false)\n\nCreate and return a matrix representing a weighted input layer. This initializer generates a weighted input matrix with random non-zero elements distributed uniformly within the range [-scaling, scaling] [lu2017].\n\nArguments\n\nrng: Random number generator. Default is Utils.default_rng() from WeightInitializers.\nT: Type of the elements in the reservoir matrix. Default is Float32.\ndims: Dimensions of the matrix. Should follow res_size x in_size.\n\nKeyword arguments\n\nscaling: The scaling factor for the weight distribution. Defaults to 0.1.\nreturn_sparse: flag for returning a sparse matrix. Default is false.\n\nExamples\n\njulia> res_input = weighted_init(8, 3)\n6×3 Matrix{Float32}:\n  0.0452399   0.0          0.0\n -0.0348047   0.0          0.0\n  0.0        -0.0386004    0.0\n  0.0         0.00981022   0.0\n  0.0         0.0          0.0577838\n  0.0         0.0         -0.0562827\n\n[lu2017]: Lu, Zhixin, et al. \"Reservoir observers: Model-free inference of unmeasured variables in chaotic systems.\" Chaos: An Interdisciplinary Journal of Nonlinear Science 27.4 (2017): 041102.\n\n\n\n\n\n","category":"function"},{"location":"api/inits/#ReservoirComputing.minimal_init","page":"ESN Initializers","title":"ReservoirComputing.minimal_init","text":"minimal_init([rng], [T], dims...;\n    sampling_type=:bernoulli_sample!, weight=0.1, irrational=pi,\n    start=1, p=0.5)\n\nCreate a layer matrix with uniform weights determined by weight [rodan2010]. The sign difference is randomly determined by the sampling chosen.\n\nArguments\n\nrng: Random number generator. Default is Utils.default_rng() from WeightInitializers.\nT: Type of the elements in the reservoir matrix. Default is Float32.\ndims: Dimensions of the matrix. Should follow res_size x in_size.\n\nKeyword arguments\n\nweight: The weight used to fill the layer matrix. Default is 0.1.\nsampling_type: Sampling that decides the distribution of weight negative numbers. If set to :no_sample the sign is unchanged. If set to :bernoulli_sample! then each weight can be positive with a probability set by positive_prob. If set to :irrational_sample! the weight is negative if the decimal number of the irrational number chosen is odd. If set to :regular_sample!, each weight will be assigned a negative sign after the chosen strides. strides can be a single number or an array. Default is :no_sample.\npositive_prob: probability of the weight being positive when sampling_type is set to :bernoulli_sample!. Default is 0.5.\nirrational: Irrational number whose decimals decide the sign of weight. Default is pi.\nstart: Which place after the decimal point the counting starts for the irrational sign counting. Default is 1.\nstrides: number of strides for assigning negative value to a weight. It can be an integer or an array. Default is 2.\n\nExamples\n\njulia> res_input = minimal_init(8, 3)\n8×3 Matrix{Float32}:\n  0.1  -0.1   0.1\n -0.1   0.1   0.1\n -0.1  -0.1   0.1\n -0.1  -0.1  -0.1\n  0.1   0.1   0.1\n -0.1  -0.1  -0.1\n -0.1  -0.1   0.1\n  0.1  -0.1   0.1\n\njulia> res_input = minimal_init(8, 3; sampling_type = :irrational)\n8×3 Matrix{Float32}:\n -0.1   0.1  -0.1\n  0.1  -0.1  -0.1\n  0.1   0.1  -0.1\n  0.1   0.1   0.1\n -0.1  -0.1  -0.1\n  0.1   0.1   0.1\n  0.1   0.1  -0.1\n -0.1   0.1  -0.1\n\njulia> res_input = minimal_init(8, 3; p = 0.1) # lower p -> more negative signs\n8×3 Matrix{Float32}:\n -0.1  -0.1  -0.1\n -0.1  -0.1  -0.1\n -0.1  -0.1  -0.1\n -0.1  -0.1  -0.1\n  0.1  -0.1  -0.1\n -0.1  -0.1  -0.1\n -0.1  -0.1  -0.1\n -0.1  -0.1  -0.1\n\njulia> res_input = minimal_init(8, 3; p = 0.8)# higher p -> more positive signs\n8×3 Matrix{Float32}:\n  0.1   0.1  0.1\n -0.1   0.1  0.1\n -0.1   0.1  0.1\n  0.1   0.1  0.1\n  0.1   0.1  0.1\n  0.1  -0.1  0.1\n -0.1   0.1  0.1\n  0.1   0.1  0.1\n\n[rodan2010]: Rodan, Ali, and Peter Tino. \"Minimum complexity echo state network.\" IEEE transactions on neural networks 22.1 (2010): 131-144.\n\n\n\n\n\n","category":"function"},{"location":"api/inits/#ReservoirComputing.weighted_minimal","page":"ESN Initializers","title":"ReservoirComputing.weighted_minimal","text":"weighted_minimal([rng], [T], dims...;\n    weight=0.1, return_sparse=false,\n    sampling_type=:no_sample)\n\nCreate and return a minimal weighted input layer matrix. This initializer generates a weighted input matrix with equal, deterministic elements in the same construction as [weighted_minimal](@ref), inspired by [lu2017].\n\nPlease note that this initializer computes its own reservoir size! If the computed reservoir size is different than the provided one it will raise a warning.\n\nArguments\n\nrng: Random number generator. Default is Utils.default_rng() from WeightInitializers.\nT: Type of the elements in the reservoir matrix. Default is Float32.\ndims: Dimensions of the matrix. Should follow res_size x in_size.\n\nKeyword arguments\n\nweight: The value for all the weights in the input matrix. Defaults to 0.1.\nreturn_sparse: flag for returning a sparse matrix. Default is false.\nsampling_type: Sampling that decides the distribution of weight negative numbers. If set to :no_sample the sign is unchanged. If set to :bernoulli_sample! then each weight can be positive with a probability set by positive_prob. If set to :irrational_sample! the weight is negative if the decimal number of the irrational number chosen is odd. If set to :regular_sample!, each weight will be assigned a negative sign after the chosen strides. strides can be a single number or an array. Default is :no_sample.\npositive_prob: probability of the weight being positive when sampling_type is set to :bernoulli_sample!. Default is 0.5.\nirrational: Irrational number whose decimals decide the sign of weight. Default is pi.\nstart: Which place after the decimal point the counting starts for the irrational sign counting. Default is 1.\nstrides: number of strides for assigning negative value to a weight. It can be an integer or an array. Default is 2.\n\nExamples\n\njulia> res_input = weighted_minimal(8, 3)\n┌ Warning: Reservoir size has changed!\n│ \n│     Computed reservoir size (6) does not equal the provided reservoir size (8). \n│  \n│     Using computed value (6). Make sure to modify the reservoir initializer accordingly. \n│ \n└ @ ReservoirComputing ~/.julia/dev/ReservoirComputing/src/esn/esn_inits.jl:159\n6×3 Matrix{Float32}:\n 0.1  0.0  0.0\n 0.1  0.0  0.0\n 0.0  0.1  0.0\n 0.0  0.1  0.0\n 0.0  0.0  0.1\n 0.0  0.0  0.1\n\njulia> res_input = weighted_minimal(9, 3; weight = 0.99)\n9×3 Matrix{Float32}:\n 0.99  0.0   0.0\n 0.99  0.0   0.0\n 0.99  0.0   0.0\n 0.0   0.99  0.0\n 0.0   0.99  0.0\n 0.0   0.99  0.0\n 0.0   0.0   0.99\n 0.0   0.0   0.99\n 0.0   0.0   0.99\n\njulia> res_input = weighted_minimal(9, 3; sampling_type = :bernoulli_sample!)\n9×3 Matrix{Float32}:\n  0.1  -0.0  -0.0\n -0.1  -0.0  -0.0\n  0.1  -0.0   0.0\n -0.0   0.1   0.0\n  0.0   0.1  -0.0\n  0.0   0.1   0.0\n -0.0  -0.0  -0.1\n -0.0  -0.0   0.1\n  0.0  -0.0   0.1\n\n[lu2017]: Lu, Zhixin, et al. \"Reservoir observers: Model-free inference of unmeasured variables in chaotic systems.\" Chaos: An Interdisciplinary Journal of Nonlinear Science 27.4 (2017): 041102.\n\n\n\n\n\n","category":"function"},{"location":"api/inits/#ReservoirComputing.chebyshev_mapping","page":"ESN Initializers","title":"ReservoirComputing.chebyshev_mapping","text":"chebyshev_mapping([rng], [T], dims...;\n    amplitude=one(T), sine_divisor=one(T),\n    chebyshev_parameter=one(T), return_sparse=false)\n\nGenerate a Chebyshev-mapped matrix [xie2024]. The first row is initialized using a sine function and subsequent rows are iteratively generated via the Chebyshev mapping. The first row is defined as:\n\n    W1 j = textamplitude cdot sin(j cdot pi  (textsine_divisor \n        cdot textn_cols))\n\nfor j = 1, 2, …, ncols (with ncols typically equal to K+1, where K is the number of input layer neurons). Subsequent rows are generated by applying the mapping:\n\n    Wi+1 j = cos( textchebyshev_parameter cdot acos(Wpi j))\n\nArguments\n\nrng: Random number generator. Default is Utils.default_rng() from WeightInitializers.\nT: Type of the elements in the reservoir matrix. Default is Float32.\ndims: Dimensions of the matrix. Should follow res_size x in_size. res_size is assumed to be K+1.\n\nKeyword arguments\n\namplitude: Scaling factor used to initialize the first row. This parameter adjusts the amplitude of the sine function. Default value is one.\nsine_divisor: Divisor applied in the sine function's phase. Default value is one.\nchebyshev_parameter: Control parameter for the Chebyshev mapping in subsequent rows. This parameter influences the distribution of the matrix elements. Default is one.\nreturn_sparse: If true, the function returns the matrix as a sparse matrix. Default is false.\n\nExamples\n\njulia> input_matrix = chebyshev_mapping(10, 3)\n10×3 Matrix{Float32}:\n 0.866025  0.866025   1.22465f-16\n 0.866025  0.866025  -4.37114f-8\n 0.866025  0.866025  -4.37114f-8\n 0.866025  0.866025  -4.37114f-8\n 0.866025  0.866025  -4.37114f-8\n 0.866025  0.866025  -4.37114f-8\n 0.866025  0.866025  -4.37114f-8\n 0.866025  0.866025  -4.37114f-8\n 0.866025  0.866025  -4.37114f-8\n 0.866025  0.866025  -4.37114f-8\n\n[xie2024]: Xie, Minzhi, Qianxue Wang, and Simin Yu. \"Time Series Prediction of ESN Based on Chebyshev Mapping and Strongly Connected Topology.\" Neural Processing Letters 56.1 (2024): 30.\n\n\n\n\n\n","category":"function"},{"location":"api/inits/#ReservoirComputing.logistic_mapping","page":"ESN Initializers","title":"ReservoirComputing.logistic_mapping","text":"logistic_mapping([rng], [T], dims...;\n    amplitude=0.3, sine_divisor=5.9, logistic_parameter=3.7,\n    return_sparse=false)\n\nGenerate an input weight matrix using a logistic mapping [wang2022].The first row is initialized using a sine function:\n\n    W1 j = textamplitude cdot sin(j cdot pi  \n        (textsine_divisor cdot in_size))\n\nfor each input index j, with in_size being the number of columns provided in dims. Subsequent rows are generated recursively using the logistic map recurrence:\n\n    Wi+1 j = textlogistic_parameter cdot W(i j) cdot (1 - Wi j)\n\nArguments\n\nrng: Random number generator. Default is Utils.default_rng() from WeightInitializers.\nT: Type of the elements in the reservoir matrix. Default is Float32.\ndims: Dimensions of the matrix. Should follow res_size x in_size.\n\nKeyword arguments\n\namplitude: Scaling parameter used in the sine initialization of the first row. Default is 0.3.\nsine_divisor: Parameter used to adjust the phase in the sine initialization. Default is 5.9.\nlogistic_parameter: The parameter in the logistic mapping recurrence that governs the dynamics. Default is 3.7.\nreturn_sparse: If true, returns the resulting matrix as a sparse matrix. Default is false.\n\nExamples\n\njulia> logistic_mapping(8, 3)\n8×3 Matrix{Float32}:\n 0.0529682  0.104272  0.1523\n 0.185602   0.345578  0.477687\n 0.559268   0.836769  0.923158\n 0.912003   0.50537   0.262468\n 0.296938   0.924893  0.716241\n 0.772434   0.257023  0.751987\n 0.650385   0.70656   0.69006\n 0.841322   0.767132  0.791346\n\n\n[wang2022]: Wang, Heshan, et al. \"Echo state network with logistic mapping and bias dropout for time series prediction.\" Neurocomputing 489 (2022): 196-210.\n\n\n\n\n\n","category":"function"},{"location":"api/inits/#ReservoirComputing.modified_lm","page":"ESN Initializers","title":"ReservoirComputing.modified_lm","text":"modified_lm([rng], [T], dims...;\n    factor, amplitude=0.3, sine_divisor=5.9, logistic_parameter=2.35,\n    return_sparse=false)\n\nGenerate a input weight matrix based on the logistic mapping [viehweg2025]. The matrix is built so that each input is transformed into a high-dimensional feature space via a recursive logistic map. For each input, a chain of weights is generated as follows:\n\nThe first element of the chain is initialized using a sine function:\n\n      W1j = textamplitude cdot sin( (j cdot pi)  \n          (textfactor cdot textn cdot textsine_divisor) )\n\nwhere j is the index corresponding to the input and n is the number of inputs.\n\nSubsequent elements are recursively computed using the logistic mapping:\n\n      Wi+1j = textlogistic_parameter cdot Wij cdot (1 - Wij)\n\nThe resulting matrix has dimensions (factor * in_size) x in_size, where in_size corresponds to the number of columns provided in dims. If the provided number of rows does not match factor * in_size  the number of rows is overridden.\n\nArguments\n\nrng: Random number generator. Default is Utils.default_rng() from WeightInitializers.\nT: Type of the elements in the reservoir matrix. Default is Float32.\ndims: Dimensions of the matrix. Should follow res_size x in_size.\n\nKeyword arguments\n\nfactor: The number of logistic map iterations (chain length) per input, determining the number of rows per input.\namplitude: Scaling parameter A for the sine-based initialization of the first element in each logistic chain. Default is 0.3.\nsine_divisor: Parameter B used to adjust the phase in the sine initialization. Default is 5.9.\nlogistic_parameter: The parameter r in the logistic recurrence that governs the chain dynamics. Default is 2.35.\nreturn_sparse: If true, returns the resulting matrix as a sparse matrix. Default is false.\n\nExamples\n\njulia> modified_lm(20, 10; factor=2)\n20×10 SparseArrays.SparseMatrixCSC{Float32, Int64} with 18 stored entries:\n⎡⢠⠀⠀⠀⠀⎤\n⎢⠀⢣⠀⠀⠀⎥\n⎢⠀⠀⢣⠀⠀⎥\n⎢⠀⠀⠀⢣⠀⎥\n⎣⠀⠀⠀⠀⢣⎦\n\njulia> modified_lm(12, 4; factor=3)\n12×4 SparseArrays.SparseMatrixCSC{Float32, Int64} with 9 stored entries:\n  ⋅    ⋅          ⋅          ⋅ \n  ⋅    ⋅          ⋅          ⋅ \n  ⋅    ⋅          ⋅          ⋅ \n  ⋅   0.0133075   ⋅          ⋅ \n  ⋅   0.0308564   ⋅          ⋅ \n  ⋅   0.070275    ⋅          ⋅ \n  ⋅    ⋅         0.0265887   ⋅ \n  ⋅    ⋅         0.0608222   ⋅ \n  ⋅    ⋅         0.134239    ⋅ \n  ⋅    ⋅          ⋅         0.0398177\n  ⋅    ⋅          ⋅         0.0898457\n  ⋅    ⋅          ⋅         0.192168\n\n\n[viehweg2025]: Viehweg, Johannes, Constanze Poll, and Patrick Mäder. \"Deterministic Reservoir Computing for Chaotic Time Series Prediction.\" arXiv preprint arXiv:2501.15615 (2025).\n\n\n\n\n\n","category":"function"},{"location":"api/inits/#ReservoirComputing.informed_init","page":"ESN Initializers","title":"ReservoirComputing.informed_init","text":"informed_init([rng], [T], dims...;\n    scaling=0.1, model_in_size, gamma=0.5)\n\nCreate an input layer for informed echo state networks [pathak2018].\n\nArguments\n\nrng: Random number generator. Default is Utils.default_rng() from WeightInitializers.\nT: Type of the elements in the reservoir matrix. Default is Float32.\ndims: Dimensions of the matrix. Should follow res_size x in_size.\n\nKeyword arguments\n\nscaling: The scaling factor for the input matrix. Default is 0.1.\nmodel_in_size: The size of the input model.\ngamma: The gamma value. Default is 0.5.\n\nExamples\n\n[pathak2018]: Pathak, Jaideep, et al. \"Hybrid forecasting of chaotic processes: Using machine learning in conjunction with a knowledge-based model.\" Chaos: An Interdisciplinary Journal of Nonlinear Science 28.4 (2018).\n\n\n\n\n\n","category":"function"},{"location":"api/inits/#Reservoirs","page":"ESN Initializers","title":"Reservoirs","text":"","category":"section"},{"location":"api/inits/#ReservoirComputing.rand_sparse","page":"ESN Initializers","title":"ReservoirComputing.rand_sparse","text":"rand_sparse([rng], [T], dims...;\n    radius=1.0, sparsity=0.1, std=1.0, return_sparse=false)\n\nCreate and return a random sparse reservoir matrix. The matrix will be of size specified by dims, with specified sparsity and scaled spectral radius according to radius.\n\nArguments\n\nrng: Random number generator. Default is Utils.default_rng() from WeightInitializers.\nT: Type of the elements in the reservoir matrix. Default is Float32.\ndims: Dimensions of the reservoir matrix.\n\nKeyword arguments\n\nradius: The desired spectral radius of the reservoir. Defaults to 1.0.\nsparsity: The sparsity level of the reservoir matrix, controlling the fraction of zero elements. Defaults to 0.1.\nreturn_sparse: flag for returning a sparse matrix. Default is false.\n\nExamples\n\njulia> res_matrix = rand_sparse(5, 5; sparsity = 0.5)\n5×5 Matrix{Float32}:\n 0.0        0.0        0.0        0.0      0.0\n 0.0        0.794565   0.0        0.26164  0.0\n 0.0        0.0       -0.931294   0.0      0.553706\n 0.723235  -0.524727   0.0        0.0      0.0\n 1.23723    0.0        0.181824  -1.5478   0.465328\n\n\n\n\n\n","category":"function"},{"location":"api/inits/#ReservoirComputing.pseudo_svd","page":"ESN Initializers","title":"ReservoirComputing.pseudo_svd","text":"pseudo_svd([rng], [T], dims...; \n    max_value=1.0, sparsity=0.1, sorted=true, reverse_sort=false,\n    return_sparse=false)\n\nReturns an initializer to build a sparse reservoir matrix with the given sparsity by using a pseudo-SVD approach as described in [yang2018].\n\nArguments\n\nrng: Random number generator. Default is Utils.default_rng() from WeightInitializers.\nT: Type of the elements in the reservoir matrix. Default is Float32.\ndims: Dimensions of the reservoir matrix.\n\nKeyword arguments\n\nmax_value: The maximum absolute value of elements in the matrix. Default is 1.0\nsparsity: The desired sparsity level of the reservoir matrix. Default is 0.1\nsorted: A boolean indicating whether to sort the singular values before creating the diagonal matrix. Default is true.\nreverse_sort: A boolean indicating whether to reverse the sorted singular values. Default is false.\nreturn_sparse: flag for returning a sparse matrix. Default is false.\nreturn_diag: flag for returning a Diagonal matrix. If both return_diag and return_sparse are set to true priority is given to return_diag. Default is false.\n\nExamples\n\njulia> res_matrix = pseudo_svd(5, 5)\n5×5 Matrix{Float32}:\n 0.306998  0.0       0.0       0.0       0.0\n 0.0       0.325977  0.0       0.0       0.0\n 0.0       0.0       0.549051  0.0       0.0\n 0.0       0.0       0.0       0.726199  0.0\n 0.0       0.0       0.0       0.0       1.0\n\n[yang2018]: Yang, Cuili, et al. \"Design of polynomial echo state networks for time series prediction.\" Neurocomputing 290 (2018): 148-160.\n\n\n\n\n\n","category":"function"},{"location":"api/inits/#ReservoirComputing.chaotic_init","page":"ESN Initializers","title":"ReservoirComputing.chaotic_init","text":"chaotic_init([rng], [T], dims...;\n    extra_edge_probability=T(0.1), spectral_radius=one(T),\n    return_sparse=false)\n\nConstruct a chaotic reservoir matrix using a digital chaotic system [xie2024].\n\nThe matrix topology is derived from a strongly connected adjacency matrix based on a digital chaotic system operating at finite precision. If the requested matrix order does not exactly match a valid order the closest valid order is used.\n\nArguments\n\nrng: Random number generator. Default is Utils.default_rng() from WeightInitializers.\nT: Type of the elements in the reservoir matrix. Default is Float32.\ndims: Dimensions of the reservoir matrix.\n\nKeyword arguments\n\nextra_edge_probability: Probability of adding extra random edges in the adjacency matrix to enhance connectivity. Default is 0.1.\ndesired_spectral_radius: The target spectral radius for the reservoir matrix. Default is one.\nreturn_sparse: If true, the function returns the reservoir matrix as a sparse matrix. Default is false.\n\nExamples\n\njulia> res_matrix = chaotic_init(8, 8)\n┌ Warning: \n│ \n│     Adjusting reservoir matrix order:\n│         from 8 (requested) to 4\n│     based on computed bit precision = 1. \n│ \n└ @ ReservoirComputing ~/.julia/dev/ReservoirComputing/src/esn/esn_inits.jl:805\n4×4 SparseArrays.SparseMatrixCSC{Float32, Int64} with 6 stored entries:\n   ⋅        -0.600945   ⋅          ⋅ \n   ⋅          ⋅        0.132667   2.21354\n   ⋅        -2.60383    ⋅        -2.90391\n -0.578156    ⋅         ⋅          ⋅\n\n[xie2024]: Xie, Minzhi, Qianxue Wang, and Simin Yu. \"Time Series Prediction of ESN Based on Chebyshev Mapping and Strongly Connected Topology.\" Neural Processing Letters 56.1 (2024): 30.\n\n\n\n\n\n","category":"function"},{"location":"api/inits/#ReservoirComputing.low_connectivity","page":"ESN Initializers","title":"ReservoirComputing.low_connectivity","text":"low_connectivity([rng], [T], dims...;\n                 return_sparse = false, connected=false,\n                 in_degree = 1, radius = 1.0, cut_cycle = false)\n\nConstruct an internal reservoir connectivity matrix with low connectivity.\n\nThis function creates a square reservoir matrix with the specified in-degree for each node [griffith2019]. When in_degree is 1, the function can enforce a fully connected cycle if connected is true; otherwise, it generates a random connectivity pattern.\n\nArguments\n\nrng: Random number generator. Default is Utils.default_rng() from WeightInitializers.\nT: Type of the elements in the reservoir matrix. Default is Float32.\ndims: Dimensions of the reservoir matrix.\n\nKeyword Arguments\n\nreturn_sparse: If true, the function returns the reservoir matrix as a sparse matrix. Default is false.\nconnected: For in_degree == 1, if true a connected cycle is enforced. Default is false.\nin_degree: The number of incoming connections per node. Must not exceed the number of nodes. Default is 1.\nradius: The desired spectral radius of the reservoir. Defaults to 1.0.\ncut_cycle: If true, removes one edge from the cycle to cut it. Default is false.\n\n[griffith2019]: Griffith, Aaron, Andrew Pomerance, and Daniel J. Gauthier. \"Forecasting chaotic systems with very low connectivity reservoir computers.\" Chaos: An Interdisciplinary Journal of Nonlinear Science 29.12 (2019).\n\n\n\n\n\n","category":"function"},{"location":"api/inits/#ReservoirComputing.delay_line","page":"ESN Initializers","title":"ReservoirComputing.delay_line","text":"delay_line([rng], [T], dims...;\n    weight=0.1, return_sparse=false,\n    kwargs...)\n\nCreate and return a delay line reservoir matrix [rodan2010].\n\nArguments\n\nrng: Random number generator. Default is Utils.default_rng() from WeightInitializers.\nT: Type of the elements in the reservoir matrix. Default is Float32.\ndims: Dimensions of the reservoir matrix.\n\nKeyword arguments\n\nweight: Determines the value of all connections in the reservoir. This can be provided as a single value or an array. In case it is provided as an array please make sure that the lenght of the array matches the lenght of the sub-diagonal you want to populate. Default is 0.1.\nshift: delay line shift. Default is 1.\nreturn_sparse: flag for returning a sparse matrix. Default is false.\nsampling_type: Sampling that decides the distribution of weight negative numbers. If set to :no_sample the sign is unchanged. If set to :bernoulli_sample! then each weight can be positive with a probability set by positive_prob. If set to :irrational_sample! the weight is negative if the decimal number of the irrational number chosen is odd. If set to :regular_sample!, each weight will be assigned a negative sign after the chosen strides. strides can be a single number or an array. Default is :no_sample.\npositive_prob: probability of the weight being positive when sampling_type is set to :bernoulli_sample!. Default is 0.5.\nirrational: Irrational number whose decimals decide the sign of weight. Default is pi.\nstart: Which place after the decimal point the counting starts for the irrational sign counting. Default is 1.\nstrides: number of strides for assigning negative value to a weight. It can be an integer or an array. Default is 2.\n\nExamples\n\njulia> res_matrix = delay_line(5, 5)\n5×5 Matrix{Float32}:\n 0.0  0.0  0.0  0.0  0.0\n 0.1  0.0  0.0  0.0  0.0\n 0.0  0.1  0.0  0.0  0.0\n 0.0  0.0  0.1  0.0  0.0\n 0.0  0.0  0.0  0.1  0.0\n\njulia> res_matrix = delay_line(5, 5; weight = 1)\n5×5 Matrix{Float32}:\n 0.0  0.0  0.0  0.0  0.0\n 1.0  0.0  0.0  0.0  0.0\n 0.0  1.0  0.0  0.0  0.0\n 0.0  0.0  1.0  0.0  0.0\n 0.0  0.0  0.0  1.0  0.0\n\n[rodan2010]: Rodan, Ali, and Peter Tino. \"Minimum complexity echo state network.\" IEEE transactions on neural networks 22.1 (2010): 131-144.\n\n\n\n\n\n","category":"function"},{"location":"api/inits/#ReservoirComputing.delay_line_backward","page":"ESN Initializers","title":"ReservoirComputing.delay_line_backward","text":"delay_line_backward([rng], [T], dims...;\n    weight=0.1, fb_weight=0.2, return_sparse=false,\n    delay_kwargs=(), fb_kwargs=())\n\nCreate a delay line backward reservoir with the specified by dims and weights. Creates a matrix with backward connections as described in [rodan2010].\n\nArguments\n\nrng: Random number generator. Default is Utils.default_rng() from WeightInitializers.\nT: Type of the elements in the reservoir matrix. Default is Float32.\ndims: Dimensions of the reservoir matrix.\n\nKeyword arguments\n\nweight: The weight determines the absolute value of forward connections in the reservoir. This can be provided as a single value or an array. In case it is provided as an array please make sure that the lenght of the array matches the lenght of the sub-diagonal you want to populate. Default is 0.1\nfb_weight: Determines the absolute value of backward connections in the reservoir. This can be provided as a single value or an array. In case it is provided as an array please make sure that the lenght of the array matches the lenght of the sub-diagonal you want to populate. Default is 0.2.\nfb_shift: How far the backward connection will be from the diagonal. Default is 2.\nreturn_sparse: flag for returning a sparse matrix. Default is false.\ndelay_kwargs and fb_kwargs: named tuples that control the kwargs for the delay line weight and feedback weights respectively. The kwargs are as follows:\nsampling_type: Sampling that decides the distribution of weight negative numbers. If set to :no_sample the sign is unchanged. If set to :bernoulli_sample! then each weight can be positive with a probability set by positive_prob. If set to :irrational_sample! the weight is negative if the decimal number of the irrational number chosen is odd. If set to :regular_sample!, each weight will be assigned a negative sign after the chosen strides. strides can be a single number or an array. Default is :no_sample.\npositive_prob: probability of the weight being positive when sampling_type is set to :bernoulli_sample!. Default is 0.5.\nirrational: Irrational number whose decimals decide the sign of weight. Default is pi.\nstart: Which place after the decimal point the counting starts for the irrational sign counting. Default is 1.\nstrides: number of strides for assigning negative value to a weight. It can be an integer or an array. Default is 2.\n\nExamples\n\njulia> res_matrix = delay_line_backward(5, 5)\n5×5 Matrix{Float32}:\n 0.0  0.2  0.0  0.0  0.0\n 0.1  0.0  0.2  0.0  0.0\n 0.0  0.1  0.0  0.2  0.0\n 0.0  0.0  0.1  0.0  0.2\n 0.0  0.0  0.0  0.1  0.0\n\njulia> res_matrix = delay_line_backward(Float16, 5, 5)\n5×5 Matrix{Float16}:\n 0.0  0.2  0.0  0.0  0.0\n 0.1  0.0  0.2  0.0  0.0\n 0.0  0.1  0.0  0.2  0.0\n 0.0  0.0  0.1  0.0  0.2\n 0.0  0.0  0.0  0.1  0.0\n\n[rodan2010]: Rodan, Ali, and Peter Tino. \"Minimum complexity echo state network.\" IEEE transactions on neural networks 22.1 (2010): 131-144.\n\n\n\n\n\n","category":"function"},{"location":"api/inits/#ReservoirComputing.simple_cycle","page":"ESN Initializers","title":"ReservoirComputing.simple_cycle","text":"simple_cycle([rng], [T], dims...; \n    weight=0.1, return_sparse=false,\n    kwargs...)\n\nCreate a simple cycle reservoir [rodan2010].\n\nArguments\n\nrng: Random number generator. Default is Utils.default_rng() from WeightInitializers.\nT: Type of the elements in the reservoir matrix. Default is Float32.\ndims: Dimensions of the reservoir matrix.\n\nKeyword arguments\n\nweight: Weight of the connections in the reservoir matrix. This can be provided as a single value or an array. In case it is provided as an array please make sure that the lenght of the array matches the lenght of the cycle you want to populate. Default is 0.1.\nreturn_sparse: flag for returning a sparse matrix. Default is false.\nsampling_type: Sampling that decides the distribution of weight negative numbers. If set to :no_sample the sign is unchanged. If set to :bernoulli_sample! then each weight can be positive with a probability set by positive_prob. If set to :irrational_sample! the weight is negative if the decimal number of the irrational number chosen is odd. If set to :regular_sample!, each weight will be assigned a negative sign after the chosen strides. strides can be a single number or an array. Default is :no_sample.\npositive_prob: probability of the weight being positive when sampling_type is set to :bernoulli_sample!. Default is 0.5.\nirrational: Irrational number whose decimals decide the sign of weight. Default is pi.\nstart: Which place after the decimal point the counting starts for the irrational sign counting. Default is 1.\nstrides: number of strides for assigning negative value to a weight. It can be an integer or an array. Default is 2.\n\nExamples\n\njulia> res_matrix = simple_cycle(5, 5)\n5×5 Matrix{Float32}:\n 0.0  0.0  0.0  0.0  0.1\n 0.1  0.0  0.0  0.0  0.0\n 0.0  0.1  0.0  0.0  0.0\n 0.0  0.0  0.1  0.0  0.0\n 0.0  0.0  0.0  0.1  0.0\n\njulia> res_matrix = simple_cycle(5, 5; weight = 11)\n5×5 Matrix{Float32}:\n  0.0   0.0   0.0   0.0  11.0\n 11.0   0.0   0.0   0.0   0.0\n  0.0  11.0   0.0   0.0   0.0\n  0.0   0.0  11.0   0.0   0.0\n  0.0   0.0   0.0  11.0   0.0\n\n[rodan2010]: Rodan, Ali, and Peter Tino. \"Minimum complexity echo state network.\" IEEE transactions on neural networks 22.1 (2010): 131-144.\n\n\n\n\n\n","category":"function"},{"location":"api/inits/#ReservoirComputing.cycle_jumps","page":"ESN Initializers","title":"ReservoirComputing.cycle_jumps","text":"cycle_jumps([rng], [T], dims...; \n    cycle_weight=0.1, jump_weight=0.1, jump_size=3, return_sparse=false,\n    cycle_kwargs=(), jump_kwargs=())\n\nCreate a cycle jumps reservoir [Rodan2012].\n\nArguments\n\nrng: Random number generator. Default is Utils.default_rng() from WeightInitializers.\nT: Type of the elements in the reservoir matrix. Default is Float32.\ndims: Dimensions of the reservoir matrix.\n\nKeyword arguments\n\ncycle_weight:  The weight of cycle connections. This can be provided as a single value or an array. In case it is provided as an array please make sure that the lenght of the array matches the lenght of the cycle you want to populate. Default is 0.1.\njump_weight: The weight of jump connections. This can be provided as a single value or an array. In case it is provided as an array please make sure that the lenght of the array matches the lenght of the jumps you want to populate. Default is 0.1.\njump_size:  The number of steps between jump connections. Default is 3.\nreturn_sparse: flag for returning a sparse matrix. Default is false.\ncycle_kwargs and jump_kwargs: named tuples that control the kwargs for the cycle and jump weights respectively. The kwargs are as follows:\nsampling_type: Sampling that decides the distribution of weight negative numbers. If set to :no_sample the sign is unchanged. If set to :bernoulli_sample! then each weight can be positive with a probability set by positive_prob. If set to :irrational_sample! the weight is negative if the decimal number of the irrational number chosen is odd. If set to :regular_sample!, each weight will be assigned a negative sign after the chosen strides. strides can be a single number or an array. Default is :no_sample.\npositive_prob: probability of the weight being positive when sampling_type is set to :bernoulli_sample!. Default is 0.5.\nirrational: Irrational number whose decimals decide the sign of weight. Default is pi.\nstart: Which place after the decimal point the counting starts for the irrational sign counting. Default is 1.\nstrides: number of strides for assigning negative value to a weight. It can be an integer or an array. Default is 2.\n\nExamples\n\njulia> res_matrix = cycle_jumps(5, 5)\n5×5 Matrix{Float32}:\n 0.0  0.0  0.0  0.1  0.1\n 0.1  0.0  0.0  0.0  0.0\n 0.0  0.1  0.0  0.0  0.0\n 0.1  0.0  0.1  0.0  0.0\n 0.0  0.0  0.0  0.1  0.0\n\njulia> res_matrix = cycle_jumps(5, 5; jump_size = 2)\n5×5 Matrix{Float32}:\n 0.0  0.0  0.1  0.0  0.1\n 0.1  0.0  0.0  0.0  0.0\n 0.1  0.1  0.0  0.0  0.1\n 0.0  0.0  0.1  0.0  0.0\n 0.0  0.0  0.1  0.1  0.0\n\n[rodan2012]: Rodan, Ali, and Peter Tiňo. \"Simple deterministically constructed cycle reservoirs with regular jumps.\" Neural computation 24.7 (2012): 1822-1852.\n\n\n\n\n\n","category":"function"},{"location":"api/inits/#ReservoirComputing.double_cycle","page":"ESN Initializers","title":"ReservoirComputing.double_cycle","text":"double_cycle([rng], [T], dims...; \n    cycle_weight=0.1, second_cycle_weight=0.1,\n    return_sparse=false)\n\nCreates a double cycle reservoir [fu2023].\n\nArguments\n\nrng: Random number generator. Default is Utils.default_rng() from WeightInitializers.\nT: Type of the elements in the reservoir matrix. Default is Float32.\ndims: Dimensions of the reservoir matrix.\n\nKeyword arguments\n\ncycle_weight: Weight of the upper cycle connections in the reservoir matrix. Default is 0.1.\nsecond_cycle_weight: Weight of the lower cycle connections in the reservoir matrix. Default is 0.1.\nreturn_sparse: flag for returning a sparse matrix. Default is false.\n\nExamples\n\njulia> reservoir_matrix = double_cycle(5, 5; cycle_weight = 0.1, second_cycle_weight = 0.3)\n5×5 Matrix{Float32}:\n 0.0  0.3  0.0  0.0  0.3\n 0.1  0.0  0.3  0.0  0.0\n 0.0  0.1  0.0  0.3  0.0\n 0.0  0.0  0.1  0.0  0.3\n 0.1  0.0  0.0  0.1  0.0\n\n[fu2023]: Fu, Jun, et al. \"A double-cycle echo state network topology for time series prediction.\" Chaos: An Interdisciplinary Journal of Nonlinear Science 33.9 (2023).\n\n\n\n\n\n","category":"function"},{"location":"api/inits/#ReservoirComputing.true_double_cycle","page":"ESN Initializers","title":"ReservoirComputing.true_double_cycle","text":"true_double_cycle([rng], [T], dims...; \n    cycle_weight=0.1, second_cycle_weight=0.1,\n    return_sparse=false)\n\nCreates a true double cycle reservoir, ispired by [fu2023], with cycles built on the definition by [rodan2010].\n\nArguments\n\nrng: Random number generator. Default is Utils.default_rng() from WeightInitializers.\nT: Type of the elements in the reservoir matrix. Default is Float32.\ndims: Dimensions of the reservoir matrix.\n\nKeyword arguments\n\ncycle_weight: Weight of the upper cycle connections in the reservoir matrix. Default is 0.1.\nsecond_cycle_weight: Weight of the lower cycle connections in the reservoir matrix. Default is 0.1.\nreturn_sparse: flag for returning a sparse matrix. Default is false.\ncycle_kwargs, and second_cycle_kwargs: named tuples that control the kwargs for the weights generation. The kwargs are as follows:\nsampling_type: Sampling that decides the distribution of weight negative numbers. If set to :no_sample the sign is unchanged. If set to :bernoulli_sample! then each weight can be positive with a probability set by positive_prob. If set to :irrational_sample! the weight is negative if the decimal number of the irrational number chosen is odd. If set to :regular_sample!, each weight will be assigned a negative sign after the chosen strides. strides can be a single number or an array. Default is :no_sample.\npositive_prob: probability of the weight being positive when sampling_type is set to :bernoulli_sample!. Default is 0.5.\nirrational: Irrational number whose decimals decide the sign of weight. Default is pi.\nstart: Which place after the decimal point the counting starts for the irrational sign counting. Default is 1.\nstrides: number of strides for assigning negative value to a weight. It can be an integer or an array. Default is 2.\n\nExamples\n\njulia> true_double_cycle(5, 5; cycle_weight = 0.1, second_cycle_weight = 0.3)\n5×5 Matrix{Float32}:\n 0.0  0.3  0.0  0.0  0.1\n 0.1  0.0  0.3  0.0  0.0\n 0.0  0.1  0.0  0.3  0.0\n 0.0  0.0  0.1  0.0  0.3\n 0.3  0.0  0.0  0.1  0.0\n\n[fu2023]: Fu, Jun, et al. \"A double-cycle echo state network topology for time series prediction.\" Chaos: An Interdisciplinary Journal of Nonlinear Science 33.9 (2023).\n\n[rodan2010]: Rodan, Ali, and Peter Tino. \"Minimum complexity echo state network.\" IEEE transactions on neural networks 22.1 (2010): 131-144.\n\n\n\n\n\n","category":"function"},{"location":"api/inits/#ReservoirComputing.selfloop_cycle","page":"ESN Initializers","title":"ReservoirComputing.selfloop_cycle","text":"selfloop_cycle([rng], [T], dims...; \n    cycle_weight=0.1, selfloop_weight=0.1,\n    return_sparse=false, kwargs...)\n\nCreates a simple cycle reservoir with the addition of self loops [elsarraj2019].\n\nThis architecture is referred to as TP1 in the original paper.\n\nEquations\n\nW_ij =\nbegincases\n    ll  textif  i = j \n    r  textif  j = i - 1 text for  i = 2 dots N \n    r  textif  i = 1 j = N \n    0  textotherwise\nendcases\n\nArguments\n\nrng: Random number generator. Default is Utils.default_rng() from WeightInitializers.\nT: Type of the elements in the reservoir matrix. Default is Float32.\ndims: Dimensions of the reservoir matrix.\n\nKeyword arguments\n\ncycle_weight: Weight of the cycle connections in the reservoir matrix. This can be provided as a single value or an array. In case it is provided as an array please make sure that the lenght of the array matches the lenght of the cycle you want to populate. Default is 0.1.\nselfloop_weight: Weight of the self loops in the reservoir matrix. This can be provided as a single value or an array. In case it is provided as an array please make sure that the lenght of the array matches the lenght of the diagonal you want to populate. Default is 0.1.\nreturn_sparse: flag for returning a sparse matrix. Default is false.\nsampling_type: Sampling that decides the distribution of weight negative numbers. If set to :no_sample the sign is unchanged. If set to :bernoulli_sample! then each weight can be positive with a probability set by positive_prob. If set to :irrational_sample! the weight is negative if the decimal number of the irrational number chosen is odd. If set to :regular_sample!, each weight will be assigned a negative sign after the chosen strides. strides can be a single number or an array. Default is :no_sample.\npositive_prob: probability of the weight being positive when sampling_type is set to :bernoulli_sample!. Default is 0.5.\nirrational: Irrational number whose decimals decide the sign of weight. Default is pi.\nstart: Which place after the decimal point the counting starts for the irrational sign counting. Default is 1.\nstrides: number of strides for assigning negative value to a weight. It can be an integer or an array. Default is 2.\n\nExamples\n\njulia> reservoir_matrix = selfloop_cycle(5, 5)\n5×5 Matrix{Float32}:\n 0.1  0.0  0.0  0.0  0.1\n 0.1  0.1  0.0  0.0  0.0\n 0.0  0.1  0.1  0.0  0.0\n 0.0  0.0  0.1  0.1  0.0\n 0.0  0.0  0.0  0.1  0.1\n\njulia> reservoir_matrix = selfloop_cycle(5, 5; weight=0.2, selfloop_weight=0.5)\n5×5 Matrix{Float32}:\n 0.5  0.0  0.0  0.0  0.2\n 0.2  0.5  0.0  0.0  0.0\n 0.0  0.2  0.5  0.0  0.0\n 0.0  0.0  0.2  0.5  0.0\n 0.0  0.0  0.0  0.2  0.5\n\n[elsarraj2019]: Elsarraj, Duaa, et al. \"Demystifying echo state network with deterministic simple topologies.\" International Journal of Computational Science and Engineering 19.3 (2019): 407-417.\n\n\n\n\n\n","category":"function"},{"location":"api/inits/#ReservoirComputing.selfloop_feedback_cycle","page":"ESN Initializers","title":"ReservoirComputing.selfloop_feedback_cycle","text":"selfloop_feedback_cycle([rng], [T], dims...; \n    cycle_weight=0.1, selfloop_weight=0.1,\n    return_sparse=false)\n\nCreates a cycle reservoir with feedback connections on even neurons and self loops on odd neurons [elsarraj2019].\n\nThis architecture is referred to as TP2 in the original paper.\n\nEquations\n\nW_ij =\nbegincases\n    r  textif  j = i - 1 text for  i = 2 dots N \n    r  textif  i = 1 j = N \n    ll  textif  i = j text and  i text is odd \n    r  textif  j = i + 1 text and  i text is even i neq N \n    0  textotherwise\nendcases\n\nArguments\n\nrng: Random number generator. Default is Utils.default_rng() from WeightInitializers.\nT: Type of the elements in the reservoir matrix. Default is Float32.\ndims: Dimensions of the reservoir matrix.\n\nKeyword arguments\n\ncycle_weight: Weight of the cycle connections in the reservoir matrix. This can be provided as a single value or an array. In case it is provided as an array please make sure that the lenght of the array matches the lenght of the cycle you want to populate. Default is 0.1.\nselfloop_weight: Weight of the self loops in the reservoir matrix. Default is 0.1.\nreturn_sparse: flag for returning a sparse matrix. Default is false.\n\nExamples\n\njulia> reservoir_matrix = selfloop_feedback_cycle(5, 5)\n5×5 Matrix{Float32}:\n 0.1  0.1  0.0  0.0  0.1\n 0.1  0.0  0.0  0.0  0.0\n 0.0  0.1  0.1  0.1  0.0\n 0.0  0.0  0.1  0.0  0.0\n 0.0  0.0  0.0  0.1  0.1\n\njulia> reservoir_matrix = selfloop_feedback_cycle(5, 5; self_loop_weight=0.5)\n5×5 Matrix{Float32}:\n 0.5  0.1  0.0  0.0  0.1\n 0.1  0.0  0.0  0.0  0.0\n 0.0  0.1  0.5  0.1  0.0\n 0.0  0.0  0.1  0.0  0.0\n 0.0  0.0  0.0  0.1  0.5\n\n[elsarraj2019]: Elsarraj, Duaa, et al. \"Demystifying echo state network with deterministic simple topologies.\" International Journal of Computational Science and Engineering 19.3 (2019): 407-417.\n\n\n\n\n\n","category":"function"},{"location":"api/inits/#ReservoirComputing.selfloop_delayline_backward","page":"ESN Initializers","title":"ReservoirComputing.selfloop_delayline_backward","text":"selfloop_delayline_backward([rng], [T], dims...; \n    weight=0.1, selfloop_weight=0.1, fb_weight=0.1,\n    fb_shift=2, return_sparse=false, fb_kwargs=(),\n    selfloop_kwargs=(), delay_kwargs=())\n\nCreates a reservoir based on a delay line with the addition of self loops and backward connections shifted by one [elsarraj2019].\n\nThis architecture is referred to as TP3 in the original paper.\n\nEquations\n\nW_ij =\nbegincases\n    ll  textif  i = j text for  i = 1 dots N \n    r  textif  j = i - 1 text for  i = 2 dots N \n    r  textif  j = i - 2 text for  i = 3 dots N \n    0  textotherwise\nendcases\n\nArguments\n\nrng: Random number generator. Default is Utils.default_rng() from WeightInitializers.\nT: Type of the elements in the reservoir matrix. Default is Float32.\ndims: Dimensions of the reservoir matrix.\n\nKeyword arguments\n\nweight: Weight of the cycle connections in the reservoir matrix. This can be provided as a single value or an array. In case it is provided as an array please make sure that the lenght of the array matches the lenght of the cycle you want to populate. Default is 0.1.\nselfloop_weight: Weight of the self loops in the reservoir matrix. This can be provided as a single value or an array. In case it is provided as an array please make sure that the lenght of the array matches the lenght of the diagonal you want to populate. Default is 0.1.\nfb_weight: Weight of the feedback in the reservoir matrix. This can be provided as a single value or an array. In case it is provided as an array please make sure that the lenght of the array matches the lenght of the diagonal you want to populate. Default is 0.1.\nfb_shift: How far the backward connection will be from the diagonal. Default is 2.\nreturn_sparse: flag for returning a sparse matrix. Default is false.\ndelay_kwargs, selfloop_kwargs, and fb_kwargs: named tuples that control the kwargs for the weights generation. The kwargs are as follows:\nsampling_type: Sampling that decides the distribution of weight negative numbers.   If set to :no_sample the sign is unchanged. If set to :bernoulli_sample! then each   weight can be positive with a probability set by positive_prob. If set to   :irrational_sample! the weight is negative if the decimal number of the   irrational number chosen is odd. If set to :regular_sample!, each weight will be   assigned a negative sign after the chosen strides. strides can be a single   number or an array. Default is :no_sample.\npositive_prob: probability of the weight being positive when sampling_type is set to :bernoulli_sample!. Default is 0.5.\nirrational: Irrational number whose decimals decide the sign of weight. Default is pi.\nstart: Which place after the decimal point the counting starts for the irrational sign counting. Default is 1.\nstrides: number of strides for assigning negative value to a weight. It can be an integer or an array. Default is 2.\n\nExamples\n\njulia> reservoir_matrix = selfloop_delayline_backward(5, 5)\n5×5 Matrix{Float32}:\n 0.1  0.0  0.1  0.0  0.0\n 0.1  0.1  0.0  0.1  0.0\n 0.0  0.1  0.1  0.0  0.1\n 0.0  0.0  0.1  0.1  0.0\n 0.0  0.0  0.0  0.1  0.1\n\njulia> reservoir_matrix = selfloop_delayline_backward(5, 5; weight=0.3)\n5×5 Matrix{Float32}:\n 0.1  0.0  0.3  0.0  0.0\n 0.3  0.1  0.0  0.3  0.0\n 0.0  0.3  0.1  0.0  0.3\n 0.0  0.0  0.3  0.1  0.0\n 0.0  0.0  0.0  0.3  0.1\n\n[elsarraj2019]: Elsarraj, Duaa, et al. \"Demystifying echo state network with deterministic simple topologies.\" International Journal of Computational Science and Engineering 19.3 (2019): 407-417.\n\n\n\n\n\n","category":"function"},{"location":"api/inits/#ReservoirComputing.selfloop_forward_connection","page":"ESN Initializers","title":"ReservoirComputing.selfloop_forward_connection","text":"selfloop_forward_connection([rng], [T], dims...; \n    weight=0.1, selfloop_weight=0.1,\n    return_sparse=false, selfloop_kwargs=(),\n    delay_kwargs=())\n\nCreates a reservoir based on a forward connection of weights between even nodes with the addition of self loops [elsarraj2019].\n\nThis architecture is referred to as TP4 in the original paper.\n\nEquations\n\nW_ij =\nbegincases\n    ll  textif  i = j text for  i = 1 dots N \n    r  textif  j = i - 2 text for  i = 3 dots N \n    0  textotherwise\nendcases\n\nArguments\n\nrng: Random number generator. Default is Utils.default_rng() from WeightInitializers.\nT: Type of the elements in the reservoir matrix. Default is Float32.\ndims: Dimensions of the reservoir matrix.\n\nKeyword arguments\n\nweight: Weight of the cycle connections in the reservoir matrix. This can be provided as a single value or an array. In case it is provided as an array please make sure that the lenght of the array matches the lenght of the cycle you want to populate. Default is 0.1.\nselfloop_weight: Weight of the self loops in the reservoir matrix. This can be provided as a single value or an array. In case it is provided as an array please make sure that the lenght of the array matches the lenght of the diagonal you want to populate. Default is 0.1.\nreturn_sparse: flag for returning a sparse matrix. Default is false.\ndelay_kwargs and selfloop_kwargs: named tuples that control the kwargs for the  delay line weight and self loop weights respectively. The kwargs are as follows:\nsampling_type: Sampling that decides the distribution of weight negative numbers.   If set to :no_sample the sign is unchanged. If set to :bernoulli_sample! then each   weight can be positive with a probability set by positive_prob. If set to   :irrational_sample! the weight is negative if the decimal number of the   irrational number chosen is odd. If set to :regular_sample!, each weight will be   assigned a negative sign after the chosen strides. strides can be a single   number or an array. Default is :no_sample.\npositive_prob: probability of the weight being positive when sampling_type is set to :bernoulli_sample!. Default is 0.5.\nirrational: Irrational number whose decimals decide the sign of weight. Default is pi.\nstart: Which place after the decimal point the counting starts for the irrational sign counting. Default is 1.\nstrides: number of strides for assigning negative value to a weight. It can be an integer or an array. Default is 2.\n\nExamples\n\njulia> reservoir_matrix = selfloop_forward_connection(5, 5)\n5×5 Matrix{Float32}:\n 0.1  0.0  0.0  0.0  0.0\n 0.0  0.1  0.0  0.0  0.0\n 0.1  0.0  0.1  0.0  0.0\n 0.0  0.1  0.0  0.1  0.0\n 0.0  0.0  0.1  0.0  0.1\n\njulia> reservoir_matrix = selfloop_forward_connection(5, 5; weight=0.5)\n5×5 Matrix{Float32}:\n 0.1  0.0  0.0  0.0  0.0\n 0.0  0.1  0.0  0.0  0.0\n 0.5  0.0  0.1  0.0  0.0\n 0.0  0.5  0.0  0.1  0.0\n 0.0  0.0  0.5  0.0  0.1\n\n[elsarraj2019]: Elsarraj, Duaa, et al. \"Demystifying echo state network with deterministic simple topologies.\" International Journal of Computational Science and Engineering 19.3 (2019): 407-417.\n\n\n\n\n\n","category":"function"},{"location":"api/inits/#ReservoirComputing.forward_connection","page":"ESN Initializers","title":"ReservoirComputing.forward_connection","text":"forward_connection([rng], [T], dims...; \n    weight=0.1, selfloop_weight=0.1,\n    return_sparse=false)\n\nCreates a reservoir based on a forward connection of weights [elsarraj2019].\n\nThis architecture is referred to as TP5 in the original paper.\n\nEquations\n\nW_ij =\nbegincases\n    r  textif  j = i - 2 text for  i = 3 dots N \n    0  textotherwise\nendcases\n\nArguments\n\nrng: Random number generator. Default is Utils.default_rng() from WeightInitializers.\nT: Type of the elements in the reservoir matrix. Default is Float32.\ndims: Dimensions of the reservoir matrix.\n\nKeyword arguments\n\nweight: Weight of the cycle connections in the reservoir matrix. This can be provided as a single value or an array. In case it is provided as an array please make sure that the lenght of the array matches the lenght of the sub-diagonal you want to populate. Default is 0.1.\nreturn_sparse: flag for returning a sparse matrix. Default is false.\nsampling_type: Sampling that decides the distribution of weight negative numbers. If set to :no_sample the sign is unchanged. If set to :bernoulli_sample! then each weight can be positive with a probability set by positive_prob. If set to :irrational_sample! the weight is negative if the decimal number of the irrational number chosen is odd. If set to :regular_sample!, each weight will be assigned a negative sign after the chosen strides. strides can be a single number or an array. Default is :no_sample.\npositive_prob: probability of the weight being positive when sampling_type is set to :bernoulli_sample!. Default is 0.5.\nirrational: Irrational number whose decimals decide the sign of weight. Default is pi.\nstart: Which place after the decimal point the counting starts for the irrational sign counting. Default is 1.\nstrides: number of strides for assigning negative value to a weight. It can be an integer or an array. Default is 2.\n\nExamples\n\njulia> reservoir_matrix = forward_connection(5, 5)\n5×5 Matrix{Float32}:\n 0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n 0.1  0.0  0.0  0.0  0.0\n 0.0  0.1  0.0  0.0  0.0\n 0.0  0.0  0.1  0.0  0.0\n\njulia> reservoir_matrix = forward_connection(5, 5; weight=0.5)\n5×5 Matrix{Float32}:\n 0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n 0.5  0.0  0.0  0.0  0.0\n 0.0  0.5  0.0  0.0  0.0\n 0.0  0.0  0.5  0.0  0.0\n\n[elsarraj2019]: Elsarraj, Duaa, et al. \"Demystifying echo state network with deterministic simple topologies.\" International Journal of Computational Science and Engineering 19.3 (2019): 407-417.\n\n\n\n\n\n","category":"function"},{"location":"api/inits/#Building-functions","page":"ESN Initializers","title":"Building functions","text":"","category":"section"},{"location":"api/inits/#ReservoirComputing.scale_radius!","page":"ESN Initializers","title":"ReservoirComputing.scale_radius!","text":"scale_radius!(matrix, radius)\n\nScale the spectral radius of the given matrix to be equal to the given radius\n\nArguments\n\nmatrix: Matrix to be scaled.\nradius: desidered radius to scale the given matrix to\n\n\n\n\n\n","category":"function"},{"location":"api/inits/#ReservoirComputing.delay_line!","page":"ESN Initializers","title":"ReservoirComputing.delay_line!","text":"delay_line!([rng], reservoir_matrix, weight, shift;\n    sampling_type=:no_sample, irrational=pi, start=1,\n    p=0.5)\n\nAdds a delay line in the reservoir_matrix, with given shift and weight. The weight can be a single number or an array.\n\nArguments\n\nrng: Random number generator. Default is Utils.default_rng() from WeightInitializers.\nreservoir_matrix: matrix to be changed.\nweight: weight to add as a delay line. Can be either a single number or an array.\nshift: How far the delay line will be from the diagonal.\n\nKeyword arguments\n\nsampling_type: Sampling that decides the distribution of weight negative numbers. If set to :no_sample the sign is unchanged. If set to :bernoulli_sample! then each weight can be positive with a probability set by positive_prob. If set to :irrational_sample! the weight is negative if the decimal number of the irrational number chosen is odd. If set to :regular_sample!, each weight will be assigned a negative sign after the chosen strides. strides can be a single number or an array. Default is :no_sample.\npositive_prob: probability of the weight being positive when sampling_type is set to :bernoulli_sample!. Default is 0.5.\nirrational: Irrational number whose decimals decide the sign of weight. Default is pi.\nstart: Which place after the decimal point the counting starts for the irrational sign counting. Default is 1.\nstrides: number of strides for assigning negative value to a weight. It can be an integer or an array. Default is 2.\n\nExamples\n\njulia> matrix = zeros(Float32, 5, 5)\n5×5 Matrix{Float32}:\n 0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n\njulia> delay_line!(matrix, 5.0, 2)\n5×5 Matrix{Float32}:\n 0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n 5.0  0.0  0.0  0.0  0.0\n 0.0  5.0  0.0  0.0  0.0\n 0.0  0.0  5.0  0.0  0.0\n\n julia> delay_line!(matrix, 5.0, 2; sampling_type=:bernoulli_sample!)\n5×5 Matrix{Float32}:\n 0.0   0.0  0.0  0.0  0.0\n 0.0   0.0  0.0  0.0  0.0\n 5.0   0.0  0.0  0.0  0.0\n 0.0  -5.0  0.0  0.0  0.0\n 0.0   0.0  5.0  0.0  0.0\n\n\n\n\n\n","category":"function"},{"location":"api/inits/#ReservoirComputing.backward_connection!","page":"ESN Initializers","title":"ReservoirComputing.backward_connection!","text":"backward_connection!([rng], reservoir_matrix, weight, shift;\n    sampling_type=:no_sample, irrational=pi, start=1,\n    p=0.5)\n\nAdds a backward connection in the reservoir_matrix, with given shift and weight. The weight can be a single number or an array.\n\nArguments\n\nrng: Random number generator. Default is Utils.default_rng() from WeightInitializers.\nreservoir_matrix: matrix to be changed.\nweight: weight to add as a backward connection. Can be either a single number or an array.\nshift: How far the backward connection will be from the diagonal.\n\nKeyword arguments\n\nsampling_type: Sampling that decides the distribution of weight negative numbers. If set to :no_sample the sign is unchanged. If set to :bernoulli_sample! then each weight can be positive with a probability set by positive_prob. If set to :irrational_sample! the weight is negative if the decimal number of the irrational number chosen is odd. If set to :regular_sample!, each weight will be assigned a negative sign after the chosen strides. strides can be a single number or an array. Default is :no_sample.\npositive_prob: probability of the weight being positive when sampling_type is set to :bernoulli_sample!. Default is 0.5.\nirrational: Irrational number whose decimals decide the sign of weight. Default is pi.\nstart: Which place after the decimal point the counting starts for the irrational sign counting. Default is 1.\nstrides: number of strides for assigning negative value to a weight. It can be an integer or an array. Default is 2.\n\nExamples\n\njulia> matrix = zeros(Float32, 5, 5)\n5×5 Matrix{Float32}:\n 0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n\njulia> backward_connection!(matrix, 3.0, 1)\n5×5 Matrix{Float32}:\n 0.0  3.0  0.0  0.0  0.0\n 0.0  0.0  3.0  0.0  0.0\n 0.0  0.0  0.0  3.0  0.0\n 0.0  0.0  0.0  0.0  3.0\n 0.0  0.0  0.0  0.0  0.0\n\njulia> backward_connection!(matrix, 3.0, 1; sampling_type = :bernoulli_sample!)\n5×5 Matrix{Float32}:\n 0.0  3.0   0.0  0.0   0.0\n 0.0  0.0  -3.0  0.0   0.0\n 0.0  0.0   0.0  3.0   0.0\n 0.0  0.0   0.0  0.0  -3.0\n 0.0  0.0   0.0  0.0   0.0\n\n\n\n\n\n","category":"function"},{"location":"api/inits/#ReservoirComputing.simple_cycle!","page":"ESN Initializers","title":"ReservoirComputing.simple_cycle!","text":"simple_cycle!([rng], reservoir_matrix, weight;\n    sampling_type=:no_sample, irrational=pi, start=1,\n    p=0.5)\n\nAdds a simple cycle in the reservoir_matrix, with given weight. The weight can be a single number or an array.\n\nArguments\n\nrng: Random number generator. Default is Utils.default_rng() from WeightInitializers.\nreservoir_matrix: matrix to be changed.\nweight: weight to add as a simple cycle. Can be either a single number or an array.\n\nKeyword arguments\n\nsampling_type: Sampling that decides the distribution of weight negative numbers. If set to :no_sample the sign is unchanged. If set to :bernoulli_sample! then each weight can be positive with a probability set by positive_prob. If set to :irrational_sample! the weight is negative if the decimal number of the irrational number chosen is odd. If set to :regular_sample!, each weight will be assigned a negative sign after the chosen strides. strides can be a single number or an array. Default is :no_sample.\npositive_prob: probability of the weight being positive when sampling_type is set to :bernoulli_sample!. Default is 0.5.\nirrational: Irrational number whose decimals decide the sign of weight. Default is pi.\nstart: Which place after the decimal point the counting starts for the irrational sign counting. Default is 1.\nstrides: number of strides for assigning negative value to a weight. It can be an integer or an array. Default is 2.\n\nExamples\n\njulia> matrix = zeros(Float32, 5, 5)\n5×5 Matrix{Float32}:\n 0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n\njulia> simple_cycle!(matrix, 1.0; sampling_type = :irrational_sample!)\n5×5 Matrix{Float32}:\n  0.0  0.0   0.0   0.0  -1.0\n -1.0  0.0   0.0   0.0   0.0\n  0.0  1.0   0.0   0.0   0.0\n  0.0  0.0  -1.0   0.0   0.0\n  0.0  0.0   0.0  -1.0   0.0\n\n\n\n\n\n","category":"function"},{"location":"api/inits/#ReservoirComputing.reverse_simple_cycle!","page":"ESN Initializers","title":"ReservoirComputing.reverse_simple_cycle!","text":"reverse_simple_cycle!([rng], reservoir_matrix, weight;\n    sampling_type=:no_sample, irrational=pi, start=1,\n    p=0.5)\n\nAdds a reverse simple cycle in the reservoir_matrix, with given weight. The weight can be a single number or an array.\n\nArguments\n\nrng: Random number generator. Default is Utils.default_rng() from WeightInitializers.\nreservoir_matrix: matrix to be changed.\nweight: weight to add as a simple cycle. Can be either a single number or an array.\n\nKeyword arguments\n\nsampling_type: Sampling that decides the distribution of weight negative numbers. If set to :no_sample the sign is unchanged. If set to :bernoulli_sample! then each weight can be positive with a probability set by positive_prob. If set to :irrational_sample! the weight is negative if the decimal number of the irrational number chosen is odd. If set to :regular_sample!, each weight will be assigned a negative sign after the chosen strides. strides can be a single number or an array. Default is :no_sample.\npositive_prob: probability of the weight being positive when sampling_type is set to :bernoulli_sample!. Default is 0.5.\nirrational: Irrational number whose decimals decide the sign of weight. Default is pi.\nstart: Which place after the decimal point the counting starts for the irrational sign counting. Default is 1.\nstrides: number of strides for assigning negative value to a weight. It can be an integer or an array. Default is 2.\n\nExamples\n\njulia> matrix = zeros(Float32, 5, 5)\n5×5 Matrix{Float32}:\n 0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n\njulia> reverse_simple_cycle!(matrix, 1.0; sampling_type = :regular_sample!)\n5×5 Matrix{Float32}:\n 0.0  -1.0  0.0   0.0  0.0\n 0.0   0.0  1.0   0.0  0.0\n 0.0   0.0  0.0  -1.0  0.0\n 0.0   0.0  0.0   0.0  1.0\n 1.0   0.0  0.0   0.0  0.0\n\n\n\n\n\n","category":"function"},{"location":"api/inits/#ReservoirComputing.self_loop!","page":"ESN Initializers","title":"ReservoirComputing.self_loop!","text":"self_loop!([rng], reservoir_matrix, weight, jump_size;\n    sampling_type=:no_sample, irrational=pi, start=1,\n    positive_prob=0.5)\n\nAdds jumps to a given reservoir_matrix with chosen weight and determined jump_size. weight can be either a number or an array.\n\nArguments\n\nrng: Random number generator. Default is Utils.default_rng() from WeightInitializers.\nreservoir_matrix: matrix to be changed.\nweight: weight to add as a self loop. Can be either a single number or an array.\n\nKeyword arguments\n\nsampling_type: Sampling that decides the distribution of weight negative numbers. If set to :no_sample the sign is unchanged. If set to :bernoulli_sample! then each weight can be positive with a probability set by positive_prob. If set to :irrational_sample! the weight is negative if the decimal number of the irrational number chosen is odd. If set to :regular_sample!, each weight will be assigned a negative sign after the chosen strides. strides can be a single number or an array. Default is :no_sample.\npositive_prob: probability of the weight being positive when sampling_type is set to :bernoulli_sample!. Default is 0.5.\nirrational: Irrational number whose decimals decide the sign of weight. Default is pi.\nstart: Which place after the decimal point the counting starts for the irrational sign counting. Default is 1.\nstrides: number of strides for assigning negative value to a weight. It can be an integer or an array. Default is 2.\n\nExamples\n\njulia> matrix = zeros(Float32, 5, 5)\n5×5 Matrix{Float32}:\n 0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n\njulia> self_loop!(matrix, 1.0)\n5×5 Matrix{Float32}:\n  1.0  0.0   0.0   0.0   0.0\n  0.0  1.0   0.0   0.0   0.0\n  0.0  0.0   1.0   0.0   0.0\n  0.0  0.0   0.0   1.0   0.0\n  0.0  0.0   0.0   0.0   1.0\n\n\n\n\n\n","category":"function"},{"location":"api/inits/#ReservoirComputing.add_jumps!","page":"ESN Initializers","title":"ReservoirComputing.add_jumps!","text":"add_jumps!([rng], reservoir_matrix, weight, jump_size;\n    sampling_type=:no_sample, irrational=pi, start=1,\n    positive_prob=0.5)\n\nAdds jumps to a given reservoir_matrix with chosen weight and determined jump_size. weight can be either a number or an array.\n\nArguments\n\nrng: Random number generator. Default is Utils.default_rng() from WeightInitializers.\nreservoir_matrix: matrix to be changed.\nweight: weight to add as a simple cycle. Can be either a single number or an array.\njump_size: size of the jump's distance.\n\nKeyword arguments\n\nsampling_type: Sampling that decides the distribution of weight negative numbers. If set to :no_sample the sign is unchanged. If set to :bernoulli_sample! then each weight can be positive with a probability set by positive_prob. If set to :irrational_sample! the weight is negative if the decimal number of the irrational number chosen is odd. If set to :regular_sample!, each weight will be assigned a negative sign after the chosen strides. strides can be a single number or an array. Default is :no_sample.\npositive_prob: probability of the weight being positive when sampling_type is set to :bernoulli_sample!. Default is 0.5.\nirrational: Irrational number whose decimals decide the sign of weight. Default is pi.\nstart: Which place after the decimal point the counting starts for the irrational sign counting. Default is 1.\nstrides: number of strides for assigning negative value to a weight. It can be an integer or an array. Default is 2.\n\nExamples\n\njulia> matrix = zeros(Float32, 5, 5)\n5×5 Matrix{Float32}:\n 0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n\njulia> add_jumps!(matrix, 1.0)\n5×5 Matrix{Float32}:\n  0.0  0.0   1.0   0.0   0.0\n  0.0  0.0   0.0   0.0   0.0\n  1.0  0.0   0.0   0.0   0.0\n  0.0  0.0   0.0   0.0   1.0\n  0.0  0.0   1.0   0.0   0.0\n\n\n\n\n\n","category":"function"},{"location":"api/esn/#Echo-State-Networks","page":"Echo State Networks","title":"Echo State Networks","text":"","category":"section"},{"location":"api/esn/","page":"Echo State Networks","title":"Echo State Networks","text":"The core component of an ESN is the ESN type. It represents the entire Echo State Network and includes parameters for configuring the reservoir, input scaling, and output weights. Here's the documentation for the ESN type:","category":"page"},{"location":"api/esn/#ReservoirComputing.ESN","page":"Echo State Networks","title":"ReservoirComputing.ESN","text":"ESN(train_data; kwargs...) -> ESN\n\nCreates an Echo State Network (ESN).\n\nArguments\n\ntrain_data: Matrix of training data num_features x time_steps.\nvariation: Variation of ESN (default: Default()).\ninput_layer: Input layer of ESN.\nreservoir: Reservoir of the ESN.\nbias: Bias vector for each time step.\nrng: Random number generator used for initializing weights. Default is Utils.default_rng().\nreservoir_driver: Mechanism for evolving reservoir states (default: RNN()).\nnla_type: Non-linear activation type (default: NLADefault()).\nstates_type: Format for storing states (default: StandardStates()).\nwashout: Initial time steps to discard (default: 0).\nmatrix_type: Type of matrices used internally (default: type of train_data).\n\nExamples\n\njulia> train_data = rand(Float32, 10, 100)  # 10 features, 100 time steps\n10×100 Matrix{Float32}:\n 0.567676   0.154756  0.584611  0.294015   …  0.573946    0.894333    0.429133\n 0.327073   0.729521  0.804667  0.263944      0.559342    0.020167    0.897862\n 0.453606   0.800058  0.568311  0.749441      0.0713146   0.464795    0.532854\n 0.0173253  0.536959  0.722116  0.910328      0.00224048  0.00202501  0.631075\n 0.366744   0.119761  0.100593  0.125122      0.700562    0.675474    0.102947\n 0.539737   0.768351  0.54681   0.648672   …  0.256738    0.223784    0.94327\n 0.558099   0.42676   0.1948    0.735625      0.0989234   0.119342    0.624182\n 0.0603135  0.929999  0.263439  0.0372732     0.066125    0.332769    0.25562\n 0.4463     0.334423  0.444679  0.311695      0.0494497   0.27171     0.214925\n 0.987182   0.898593  0.295241  0.233098      0.789699    0.453692    0.759205\n\njulia> esn = ESN(train_data, 10, 300; washout = 10)\nESN(10 => 300)\n\n\n\n\n\n","category":"type"},{"location":"api/esn/#Training","page":"Echo State Networks","title":"Training","text":"","category":"section"},{"location":"api/esn/","page":"Echo State Networks","title":"Echo State Networks","text":"To train an ESN model, you can use the train function. It takes the ESN model, training data, and other optional parameters as input and returns a trained model. Here's the documentation for the train function:","category":"page"},{"location":"api/esn/#ReservoirComputing.train","page":"Echo State Networks","title":"ReservoirComputing.train","text":"train(esn::AbstractEchoStateNetwork, target_data, training_method = StandardRidge(0.0))\n\nTrains an Echo State Network (ESN) using the provided target data and a specified training method.\n\nParameters\n\nesn::AbstractEchoStateNetwork: The ESN instance to be trained.\ntarget_data: Supervised training data for the ESN.\ntraining_method: The method for training the ESN (default: StandardRidge(0.0)).\n\nExample\n\njulia> train_data = rand(Float32, 10, 100)  # 10 features, 100 time steps\n10×100 Matrix{Float32}:\n 0.11437   0.425367  0.585867   0.34078   …  0.0531493  0.761425  0.883164\n 0.301373  0.497806  0.279603   0.802417     0.49873    0.270156  0.333333\n 0.135224  0.660179  0.394233   0.512753     0.901221   0.784377  0.687691\n 0.510203  0.877234  0.614245   0.978405     0.332775   0.768826  0.527077\n 0.955027  0.398322  0.312156   0.981938     0.473357   0.156704  0.476101\n 0.353024  0.997632  0.164328   0.470783  …  0.745613   0.85797   0.465201\n 0.966044  0.194299  0.599167   0.040475     0.0996013  0.325959  0.770103\n 0.292068  0.495138  0.481299   0.214566     0.819573   0.155951  0.227168\n 0.133498  0.451058  0.0761995  0.90421      0.994212   0.332164  0.545112\n 0.214467  0.791524  0.124105   0.951805     0.947166   0.954244  0.889733\n\njulia> esn = ESN(train_data, 10, 300; washout = 10)\nESN(10 => 300)\n\njulia> output_layer = train(esn, rand(Float32, 3, 90))\nOutputLayer successfully trained with output size: 3\n\n\n\n\n\n","category":"function"},{"location":"api/esn/","page":"Echo State Networks","title":"Echo State Networks","text":"With these components and variations, you can configure and train ESN models for various time series and sequential data prediction tasks.","category":"page"}]
}
