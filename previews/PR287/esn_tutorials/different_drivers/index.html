<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Using Different Reservoir Drivers · ReservoirComputing.jl</title><meta name="title" content="Using Different Reservoir Drivers · ReservoirComputing.jl"/><meta property="og:title" content="Using Different Reservoir Drivers · ReservoirComputing.jl"/><meta property="twitter:title" content="Using Different Reservoir Drivers · ReservoirComputing.jl"/><meta name="description" content="Documentation for ReservoirComputing.jl."/><meta property="og:description" content="Documentation for ReservoirComputing.jl."/><meta property="twitter:description" content="Documentation for ReservoirComputing.jl."/><meta property="og:url" content="https://docs.sciml.ai/ReservoirComputing/stable/esn_tutorials/different_drivers/"/><meta property="twitter:url" content="https://docs.sciml.ai/ReservoirComputing/stable/esn_tutorials/different_drivers/"/><link rel="canonical" href="https://docs.sciml.ai/ReservoirComputing/stable/esn_tutorials/different_drivers/"/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/favicon.ico" rel="icon" type="image/x-icon"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.png" alt="ReservoirComputing.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">ReservoirComputing.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">ReservoirComputing.jl</a></li><li><span class="tocitem">General Settings</span><ul><li><a class="tocitem" href="../../general/different_training/">Changing Training Algorithms</a></li><li><a class="tocitem" href="../../general/states_variation/">Altering States</a></li><li><a class="tocitem" href="../../general/predictive_generative/">Generative vs Predictive</a></li></ul></li><li><span class="tocitem">Echo State Network Tutorials</span><ul><li><a class="tocitem" href="../lorenz_basic/">Lorenz System Forecasting</a></li><li><a class="tocitem" href="../change_layers/">Using Different Layers</a></li><li class="is-active"><a class="tocitem" href>Using Different Reservoir Drivers</a><ul class="internal"><li><a class="tocitem" href="#Multiple-Activation-Function-RNN"><span>Multiple Activation Function RNN</span></a></li><li><a class="tocitem" href="#Gated-Recurrent-Unit"><span>Gated Recurrent Unit</span></a></li><li><a class="tocitem" href="#References"><span>References</span></a></li></ul></li><li><a class="tocitem" href="../deep_esn/">Deep Echo State Networks</a></li><li><a class="tocitem" href="../hybrid/">Hybrid Echo State Networks</a></li></ul></li><li><a class="tocitem" href="../../reca_tutorials/reca/">Reservoir Computing with Cellular Automata</a></li><li><span class="tocitem">API Documentation</span><ul><li><a class="tocitem" href="../../api/training/">Training Algorithms</a></li><li><a class="tocitem" href="../../api/states/">States Modifications</a></li><li><a class="tocitem" href="../../api/predict/">Prediction Types</a></li><li><a class="tocitem" href="../../api/esn/">Echo State Networks</a></li><li><a class="tocitem" href="../../api/inits/">ESN Initializers</a></li><li><a class="tocitem" href="../../api/esn_drivers/">ESN Drivers</a></li><li><a class="tocitem" href="../../api/esn_variations/">ESN Variations</a></li><li><a class="tocitem" href="../../api/reca/">ReCA</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Echo State Network Tutorials</a></li><li class="is-active"><a href>Using Different Reservoir Drivers</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Using Different Reservoir Drivers</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/SciML/ReservoirComputing.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/SciML/ReservoirComputing.jl/blob/master/docs/src/esn_tutorials/different_drivers.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Using-Different-Reservoir-Drivers"><a class="docs-heading-anchor" href="#Using-Different-Reservoir-Drivers">Using Different Reservoir Drivers</a><a id="Using-Different-Reservoir-Drivers-1"></a><a class="docs-heading-anchor-permalink" href="#Using-Different-Reservoir-Drivers" title="Permalink"></a></h1><p>While the original implementation of the Echo State Network implemented the model using the equations of Recurrent Neural Networks to obtain non-linearity in the reservoir, other variations have been proposed in recent years. More specifically, the different drivers implemented in ReservoirComputing.jl are the multiple activation function RNN <code>MRNN()</code> and the Gated Recurrent Unit <code>GRU()</code>. To change them, it suffices to give the chosen method to the <code>ESN</code> keyword argument <code>reservoir_driver</code>. In this section, some examples, of their usage will be given, as well as a brief introduction to their equations.</p><h2 id="Multiple-Activation-Function-RNN"><a class="docs-heading-anchor" href="#Multiple-Activation-Function-RNN">Multiple Activation Function RNN</a><a id="Multiple-Activation-Function-RNN-1"></a><a class="docs-heading-anchor-permalink" href="#Multiple-Activation-Function-RNN" title="Permalink"></a></h2><p>Based on the double activation function ESN (DAFESN) proposed in (<a href="../../references/#Lun2015">Lun <em>et al.</em>, 2015</a>), the Multiple Activation Function ESN expands the idea and allows a custom number of activation functions to be used in the reservoir dynamics. This can be thought of as a linear combination of multiple activation functions with corresponding parameters.</p><p class="math-container">\[\mathbf{x}(t+1) = (1-\alpha)\mathbf{x}(t) + \lambda_1 f_1(\mathbf{W}\mathbf{x}(t)+\mathbf{W}_{in}\mathbf{u}(t)) + \dots + \lambda_D f_D(\mathbf{W}\mathbf{x}(t)+\mathbf{W}_{in}\mathbf{u}(t))\]</p><p>where <span>$D$</span> is the number of activation functions and respective parameters chosen.</p><p>The method to call to use the multiple activation function ESN is <code>MRNN(activation_function, leaky_coefficient, scaling_factor)</code>. The arguments can be used as both <code>args</code> and <code>kwargs</code>. <code>activation_function</code> and <code>scaling_factor</code> have to be vectors (or tuples) containing the chosen activation functions and respective scaling factors (<span>$f_1,...,f_D$</span> and <span>$\lambda_1,...,\lambda_D$</span> following the nomenclature introduced above). The <code>leaky_coefficient</code> represents <span>$\alpha$</span> and it is a single value.</p><p>Starting with the example, the data used is based on the following function based on the DAFESN paper (<a href="../../references/#Lun2015">Lun <em>et al.</em>, 2015</a>).</p><pre><code class="language-julia hljs">u(t) = sin(t) + sin(0.51 * t) + sin(0.22 * t) + sin(0.1002 * t) + sin(0.05343 * t)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">u (generic function with 1 method)</code></pre><p>For this example, the type of prediction will be one step ahead. The metric used to assure a good prediction will be the normalized root-mean-square deviation <code>rmsd</code> from <a href="https://juliastats.org/StatsBase.jl/stable/">StatsBase</a>. Like in the other examples, first it is needed to gather the data:</p><pre><code class="language-julia hljs">train_len = 3000
predict_len = 2000
shift = 1

data = u.(collect(0.0:0.01:500))
training_input = reduce(hcat, data[shift:(shift + train_len - 1)])
training_target = reduce(hcat, data[(shift + 1):(shift + train_len)])
testing_input = reduce(hcat,
    data[(shift + train_len):(shift + train_len + predict_len - 1)])
testing_target = reduce(hcat,
    data[(shift + train_len + 1):(shift + train_len + predict_len)])</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">1×2000 Matrix{Float64}:
 0.852897  0.850969  0.849127  0.847371  …  -1.4348  -1.4206  -1.40638</code></pre><p>To follow the paper more closely, it is necessary to define a couple of activation functions. The numbering of them follows the ones in the paper. Of course, one can also use any custom-defined function, available in the base language or any activation function from <a href="https://fluxml.ai/NNlib.jl/stable/reference/#Activation-Functions">NNlib</a>.</p><pre><code class="language-julia hljs">f2(x) = (1 - exp(-x)) / (2 * (1 + exp(-x)))
f3(x) = (2 / pi) * atan((pi / 2) * x)
f4(x) = x / sqrt(1 + x * x)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">f4 (generic function with 1 method)</code></pre><p>It is now possible to build different drivers, using the parameters suggested by the paper. Also, in this instance, the numbering follows the test cases of the paper. In the end, a simple for loop is implemented to compare the different drivers and activation functions.</p><pre><code class="language-julia hljs">using ReservoirComputing, Random, StatsBase

#fix seed for reproducibility
Random.seed!(42)

#baseline case with RNN() driver. Parameter given as args
base_case = RNN(tanh, 0.85)

#MRNN() test cases
#Parameter given as kwargs
case3 = MRNN(; activation_function=[tanh, f2],
    leaky_coefficient=0.85,
    scaling_factor=[0.5, 0.3])

#Parameter given as kwargs
case4 = MRNN(; activation_function=[tanh, f3],
    leaky_coefficient=0.9,
    scaling_factor=[0.45, 0.35])

#Parameter given as args
case5 = MRNN([tanh, f4], 0.9, [0.43, 0.13])

#tests
test_cases = [base_case, case3, case4, case5]
for case in test_cases
    esn = ESN(training_input, 1, 100;
        input_layer=weighted_init(; scaling=0.3),
        reservoir=rand_sparse(; radius=0.4),
        reservoir_driver=case,
        states_type=ExtendedStates())
    wout = train(esn, training_target, StandardRidge(10e-6))
    output = esn(Predictive(testing_input), wout)
    println(rmsd(testing_target, output; normalize=true))
end</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">1.1442590780710705e-5
9.128307322964925e-5
0.0001348192607379247
0.0001559992855139628</code></pre><p>In this example, it is also possible to observe the input of parameters to the methods <code>RNN()</code> <code>MRNN()</code>, both by argument and by keyword argument.</p><h2 id="Gated-Recurrent-Unit"><a class="docs-heading-anchor" href="#Gated-Recurrent-Unit">Gated Recurrent Unit</a><a id="Gated-Recurrent-Unit-1"></a><a class="docs-heading-anchor-permalink" href="#Gated-Recurrent-Unit" title="Permalink"></a></h2><p>Gated Recurrent Units (GRUs) (<a href="../../references/#Cho2014">Cho <em>et al.</em>, 2014</a>) have been proposed in more recent years with the intent of limiting notable problems of RNNs, like the vanishing gradient. This change in the underlying equations can be easily transported into the Reservoir Computing paradigm, by switching the RNN equations in the reservoir with the GRU equations. This approach has been explored in (<a href="../../references/#Wang2020">Wang <em>et al.</em>, Jul 2020</a>) and (<a href="../../references/#Sarli2020">Sarli <em>et al.</em>, Aug 2020</a>). Different variations of GRU have been proposed (<a href="../../references/#Dey2017">Dey and Salem, Aug 2017</a>); this section is subdivided into different sections that go into detail about the governing equations and the implementation of them into ReservoirComputing.jl. Like before, to access the GRU reservoir driver, it suffices to change the <code>reservoir_diver</code> keyword argument for <code>ESN</code> with <code>GRU()</code>. All the variations that will be presented can be used in this package by leveraging the keyword argument <code>variant</code> in the method <code>GRU()</code> and specifying the chosen variant: <code>FullyGated()</code> or <code>Minimal()</code>. Other variations are possible by modifying the inner layers and reservoirs. The default is set to the standard version <code>FullyGated()</code>. The first section will go into more detail about the default of the <code>GRU()</code> method, and the following ones will refer to it to minimize repetitions.</p><h3 id="Standard-GRU"><a class="docs-heading-anchor" href="#Standard-GRU">Standard GRU</a><a id="Standard-GRU-1"></a><a class="docs-heading-anchor-permalink" href="#Standard-GRU" title="Permalink"></a></h3><p>The equations for the standard GRU are as follows:</p><p class="math-container">\[\mathbf{r}(t) = \sigma (\mathbf{W}^r_{\text{in}}\mathbf{u}(t)+\mathbf{W}^r\mathbf{x}(t-1)+\mathbf{b}_r) \\
\mathbf{z}(t) = \sigma (\mathbf{W}^z_{\text{in}}\mathbf{u}(t)+\mathbf{W}^z\mathbf{x}(t-1)+\mathbf{b}_z) \\
\tilde{\mathbf{x}}(t) = \text{tanh}(\mathbf{W}_{in}\mathbf{u}(t)+\mathbf{W}(\mathbf{r}(t) \odot \mathbf{x}(t-1))+\mathbf{b}) \\
\mathbf{x}(t) = \mathbf{z}(t) \odot \mathbf{x}(t-1)+(1-\mathbf{z}(t)) \odot \tilde{\mathbf{x}}(t)\]</p><p>Going over the <code>GRU</code> keyword argument, it will be explained how to feed the desired input to the model.</p><ul><li><code>activation_function</code> is a vector with default values <code>[NNlib.sigmoid, NNlib.sigmoid, tanh]</code>. This argument controls the activation functions of the GRU, going from top to bottom. Changing the first element corresponds to changing the activation function for <span>$\mathbf{r}(t)$</span> and so on.</li><li><code>inner_layer</code> is a vector with default values <code>fill(DenseLayer(), 2)</code>. This keyword argument controls the <span>$\mathbf{W}_{\text{in}}$</span>s going from top to bottom like before.</li><li><code>reservoir</code> is a vector with default value <code>fill(RandSparseReservoir(), 2)</code>. Similarly to <code>inner_layer</code>, this keyword argument controls the reservoir matrix construction in a top to bottom order.</li><li><code>bias</code> is again a vector with default value <code>fill(DenseLayer(), 2)</code>. It is meant to control the <span>$\mathbf{b}$</span>s, going as usual from top to bottom.</li><li><code>variant</code> controls the GRU variant. The default value is set to <code>FullyGated()</code>.</li></ul><p>It is important to notice that <code>inner_layer</code> and <code>reservoir</code> control every layer except <span>$\mathbf{W}_{in}$</span> and <span>$\mathbf{W}$</span> and <span>$\mathbf{b}$</span>. These arguments are given as input to the <code>ESN()</code> call as <code>input_layer</code>, <code>reservoir</code> and <code>bias</code>.</p><p>The following sections are going to illustrate the variations of the GRU architecture and how to obtain them in ReservoirComputing.jl</p><h3 id="Type-1"><a class="docs-heading-anchor" href="#Type-1">Type 1</a><a id="Type-1-1"></a><a class="docs-heading-anchor-permalink" href="#Type-1" title="Permalink"></a></h3><p>The first variation of the GRU is dependent only on the previous hidden state and the bias:</p><p class="math-container">\[\mathbf{r}(t) = \sigma (\mathbf{W}^r\mathbf{x}(t-1)+\mathbf{b}_r) \\
\mathbf{z}(t) = \sigma (\mathbf{W}^z\mathbf{x}(t-1)+\mathbf{b}_z) \\\]</p><p>To obtain this variation, it will suffice to set <code>inner_layer = fill(NullLayer(), 2)</code> and leaving the <code>variant = FullyGated()</code>.</p><h3 id="Type-2"><a class="docs-heading-anchor" href="#Type-2">Type 2</a><a id="Type-2-1"></a><a class="docs-heading-anchor-permalink" href="#Type-2" title="Permalink"></a></h3><p>The second variation only depends on the previous hidden state:</p><p class="math-container">\[\mathbf{r}(t) = \sigma (\mathbf{W}^r\mathbf{x}(t-1)) \\
\mathbf{z}(t) = \sigma (\mathbf{W}^z\mathbf{x}(t-1)) \\\]</p><p>Similarly to before, to obtain this variation, it is only required to set <code>inner_layer = fill(NullLayer(), 2)</code> and <code>bias = fill(NullLayer(), 2)</code> while keeping <code>variant = FullyGated()</code>.</p><h3 id="Type-3"><a class="docs-heading-anchor" href="#Type-3">Type 3</a><a id="Type-3-1"></a><a class="docs-heading-anchor-permalink" href="#Type-3" title="Permalink"></a></h3><p>The final variation, before the minimal one, depends only on the biases</p><p class="math-container">\[\mathbf{r}(t) = \sigma (\mathbf{b}_r) \\
\mathbf{z}(t) = \sigma (\mathbf{b}_z) \\\]</p><p>This means that it is only needed to set <code>inner_layer = fill(NullLayer(), 2)</code> and <code>reservoir = fill(NullReservoir(), 2)</code> while keeping <code>variant = FullyGated()</code>.</p><h3 id="Minimal"><a class="docs-heading-anchor" href="#Minimal">Minimal</a><a id="Minimal-1"></a><a class="docs-heading-anchor-permalink" href="#Minimal" title="Permalink"></a></h3><p>The minimal GRU variation merges two gates into one:</p><p class="math-container">\[\mathbf{f}(t) = \sigma (\mathbf{W}^f_{\text{in}}\mathbf{u}(t)+\mathbf{W}^f\mathbf{x}(t-1)+\mathbf{b}_f) \\
\tilde{\mathbf{x}}(t) = \text{tanh}(\mathbf{W}_{in}\mathbf{u}(t)+\mathbf{W}(\mathbf{f}(t) \odot \mathbf{x}(t-1))+\mathbf{b}) \\
\mathbf{x}(t) = (1-\mathbf{f}(t)) \odot \mathbf{x}(t-1) + \mathbf{f}(t) \odot \tilde{\mathbf{x}}(t)\]</p><p>This variation can be obtained by setting <code>variation=Minimal()</code>. The <code>inner_layer</code>, <code>reservoir</code> and <code>bias</code> kwargs this time are <strong>not</strong> vectors, but must be defined like, for example <code>inner_layer = DenseLayer()</code> or <code>reservoir = SparseDenseReservoir()</code>.</p><h3 id="Examples"><a class="docs-heading-anchor" href="#Examples">Examples</a><a id="Examples-1"></a><a class="docs-heading-anchor-permalink" href="#Examples" title="Permalink"></a></h3><p>To showcase the use of the <code>GRU()</code> method, this section will only illustrate the standard <code>FullyGated()</code> version. The full script for this example with the data can be found <a href="https://github.com/MartinuzziFrancesco/reservoir-computing-examples/tree/main/change_drivers/gru">here</a>.</p><p>The data used for this example is the Santa Fe laser dataset (<a href="../../references/#Hbner1989">Hübner <em>et al.</em>, 1989</a>) retrieved from <a href="https://web.archive.org/web/20160427182805/http://www-psych.stanford.edu/%7Eandreas/Time-Series/SantaFe.html">here</a>. The data is split to account for a next step prediction.</p><pre><code class="language-julia hljs">using DelimitedFiles

data = reduce(hcat, readdlm(&quot;./data/santafe_laser.txt&quot;))

train_len = 5000
predict_len = 2000

training_input = data[:, 1:train_len]
training_target = data[:, 2:(train_len + 1)]
testing_input = data[:, (train_len + 1):(train_len + predict_len)]
testing_target = data[:, (train_len + 2):(train_len + predict_len + 1)]</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">1×2000 Matrix{Float64}:
 49.0  107.0  126.0  67.0  31.0  22.0  …  9.0  7.0  6.0  6.0  17.0  95.0</code></pre><p>The construction of the ESN proceeds as usual.</p><pre><code class="language-julia hljs">using ReservoirComputing, Random

res_size = 300
res_radius = 1.4

Random.seed!(42)
esn = ESN(training_input, 1, res_size;
    reservoir=rand_sparse(; radius=res_radius),
    reservoir_driver=GRU())</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">ESN(1 =&gt; 300)</code></pre><p>The default inner reservoir and input layer for the GRU are the same defaults for the <code>reservoir</code> and <code>input_layer</code> of the ESN. One can use the explicit call if they choose to.</p><pre><code class="language-julia hljs">gru = GRU(; reservoir=[rand_sparse,
        rand_sparse],
    inner_layer=[scaled_rand, scaled_rand])
esn = ESN(training_input, 1, res_size;
    reservoir=rand_sparse(; radius=res_radius),
    reservoir_driver=gru)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">ESN(1 =&gt; 300)</code></pre><p>The training and prediction can proceed as usual:</p><pre><code class="language-julia hljs">training_method = StandardRidge(0.0)
output_layer = train(esn, training_target, training_method)
output = esn(Predictive(testing_input), output_layer)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">1×2000 Matrix{Float64}:
 50.5302  108.952  127.601  66.6522  …  5.12525  7.94983  15.3569  101.566</code></pre><p>The results can be plotted using Plots.jl</p><pre><code class="language-julia hljs">using Plots

plot([testing_target&#39; output&#39;]; label=[&quot;actual&quot; &quot;predicted&quot;],
    plot_title=&quot;Santa Fe Laser&quot;,
    titlefontsize=20,
    legendfontsize=12,
    linewidth=2.5,
    xtickfontsize=12,
    ytickfontsize=12,
    size=(1080, 720))</code></pre><img src="d775ad26.svg" alt="Example block output"/><p>It is interesting to see a comparison of the GRU driven ESN and the standard RNN driven ESN. Using the same parameters defined before it is possible to do the following</p><pre><code class="language-julia hljs">using StatsBase

esn_rnn = ESN(training_input, 1, res_size;
    reservoir=rand_sparse(; radius=res_radius),
    reservoir_driver=RNN())

output_layer = train(esn_rnn, training_target, training_method)
output_rnn = esn_rnn(Predictive(testing_input), output_layer)

println(msd(testing_target, output))
println(msd(testing_target, output_rnn))</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">11.971200007092337
6.816207887932469</code></pre><h2 id="References"><a class="docs-heading-anchor" href="#References">References</a><a id="References-1"></a><a class="docs-heading-anchor-permalink" href="#References" title="Permalink"></a></h2><div class="citation noncanonical"><ul><li><div>Cho, K.; Van Merriënboer, B.; Gulcehre, C.; Bahdanau, D.; Bougares, F.; Schwenk, H. and Bengio, Y. (2014). <em>Learning phrase representations using RNN encoder-decoder for statistical machine translation</em>, arXiv preprint arXiv:1406.1078.</div></li><li><div>Dey, R. and Salem, F. M. (Aug 2017). <a href="http://dx.doi.org/10.1109/MWSCAS.2017.8053243"><em>Gate-variants of Gated Recurrent Unit (GRU) neural networks</em></a>. In: <a href="https://doi.org/10.1109/mwscas.2017.8053243"><em>2017 IEEE 60th International Midwest Symposium on Circuits and Systems (MWSCAS)</em></a> (IEEE); pp. 1597–1600.</div></li><li><div>Hübner, U.; Abraham, N. B. and Weiss, C. O. (1989). <a href="http://dx.doi.org/10.1103/PhysRevA.40.6354"><em>Dimensions and entropies of chaotic intensity pulsations in a single-mode far-infrared&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;inline&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;NH&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;3&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;laser</em></a>. <a href="https://doi.org/10.1103/physreva.40.6354">Physical Review A <strong>40</strong>, 6354–6365</a>.</div></li><li><div>Lun, S.-X.; Yao, X.-S.; Qi, H.-Y. and Hu, H.-F. (2015). <a href="http://dx.doi.org/10.1016/j.neucom.2015.02.029"><em>A novel model of leaky integrator echo state network for time-series prediction</em></a>. <a href="https://doi.org/10.1016/j.neucom.2015.02.029">Neurocomputing <strong>159</strong>, 58–66</a>.</div></li><li><div>Sarli, D. D.; Gallicchio, C. and Micheli, A. (Aug 2020). <a href="http://dx.doi.org/10.1109/INISTA49547.2020.9194681"><em>Gated Echo State Networks: a preliminary study</em></a>. In: <a href="https://doi.org/10.1109/inista49547.2020.9194681"><em>2020 International Conference on INnovations in Intelligent SysTems and Applications (INISTA)</em></a> (IEEE); pp. 1–5.</div></li><li><div>Wang, X.; Jin, Y. and Hao, K. (Jul 2020). <a href="http://dx.doi.org/10.1109/IJCNN48605.2020.9206786"><em>A Gated Recurrent Unit based Echo State Network</em></a>. In: <a href="https://doi.org/10.1109/ijcnn48605.2020.9206786"><em>2020 International Joint Conference on Neural Networks (IJCNN)</em></a> (IEEE); pp. 1–7.</div></li></ul></div></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../change_layers/">« Using Different Layers</a><a class="docs-footer-nextpage" href="../deep_esn/">Deep Echo State Networks »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.13.0 on <span class="colophon-date" title="Friday 20 June 2025 14:59">Friday 20 June 2025</span>. Using Julia version 1.11.5.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
