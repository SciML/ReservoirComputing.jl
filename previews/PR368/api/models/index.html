<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Models · ReservoirComputing.jl</title><meta name="title" content="Models · ReservoirComputing.jl"/><meta property="og:title" content="Models · ReservoirComputing.jl"/><meta property="twitter:title" content="Models · ReservoirComputing.jl"/><meta name="description" content="Documentation for ReservoirComputing.jl."/><meta property="og:description" content="Documentation for ReservoirComputing.jl."/><meta property="twitter:description" content="Documentation for ReservoirComputing.jl."/><meta property="og:url" content="https://docs.sciml.ai/ReservoirComputing/stable/api/models/"/><meta property="twitter:url" content="https://docs.sciml.ai/ReservoirComputing/stable/api/models/"/><link rel="canonical" href="https://docs.sciml.ai/ReservoirComputing/stable/api/models/"/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/favicon.ico" rel="icon" type="image/x-icon"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.png" alt="ReservoirComputing.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">ReservoirComputing.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">ReservoirComputing.jl</a></li><li><a class="tocitem" href="../../getting_started/">Getting Started with ReservoirComputing.jl</a></li><li><span class="tocitem">Tutorials</span><ul><li><a class="tocitem" href="../../tutorials/scratch/">Building a model from scratch</a></li><li><a class="tocitem" href="../../tutorials/lorenz_basic/">Chaos forecasting with an ESN</a></li><li><a class="tocitem" href="../../tutorials/ngrc/">Fitting a Next Generation Reservoir Computer</a></li><li><a class="tocitem" href="../../tutorials/deep_esn/">Deep Echo State Networks</a></li><li><a class="tocitem" href="../../tutorials/reca/">Reservoir Computing with Cellular Automata</a></li></ul></li><li><span class="tocitem">Examples</span><ul><li><a class="tocitem" href="../../examples/model_es2n/">Building a model to add to ReservoirComputing.jl</a></li></ul></li><li><span class="tocitem">API Documentation</span><ul><li><a class="tocitem" href="../layers/">Layers</a></li><li class="is-active"><a class="tocitem" href>Models</a><ul class="internal"><li><a class="tocitem" href="#Echo-State-Networks"><span>Echo State Networks</span></a></li><li><a class="tocitem" href="#Next-generation-reservoir-computing"><span>Next generation reservoir computing</span></a></li><li><a class="tocitem" href="#Reservoir-Computing-with-Cellular-Automata"><span>Reservoir Computing with Cellular Automata</span></a></li></ul></li><li><a class="tocitem" href="../utils/">Utilities</a></li><li><a class="tocitem" href="../train/">Train</a></li><li><a class="tocitem" href="../predict/">Predict</a></li><li><a class="tocitem" href="../states/">States</a></li><li><a class="tocitem" href="../inits/">Initializers</a></li></ul></li><li><a class="tocitem" href="../../references/">References</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">API Documentation</a></li><li class="is-active"><a href>Models</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Models</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/SciML/ReservoirComputing.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/SciML/ReservoirComputing.jl/blob/master/docs/src/api/models.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Models"><a class="docs-heading-anchor" href="#Models">Models</a><a id="Models-1"></a><a class="docs-heading-anchor-permalink" href="#Models" title="Permalink"></a></h1><h2 id="Echo-State-Networks"><a class="docs-heading-anchor" href="#Echo-State-Networks">Echo State Networks</a><a id="Echo-State-Networks-1"></a><a class="docs-heading-anchor-permalink" href="#Echo-State-Networks" title="Permalink"></a></h2><article><details class="docstring" open="true"><summary id="ReservoirComputing.EIESN"><a class="docstring-binding" href="#ReservoirComputing.EIESN"><code>ReservoirComputing.EIESN</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">EIESN(in_dims, res_dims, out_dims, activation=tanh_fast;
    exc_recurrence_scale=0.9, inh_recurrence_scale=0.5, exc_output_scale=1.0, inh_output_scale  =1.0,
    init_reservoir=rand_sparse, init_input=scaled_rand,
    init_state=randn32,
    readout_activation=identity,
    state_modifiers=(),
    kwargs...)</code></pre><p>Excitatory-Inhibitory Echo State Network (EIESN) (<a href="../../references/#Panahi2025">Panahi <em>et al.</em>, 2025</a>).</p><p><strong>Equations</strong></p><p class="math-container">\[\begin{aligned}
    \mathbf{x}(t) &amp;= b_{\mathrm{ex}} \, \phi\!\left(\mathbf{W}_{\mathrm{in}} \mathbf{u}(t) + a_{\mathrm{ex}} \mathbf{A} \mathbf{x}(t-1)\right)
    - b_{\mathrm{inh}} \, \phi\!\left(\mathbf{W}_{\mathrm{in}} \mathbf{u}(t) + a_{\mathrm{inh}} \mathbf{A} \mathbf{x}(t-1)\right) \\
    \mathbf{z}(t) &amp;= \mathrm{Mods}\!\left(\mathbf{x}(t)\right) \\
    \mathbf{y}(t) &amp;= \rho\!\left( \mathbf{W}_{\text{out}}\, \mathbf{z}(t) + \mathbf{b}_{\text{out}} \right)
\end{aligned}\]</p><p><strong>Arguments</strong></p><ul><li><code>in_dims</code>: Input dimension.</li><li><code>res_dims</code>: Reservoir (hidden state) dimension.</li><li><code>out_dims</code>: Output dimension.</li><li><code>activation</code>: Reservoir activation (for <a href="../layers/#ReservoirComputing.EIESNCell"><code>EIESNCell</code></a>). Default: <code>tanh_fast</code>.</li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>exc_recurrence_scale</code>: Excitatory recurrence scaling factor. Default: <code>0.9</code>.</li><li><code>inh_recurrence_scale</code>: Inhibitory recurrence scaling factor. Default: <code>0.5</code>.</li><li><code>exc_output_scale</code>: Excitatory output scaling factor. Default: <code>1.0</code>.</li><li><code>inh_output_scale</code>: Inhibitory output scaling factor. Default: <code>1.0</code>.</li><li><code>init_reservoir</code>: Initializer for the reservoir matrix. Default: <a href="../inits/rand_sparse/#rand_sparse"><code>rand_sparse</code></a>.</li><li><code>init_input</code>: Initializer for the input matrix. Default: <a href="../inits/scaled_rand/#scaled_rand"><code>scaled_rand</code></a>.</li><li><code>init_state</code>: Initializer used when an external state is not provided. Default: <code>randn32</code>.</li><li><code>readout_activation</code>: Activation for the linear readout. Default: <code>identity</code>.</li><li><code>state_modifiers</code>: A layer or collection of layers applied to the reservoir state before the readout. Accepts a single layer, an <code>AbstractVector</code>, or a <code>Tuple</code>. Default: empty <code>()</code>.</li></ul><p><strong>Inputs</strong></p><ul><li><code>x :: AbstractArray (in_dims, batch)</code></li></ul><p><strong>Returns</strong></p><ul><li>Output <code>y :: (out_dims, batch)</code>.</li><li>Updated layer state (NamedTuple).</li></ul><p><strong>Parameters</strong></p><ul><li><code>reservoir</code> — parameters of the internal <a href="../layers/#ReservoirComputing.EIESNCell"><code>EIESNCell</code></a>.</li><li><code>states_modifiers</code> — a <code>Tuple</code> with parameters for each modifier layer (may be empty).</li><li><code>readout</code> — parameters of <a href="../layers/#ReservoirComputing.LinearReadout"><code>LinearReadout</code></a>.</li></ul><p><strong>States</strong></p><ul><li><code>reservoir</code> — states for the internal <a href="../layers/#ReservoirComputing.EIESNCell"><code>EIESNCell</code></a> (e.g. <code>rng</code>).</li><li><code>states_modifiers</code> — a <code>Tuple</code> with states for each modifier layer.</li><li><code>readout</code> — states for <a href="../layers/#ReservoirComputing.LinearReadout"><code>LinearReadout</code></a>.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/SciML/ReservoirComputing.jl/blob/dc3fb6dcb873ce0f7f264010f03ef2343047ee2e/src/models/eiesn.jl#L1-L64">source</a></section></details></article><article><details class="docstring" open="true"><summary id="ReservoirComputing.ES2N"><a class="docstring-binding" href="#ReservoirComputing.ES2N"><code>ReservoirComputing.ES2N</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">ES2N(in_dims, res_dims, out_dims, activation=tanh;
    proximity=1.0, init_reservoir=rand_sparse, init_input=scaled_rand,
    init_bias=zeros32, init_state=randn32, use_bias=False(),
    state_modifiers=(), readout_activation=identity,
    init_orthogonal=orthogonal,)</code></pre><p>Edge of Stability Echo State Network (ES2N) (<a href="../../references/#Ceni2025">Ceni and Gallicchio, 2025</a>).</p><p><strong>Equations</strong></p><p class="math-container">\[\begin{aligned}
    \mathbf{x}(t) &amp;= (1-\beta)\, \mathbf{O}\, \mathbf{x}(t-1) +
        \beta\, \phi\!\left(\mathbf{W}_{\text{in}} \mathbf{u}(t)
        + \mathbf{W}_r \mathbf{x}(t-1) + \mathbf{b} \right) \\
    \mathbf{z}(t) &amp;= \mathrm{Mods}\!\left(\mathbf{x}(t)\right) \\
    \mathbf{y}(t) &amp;= \rho\!\left(
        \mathbf{W}_{\text{out}}\, \mathbf{z}(t)
        + \mathbf{b}_{\text{out}} \right)
\end{aligned}\]</p><p><strong>Arguments</strong></p><ul><li><code>in_dims</code>: Input dimension.</li><li><code>res_dims</code>: Reservoir (hidden state) dimension.</li><li><code>out_dims</code>: Output dimension.</li><li><code>activation</code>: Reservoir activation (for <a href="../layers/#ReservoirComputing.ESNCell"><code>ESNCell</code></a>). Default: <code>tanh</code>.</li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>proximity</code>: proximity <code>α ∈ (0,1]</code>. Default: <code>1.0</code>.</li><li><code>init_reservoir</code>: Initializer for <code>W_res</code>. Default: <a href="../inits/rand_sparse/#rand_sparse"><code>rand_sparse</code></a>.</li><li><code>init_input</code>: Initializer for <code>W_in</code>. Default: <a href="../inits/scaled_rand/#scaled_rand"><code>scaled_rand</code></a>.</li><li><code>init_orthogonal</code>: Initializer for <code>O</code>. Default: [<code>orthogonal</code>].</li><li><code>init_bias</code>: Initializer for reservoir bias (used if <code>use_bias=true</code>). Default: <code>zeros32</code>.</li><li><code>init_state</code>: Initializer used when an external state is not provided. Default: <code>randn32</code>.</li><li><code>use_bias</code>: Whether the reservoir uses a bias term. Default: <code>false</code>.</li><li><code>state_modifiers</code>: A layer or collection of layers applied to the reservoir state before the readout. Accepts a single layer, an <code>AbstractVector</code>, or a <code>Tuple</code>. Default: empty <code>()</code>.</li><li><code>readout_activation</code>: Activation for the linear readout. Default: <code>identity</code>.</li></ul><p><strong>Inputs</strong></p><ul><li><code>x :: AbstractArray (in_dims, batch)</code></li></ul><p><strong>Returns</strong></p><ul><li>Output <code>y :: (out_dims, batch)</code>.</li><li>Updated layer state (NamedTuple).</li></ul><p><strong>Parameters</strong></p><ul><li><code>reservoir</code> — parameters of the internal <a href="../layers/#ReservoirComputing.ESNCell"><code>ESNCell</code></a>, including:<ul><li><code>input_matrix :: (res_dims × in_dims)</code> — <code>W_in</code></li><li><code>reservoir_matrix :: (res_dims × res_dims)</code> — <code>W_res</code></li><li><code>orthogonal_matrix :: (res_dims × res_dims)</code> — <code>O</code></li><li><code>bias :: (res_dims,)</code> — present only if <code>use_bias=true</code></li></ul></li><li><code>states_modifiers</code> — a <code>Tuple</code> with parameters for each modifier layer (may be empty).</li><li><code>readout</code> — parameters of <a href="../layers/#ReservoirComputing.LinearReadout"><code>LinearReadout</code></a>, typically:<ul><li><code>weight :: (out_dims × res_dims)</code> — <code>W_out</code></li><li><code>bias :: (out_dims,)</code> — <code>b_out</code> (if the readout uses bias)</li></ul></li></ul><blockquote><p>Exact field names for modifiers/readout follow their respective layer definitions.</p></blockquote><p><strong>States</strong></p><ul><li><code>reservoir</code> — states for the internal <a href="../layers/#ReservoirComputing.ES2NCell"><code>ES2NCell</code></a> (e.g. <code>rng</code> used to sample initial hidden states).</li><li><code>states_modifiers</code> — a <code>Tuple</code> with states for each modifier layer.</li><li><code>readout</code> — states for <a href="../layers/#ReservoirComputing.LinearReadout"><code>LinearReadout</code></a>.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/SciML/ReservoirComputing.jl/blob/dc3fb6dcb873ce0f7f264010f03ef2343047ee2e/src/models/es2n.jl#L1-L78">source</a></section></details></article><article><details class="docstring" open="true"><summary id="ReservoirComputing.ESN"><a class="docstring-binding" href="#ReservoirComputing.ESN"><code>ReservoirComputing.ESN</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">ESN(in_dims, res_dims, out_dims, activation=tanh;
    leak_coefficient=1.0, init_reservoir=rand_sparse, init_input=scaled_rand,
    init_bias=zeros32, init_state=randn32, use_bias=false,
    state_modifiers=(), readout_activation=identity)</code></pre><p>Echo State Network (<a href="../../references/#Jaeger2004">Jaeger and Haas, 2004</a>).</p><p><code>ESN</code> composes:</p><ol><li>a stateful <a href="../layers/#ReservoirComputing.ESNCell"><code>ESNCell</code></a> (reservoir),</li><li>zero or more <code>state_modifiers</code> applied to the reservoir state, and</li><li>a <a href="../layers/#ReservoirComputing.LinearReadout"><code>LinearReadout</code></a> mapping reservoir features to outputs.</li></ol><p><strong>Equations</strong></p><p class="math-container">\[\begin{aligned}
    \mathbf{x}(t) &amp;= (1-\alpha)\, \mathbf{x}(t-1) + \alpha\, \phi\!\left(
        \mathbf{W}_{\text{in}}\, \mathbf{u}(t) + \mathbf{W}_r\, \mathbf{x}(t-1)
        + \mathbf{b} \right) \\
    \mathbf{z}(t) &amp;= \mathrm{Mods}\!\left(\mathbf{x}(t)\right) \\
    \mathbf{y}(t) &amp;= \rho\!\left(
        \mathbf{W}_{\text{out}}\, \mathbf{z}(t)
        + \mathbf{b}_{\text{out}} \right)
\end{aligned}\]</p><p><strong>Arguments</strong></p><ul><li><code>in_dims</code>: Input dimension.</li><li><code>res_dims</code>: Reservoir (hidden state) dimension.</li><li><code>out_dims</code>: Output dimension.</li><li><code>activation</code>: Reservoir activation (for <a href="../layers/#ReservoirComputing.ESNCell"><code>ESNCell</code></a>). Default: <code>tanh</code>.</li></ul><p><strong>Keyword arguments</strong></p><p>Reservoir (passed to <a href="../layers/#ReservoirComputing.ESNCell"><code>ESNCell</code></a>):</p><ul><li><code>leak_coefficient</code>: Leak rate <code>α ∈ (0,1]</code>. Default: <code>1.0</code>.</li><li><code>init_reservoir</code>: Initializer for <code>W_res</code>. Default: <a href="../inits/rand_sparse/#rand_sparse"><code>rand_sparse</code></a>.</li><li><code>init_input</code>: Initializer for <code>W_in</code>. Default: <a href="../inits/scaled_rand/#scaled_rand"><code>scaled_rand</code></a>.</li><li><code>init_bias</code>: Initializer for reservoir bias (used if <code>use_bias=true</code>). Default: <code>zeros32</code>.</li><li><code>init_state</code>: Initializer used when an external state is not provided. Default: <code>randn32</code>.</li><li><code>use_bias</code>: Whether the reservoir uses a bias term. Default: <code>false</code>.</li></ul><p>Composition:</p><ul><li><code>state_modifiers</code>: A layer or collection of layers applied to the reservoir state before the readout. Accepts a single layer, an <code>AbstractVector</code>, or a <code>Tuple</code>. Default: empty <code>()</code>.</li><li><code>readout_activation</code>: Activation for the linear readout. Default: <code>identity</code>.</li></ul><p><strong>Inputs</strong></p><ul><li><code>x :: AbstractArray (in_dims, batch)</code></li></ul><p><strong>Returns</strong></p><ul><li>Output <code>y :: (out_dims, batch)</code>.</li><li>Updated layer state (NamedTuple).</li></ul><p><strong>Parameters</strong></p><ul><li><code>reservoir</code> — parameters of the internal <a href="../layers/#ReservoirComputing.ESNCell"><code>ESNCell</code></a>, including:<ul><li><code>input_matrix :: (res_dims × in_dims)</code> — <code>W_in</code></li><li><code>reservoir_matrix :: (res_dims × res_dims)</code> — <code>W_res</code></li><li><code>bias :: (res_dims,)</code> — present only if <code>use_bias=true</code></li></ul></li><li><code>states_modifiers</code> — a <code>Tuple</code> with parameters for each modifier layer (may be empty).</li><li><code>readout</code> — parameters of <a href="../layers/#ReservoirComputing.LinearReadout"><code>LinearReadout</code></a>, typically:<ul><li><code>weight :: (out_dims × res_dims)</code> — <code>W_out</code></li><li><code>bias :: (out_dims,)</code> — <code>b_out</code> (if the readout uses bias)</li></ul></li></ul><blockquote><p>Exact field names for modifiers/readout follow their respective layer definitions.</p></blockquote><p><strong>States</strong></p><ul><li><code>reservoir</code> — states for the internal <a href="../layers/#ReservoirComputing.ESNCell"><code>ESNCell</code></a> (e.g. <code>rng</code> used to sample initial hidden states).</li><li><code>states_modifiers</code> — a <code>Tuple</code> with states for each modifier layer.</li><li><code>readout</code> — states for <a href="../layers/#ReservoirComputing.LinearReadout"><code>LinearReadout</code></a>.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/SciML/ReservoirComputing.jl/blob/dc3fb6dcb873ce0f7f264010f03ef2343047ee2e/src/models/esn.jl#L1-L84">source</a></section></details></article><article><details class="docstring" open="true"><summary id="ReservoirComputing.EuSN"><a class="docstring-binding" href="#ReservoirComputing.EuSN"><code>ReservoirComputing.EuSN</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">EuSN(in_dims, res_dims, out_dims, activation=tanh;
    leak_coefficient=1.0, diffusion = 1.0, init_reservoir=rand_sparse, init_input=scaled_rand,
    init_bias=zeros32, init_state=randn32, use_bias=false,
    state_modifiers=(), readout_activation=identity)</code></pre><p>Euler State Network (ESN) (<a href="../../references/#Gallicchio2024">Gallicchio, 2024</a>).</p><p><strong>Equations</strong></p><p class="math-container">\[\begin{aligned}
    \mathbf{x}(t) &amp;= \mathbf{x}(t-1) + \varepsilon\, \phi\!\left(
        \mathbf{W}_{\text{in}}\, \mathbf{u}(t) +
        (\mathbf{W}_r - \gamma\, \mathbf{I})\, \mathbf{x}(t-1)
        + \mathbf{b} \right) \\
    \mathbf{z}(t) &amp;= \mathrm{Mods}\!\left(\mathbf{x}(t)\right) \\
    \mathbf{y}(t) &amp;= \rho\!\left(
        \mathbf{W}_{\text{out}}\, \mathbf{z}(t)
        + \mathbf{b}_{\text{out}} \right)
\end{aligned}\]</p><p><strong>Arguments</strong></p><ul><li><code>in_dims</code>: Input dimension.</li><li><code>res_dims</code>: Reservoir (hidden state) dimension.</li><li><code>out_dims</code>: Output dimension.</li><li><code>activation</code>: Reservoir activation (for <a href="../layers/#ReservoirComputing.ESNCell"><code>ESNCell</code></a>). Default: <code>tanh</code>.</li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>leak_coefficient</code>: Leak rate <code>α ∈ (0,1]</code>. Default: <code>1.0</code>.</li><li><code>diffusion</code>: diffusion coefficient <code>∈ (0,1]</code>. Default: <code>1.0</code>.</li><li><code>init_reservoir</code>: Initializer for <code>W_res</code>. Default: <a href="../inits/rand_sparse/#rand_sparse"><code>rand_sparse</code></a>.</li><li><code>init_input</code>: Initializer for <code>W_in</code>. Default: <a href="../inits/scaled_rand/#scaled_rand"><code>scaled_rand</code></a>.</li><li><code>init_bias</code>: Initializer for reservoir bias (used if <code>use_bias=true</code>). Default: <code>zeros32</code>.</li><li><code>init_state</code>: Initializer used when an external state is not provided. Default: <code>randn32</code>.</li><li><code>use_bias</code>: Whether the reservoir uses a bias term. Default: <code>false</code>.</li><li><code>state_modifiers</code>: A layer or collection of layers applied to the reservoir state before the readout. Accepts a single layer, an <code>AbstractVector</code>, or a <code>Tuple</code>. Default: empty <code>()</code>.</li><li><code>readout_activation</code>: Activation for the linear readout. Default: <code>identity</code>.</li></ul><p><strong>Inputs</strong></p><ul><li><code>x :: AbstractArray (in_dims, batch)</code></li></ul><p><strong>Returns</strong></p><ul><li>Output <code>y :: (out_dims, batch)</code>.</li><li>Updated layer state (NamedTuple).</li></ul><p><strong>Parameters</strong></p><ul><li><code>reservoir</code> — parameters of the internal <a href="../layers/#ReservoirComputing.ESNCell"><code>ESNCell</code></a>, including:<ul><li><code>input_matrix :: (res_dims × in_dims)</code> — <code>W_in</code></li><li><code>reservoir_matrix :: (res_dims × res_dims)</code> — <code>W_res</code></li><li><code>bias :: (res_dims,)</code> — present only if <code>use_bias=true</code></li></ul></li><li><code>states_modifiers</code> — a <code>Tuple</code> with parameters for each modifier layer (may be empty).</li><li><code>readout</code> — parameters of <a href="../layers/#ReservoirComputing.LinearReadout"><code>LinearReadout</code></a>, typically:<ul><li><code>weight :: (out_dims × res_dims)</code> — <code>W_out</code></li><li><code>bias :: (out_dims,)</code> — <code>b_out</code> (if the readout uses bias)</li></ul></li></ul><blockquote><p>Exact field names for modifiers/readout follow their respective layer definitions.</p></blockquote><p><strong>States</strong></p><ul><li><code>reservoir</code> — states for the internal <a href="../layers/#ReservoirComputing.ESNCell"><code>ESNCell</code></a> (e.g. <code>rng</code> used to sample initial hidden states).</li><li><code>states_modifiers</code> — a <code>Tuple</code> with states for each modifier layer.</li><li><code>readout</code> — states for <a href="../layers/#ReservoirComputing.LinearReadout"><code>LinearReadout</code></a>.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/SciML/ReservoirComputing.jl/blob/dc3fb6dcb873ce0f7f264010f03ef2343047ee2e/src/models/eusn.jl#L1-L76">source</a></section></details></article><article><details class="docstring" open="true"><summary id="ReservoirComputing.DeepESN"><a class="docstring-binding" href="#ReservoirComputing.DeepESN"><code>ReservoirComputing.DeepESN</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">DeepESN(in_dims, res_dims, out_dims,
        activation=tanh; depth=2, leak_coefficient=1.0, init_reservoir=rand_sparse,
        init_input=scaled_rand, init_bias=zeros32, init_state=randn32,
        use_bias=false, state_modifiers=(), readout_activation=identity)</code></pre><p>Deep Echo State Network (<a href="../../references/#Gallicchio2017">Gallicchio and Micheli, 2017</a>).</p><p><code>DeepESN</code> composes, for <code>L = length(res_dims)</code> layers:</p><ol><li>a sequence of stateful <a href="../layers/#ReservoirComputing.ESNCell"><code>ESNCell</code></a> with widths <code>res_dims[ℓ]</code>,</li><li>zero or more per-layer <code>state_modifiers[ℓ]</code> applied to the layer&#39;s state, and</li><li>a final <a href="../layers/#ReservoirComputing.LinearReadout"><code>LinearReadout</code></a> from the last layer&#39;s features to the output.</li></ol><p><strong>Equations</strong></p><p class="math-container">\[\begin{aligned}
    \mathbf{x}^{(1)}(t) &amp;= (1-\alpha_1)\, \mathbf{x}^{(1)}(t-1)
        + \alpha_1\, \phi_1\!\left(\mathbf{W}^{(1)}_{\text{in}}\, \mathbf{u}(t)
        + \mathbf{W}^{(1)}_r\, \mathbf{x}^{(1)}(t-1) + \mathbf{b}^{(1)} \right), \\
    \mathbf{u}^{(1)}(t) &amp;= \mathrm{Mods}_1\!\left(\mathbf{x}^{(1)}(t)\right), \\
    \mathbf{x}^{(\ell)}(t) &amp;= (1-\alpha_\ell)\, \mathbf{x}^{(\ell)}(t-1)
        + \alpha_\ell\, \phi_\ell\!\left(\mathbf{W}^{(\ell)}_{\text{in}}\,
        \mathbf{u}^{(\ell-1)}(t) + \mathbf{W}^{(\ell)}_r\, \mathbf{x}^{(\ell)}(t-1)
        + \mathbf{b}^{(\ell)} \right), \quad \ell = 2,\dots,L, \\
    \mathbf{u}^{(\ell)}(t) &amp;= \mathrm{Mods}_\ell\!\left(\mathbf{x}^{(\ell)}(t)\right),
        \quad \ell = 2,\dots,L, \\
    \mathbf{y}(t) &amp;= \rho\!\left(\mathbf{W}_{\text{out}}\, \mathbf{u}^{(L)}(t)
        + \mathbf{b}_{\text{out}} \right).
\end{aligned}\]</p><p><strong>Arguments</strong></p><ul><li><code>in_dims</code>: Input dimension.</li><li><code>res_dims</code>: Vector of reservoir (hidden) dimensions per layer; its length sets the depth <code>L</code>.</li><li><code>out_dims</code>: Output dimension.</li><li><code>activation</code>: Reservoir activation(s). Either a single function (broadcast to all layers) or a vector/tuple of length <code>L</code>. Default: <code>tanh</code>.</li></ul><p><strong>Keyword arguments</strong></p><p>Per-layer reservoir options (passed to each <a href="../layers/#ReservoirComputing.ESNCell"><code>ESNCell</code></a>):</p><ul><li><code>leak_coefficient</code>: Leak rate(s) <code>α_ℓ ∈ (0,1]</code>. Scalar or length-<code>L</code> collection. Default: <code>1.0</code>.</li><li><code>init_reservoir</code>: Initializer(s) for <code>W_res^{(ℓ)}</code>. Scalar or length-<code>L</code>. Default: <a href="../inits/rand_sparse/#rand_sparse"><code>rand_sparse</code></a>.</li><li><code>init_input</code>: Initializer(s) for <code>W_in^{(ℓ)}</code>. Scalar or length-<code>L</code>. Default: <a href="../inits/scaled_rand/#scaled_rand"><code>scaled_rand</code></a>.</li><li><code>init_bias</code>: Initializer(s) for reservoir bias (used iff <code>use_bias[ℓ]=true</code>). Scalar or length-<code>L</code>. Default: <code>zeros32</code>.</li><li><code>init_state</code>: Initializer(s) used when an external state is not provided. Scalar or length-<code>L</code>. Default: <code>randn32</code>.</li><li><code>use_bias</code>: Whether each reservoir uses a bias term. Boolean scalar or length-<code>L</code>. Default: <code>false</code>.</li><li><code>depth</code>: Depth of the DeepESN. If the reservoir size is given as a number instead of a vector, this parameter controls the depth of the model. Default is 2.</li></ul><p>Composition:</p><ul><li><code>state_modifiers</code>: Per-layer modifier(s) applied to each layer’s state before it feeds into the next layer (and the readout for the last layer). Accepts <code>nothing</code>, a single layer, a vector/tuple of length <code>L</code>, or per-layer collections. Defaults to no modifiers.</li><li><code>readout_activation</code>: Activation for the final linear readout. Default: <code>identity</code>.</li></ul><p><strong>Inputs</strong></p><ul><li><code>x :: AbstractArray (in_dims, batch)</code></li></ul><p><strong>Returns</strong></p><ul><li>Output <code>y :: (out_dims, batch)</code>.</li><li>Updated layer state (NamedTuple) containing states for all cells, modifiers, and readout.</li></ul><p><strong>Parameters</strong></p><ul><li><code>cells :: NTuple{L,NamedTuple}</code> — parameters for each <a href="../layers/#ReservoirComputing.ESNCell"><code>ESNCell</code></a>, including:<ul><li><code>input_matrix :: (res_dims[ℓ] × in_size[ℓ])</code> — <code>W_in^{(ℓ)}</code></li><li><code>reservoir_matrix :: (res_dims[ℓ] × res_dims[ℓ])</code> — <code>W_res^{(ℓ)}</code></li><li><code>bias :: (res_dims[ℓ],)</code> — present only if <code>use_bias[ℓ]=true</code></li></ul></li><li><code>states_modifiers :: NTuple{L,Tuple}</code> — per-layer tuples of modifier parameters (empty tuples if none).</li><li><code>readout</code> — parameters of <a href="../layers/#ReservoirComputing.LinearReadout"><code>LinearReadout</code></a>, typically:<ul><li><code>weight :: (out_dims × res_dims[L])</code> — <code>W_out</code></li><li><code>bias :: (out_dims,)</code> — <code>b_out</code> (if the readout uses bias)</li></ul></li></ul><blockquote><p>Exact field names for modifiers/readout follow their respective layer definitions.</p></blockquote><p><strong>States</strong></p><ul><li><code>cells :: NTuple{L,NamedTuple}</code> — states for each <a href="../layers/#ReservoirComputing.ESNCell"><code>ESNCell</code></a>.</li><li><code>states_modifiers :: NTuple{L,Tuple}</code> — per-layer tuples of modifier states.</li><li><code>readout</code> — states for <a href="../layers/#ReservoirComputing.LinearReadout"><code>LinearReadout</code></a>.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/SciML/ReservoirComputing.jl/blob/dc3fb6dcb873ce0f7f264010f03ef2343047ee2e/src/models/esn_deep.jl#L1-L91">source</a></section></details></article><article><details class="docstring" open="true"><summary id="ReservoirComputing.DelayESN"><a class="docstring-binding" href="#ReservoirComputing.DelayESN"><code>ReservoirComputing.DelayESN</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">DelayESN(in_dims, res_dims, out_dims, activation=tanh;
         num_delays=1, stride=1, leak_coefficient=1.0,
         init_reservoir=rand_sparse, init_input=scaled_rand,
         init_bias=zeros32, init_state=randn32, use_bias=false,
         state_modifiers=(), readout_activation=identity)</code></pre><p>Echo State Network with state delays (<a href="../../references/#Fleddermann2025">Fleddermann <em>et al.</em>, 2025</a>).</p><p><code>DelayESN</code> composes:</p><ol><li>a stateful <a href="../layers/#ReservoirComputing.ESNCell"><code>ESNCell</code></a> (reservoir),</li><li>a <a href="../layers/#ReservoirComputing.DelayLayer"><code>DelayLayer</code></a> applied to the reservoir state to build tapped-delay features,</li><li>zero or more additional <code>state_modifiers</code> applied after the delay, and</li><li>a <a href="../layers/#ReservoirComputing.LinearReadout"><code>LinearReadout</code></a> mapping delayed reservoir features to outputs.</li></ol><p>At each time step, the reservoir produces a state vector <code>h(t)</code> of length <code>res_dims</code>. The <code>DelayLayer</code> then constructs a feature vector that stacks <code>h(t)</code> together with <code>num_delays</code> past states, spaced according to <code>stride</code>, before passing it on to any further modifiers and the readout.</p><p><strong>Equations</strong></p><p class="math-container">\[\begin{aligned}
    \mathbf{x}(t) &amp;= (1-\alpha)\, \mathbf{x}(t-1) + \alpha\, \phi\!\left(
        \mathbf{W}_{\text{in}}\, \mathbf{u}(t) + \mathbf{W}_r\, \mathbf{x}(t-1)
        + \mathbf{b} \right), \\
    \mathbf{x}_{\mathrm{d}}(t) &amp;= \begin{bmatrix} \mathbf{x}(t) \\
    \mathbf{x}(t-s) \\
    \vdots \\
    \mathbf{x}\!\bigl(t-Ds\bigr) \end{bmatrix},
        \qquad D=\text{num\_delays},\ \ s=\text{stride}, \\
    \mathbf{z}(t) &amp;= \psi\!\left(\mathrm{Mods}\!\left(
        \mathbf{x}_{\mathrm{d}}(t)\right)\right), \\
        \mathbf{y}(t) &amp;= \rho\!\left(\mathbf{W}_{\text{out}}\,
        \mathbf{z}(t) + \mathbf{b}_{\text{out}} \right)
\end{aligned}\]</p><p><strong>Arguments</strong></p><ul><li><code>in_dims</code>: Input dimension.</li><li><code>res_dims</code>: Reservoir (hidden state) dimension.</li><li><code>out_dims</code>: Output dimension.</li><li><code>activation</code>: Reservoir activation (for <a href="../layers/#ReservoirComputing.ESNCell"><code>ESNCell</code></a>). Default: <code>tanh</code>.</li></ul><p><strong>Keyword arguments</strong></p><p>Reservoir (passed to <a href="../layers/#ReservoirComputing.ESNCell"><code>ESNCell</code></a>):</p><ul><li><code>leak_coefficient</code>: Leak rate in <code>(0, 1]</code>. Default: <code>1.0</code>.</li><li><code>init_reservoir</code>: Initializer for <code>W_res</code>. Default: <a href="../inits/rand_sparse/#rand_sparse"><code>rand_sparse</code></a>.</li><li><code>init_input</code>: Initializer for <code>W_in</code>. Default: <a href="../inits/scaled_rand/#scaled_rand"><code>scaled_rand</code></a>.</li><li><code>init_bias</code>: Initializer for reservoir bias (used iff <code>use_bias=true</code>). Default: <code>zeros32</code>.</li><li><code>init_state</code>: Initializer used when an external state is not provided. Default: <code>randn32</code>.</li><li><code>use_bias</code>: Whether the reservoir uses a bias term. Default: <code>false</code>.</li></ul><p>Delay expansion:</p><ul><li><code>num_delays</code>: Number of past reservoir states to include in the tapped-delay vector. The <code>DelayLayer</code> output has <code>(num_delays + 1) * res_dims</code> entries (current state plus <code>num_delays</code> past states). Default: <code>1</code>.</li><li><code>stride</code>: Delay stride in layer calls. The delay buffer is updated only when the internal clock is a multiple of <code>stride</code>. Default: <code>1</code>.</li></ul><p>Composition:</p><ul><li><code>state_modifiers</code>: A layer or collection of layers applied to the delayed reservoir features before the readout. These run <strong>after</strong> the internal <code>DelayLayer</code>. Accepts a single layer, an <code>AbstractVector</code>, or a <code>Tuple</code>. Default: empty <code>()</code>.</li><li><code>readout_activation</code>: Activation for the linear readout. Default: <code>identity</code>.</li></ul><p><strong>Inputs</strong></p><ul><li><code>x :: AbstractArray (in_dims, batch)</code></li></ul><p><strong>Returns</strong></p><ul><li>Output <code>y :: (out_dims, batch)</code>.</li><li>Updated layer state (NamedTuple).</li></ul><p><strong>Parameters</strong></p><ul><li><code>reservoir</code> — parameters of the internal <a href="../layers/#ReservoirComputing.ESNCell"><code>ESNCell</code></a>, including:<ul><li><code>input_matrix :: (res_dims × in_dims)</code> — <code>W_in</code></li><li><code>reservoir_matrix :: (res_dims × res_dims)</code> — <code>W_res</code></li><li><code>bias :: (res_dims,)</code> — present only if <code>use_bias=true</code></li></ul></li><li><code>states_modifiers</code> — a <code>Tuple</code> with parameters for:<ol><li>the internal <a href="../layers/#ReservoirComputing.DelayLayer"><code>DelayLayer</code></a>, and</li><li>any user-provided modifier layers (may be empty).</li></ol></li><li><code>readout</code> — parameters of <a href="../layers/#ReservoirComputing.LinearReadout"><code>LinearReadout</code></a>, typically:<ul><li><code>weight :: (out_dims × ((num_delays + 1) * res_dims))</code> — <code>W_out</code></li><li><code>bias :: (out_dims,)</code> — <code>b_out</code> (if the readout uses bias)</li></ul></li></ul><blockquote><p>Exact field names for modifiers/readout follow their respective layer definitions.</p></blockquote><p><strong>States</strong></p><ul><li><code>reservoir</code> — states for the internal <a href="../layers/#ReservoirComputing.ESNCell"><code>ESNCell</code></a> (e.g. <code>rng</code> used to sample initial hidden states).</li><li><code>states_modifiers</code> — a <code>Tuple</code> with states for the internal <code>DelayLayer</code> (its delay buffer and clock) and each additional modifier layer.</li><li><code>readout</code> — states for <a href="../layers/#ReservoirComputing.LinearReadout"><code>LinearReadout</code></a> (typically empty).</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/SciML/ReservoirComputing.jl/blob/dc3fb6dcb873ce0f7f264010f03ef2343047ee2e/src/models/esn_delay.jl#L1-L109">source</a></section></details></article><article><details class="docstring" open="true"><summary id="ReservoirComputing.HybridESN"><a class="docstring-binding" href="#ReservoirComputing.HybridESN"><code>ReservoirComputing.HybridESN</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">HybridESN(km, km_dims, in_dims, res_dims, out_dims, [activation];
    state_modifiers=(), readout_activation=identity,
    include_collect=true, kwargs...)</code></pre><p>Hybrid Echo State Network (<a href="../../references/#Pathak2018">Pathak <em>et al.</em>, 2018</a>).</p><p><code>HybridESN</code> composes:</p><ol><li>a knowledge model <code>km</code> producing auxiliary features from the input,</li><li>a stateful <a href="../layers/#ReservoirComputing.ESNCell"><code>ESNCell</code></a> that receives the concatenated input <code>[km(x(t)); x(t)]</code>,</li><li>zero or more <code>state_modifiers</code> applied to the reservoir state, and</li><li>a <a href="../layers/#ReservoirComputing.LinearReadout"><code>LinearReadout</code></a> mapping the combined features <code>[km(x(t)); h*(t)]</code> to the output.</li></ol><p><strong>Arguments</strong></p><ul><li><code>km</code>: Knowledge model applied to the input (e.g. a physical model, neural   submodule, or differentiable function). May be a <code>WrappedFunction</code> or any   callable layer.</li><li><code>km_dims</code>: Output dimension of the knowledge model <code>km</code>.</li><li><code>in_dims</code>: Input dimension.</li><li><code>res_dims</code>: Reservoir (hidden state) dimension.</li><li><code>out_dims</code>: Output dimension.</li><li><code>activation</code>: Reservoir activation (for <a href="../layers/#ReservoirComputing.ESNCell"><code>ESNCell</code></a>). Default: <code>tanh</code>.</li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>leak_coefficient</code>: Leak rate <code>α ∈ (0,1]</code>. Default: <code>1.0</code>.</li><li><code>init_reservoir</code>: Initializer for <code>W_res</code>. Default: <a href="../inits/rand_sparse/#rand_sparse"><code>rand_sparse</code></a>.</li><li><code>init_input</code>: Initializer for <code>W_in</code>. Default: <a href="../inits/scaled_rand/#scaled_rand"><code>scaled_rand</code></a>.</li><li><code>init_bias</code>: Initializer for reservoir bias (used if <code>use_bias=true</code>).   Default: <code>zeros32</code>.</li><li><code>init_state</code>: Initializer used when an external state is not provided.   Default: <code>randn32</code>.</li><li><code>use_bias</code>: Whether the reservoir uses a bias term. Default: <code>false</code>.</li><li><code>state_modifiers</code>: A layer or collection of layers applied to the reservoir   state before the readout. Accepts a single layer, an <code>AbstractVector</code>, or a   <code>Tuple</code>. Default: empty <code>()</code>.</li><li><code>readout_activation</code>: Activation for the linear readout. Default: <code>identity</code>.</li><li><code>include_collect</code>: Whether the readout should include collection mode.   Default: <code>true</code>.</li></ul><p><strong>Inputs</strong></p><ul><li><code>x :: AbstractArray (in_dims, batch)</code></li></ul><p><strong>Returns</strong></p><ul><li>Output <code>y :: (out_dims, batch)</code>.</li><li>Updated layer state (NamedTuple).</li></ul><p><strong>Parameters</strong></p><ul><li><code>knowledge_model</code> — parameters of the knowledge model <code>km</code>.</li><li><code>reservoir</code> — parameters of the internal <a href="../layers/#ReservoirComputing.ESNCell"><code>ESNCell</code></a>, including:<ul><li><code>input_matrix :: (res_dims × (in_dims + km_dims))</code> — <code>W_in</code></li><li><code>reservoir_matrix :: (res_dims × res_dims)</code> — <code>W_res</code></li><li><code>bias :: (res_dims,)</code> — present only if <code>use_bias=true</code></li></ul></li><li><code>states_modifiers</code> — a <code>Tuple</code> with parameters for each modifier layer (may be empty).</li><li><code>readout</code> — parameters of <a href="../layers/#ReservoirComputing.LinearReadout"><code>LinearReadout</code></a>, typically:<ul><li><code>weight :: (out_dims × (res_dims + km_dims))</code> — <code>W_out</code></li><li><code>bias :: (out_dims,)</code> — <code>b_out</code> (if the readout uses bias)</li></ul></li></ul><blockquote><p>Exact field names for modifiers/readout follow their respective layer definitions.</p></blockquote><p><strong>States</strong></p><p>Created by <code>initialstates(rng, hesn)</code>:</p><ul><li><code>knowledge_model</code> — states for the internal knowledge model.</li><li><code>reservoir</code> — states for the internal <a href="../layers/#ReservoirComputing.ESNCell"><code>ESNCell</code></a>.</li><li><code>states_modifiers</code> — a <code>Tuple</code> with states for each modifier layer.</li><li><code>readout</code> — states for <a href="../layers/#ReservoirComputing.LinearReadout"><code>LinearReadout</code></a>.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/SciML/ReservoirComputing.jl/blob/dc3fb6dcb873ce0f7f264010f03ef2343047ee2e/src/models/esn_hybrid.jl#L1-L76">source</a></section></details></article><h2 id="Next-generation-reservoir-computing"><a class="docs-heading-anchor" href="#Next-generation-reservoir-computing">Next generation reservoir computing</a><a id="Next-generation-reservoir-computing-1"></a><a class="docs-heading-anchor-permalink" href="#Next-generation-reservoir-computing" title="Permalink"></a></h2><article><details class="docstring" open="true"><summary id="ReservoirComputing.NGRC"><a class="docstring-binding" href="#ReservoirComputing.NGRC"><code>ReservoirComputing.NGRC</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">NGRC(in_dims, out_dims; num_delays=2, stride=1,
     features=(), include_input=true, init_delay=zeros32,
     readout_activation=identity, state_modifiers=(),
     ro_dims=nothing)</code></pre><p>Next Generation Reservoir Computing (<a href="../../references/#Gauthier2021">Gauthier <em>et al.</em>, 2021</a>).</p><p><code>NGRC</code> composes:</p><ol><li>a <a href="../layers/#ReservoirComputing.DelayLayer"><code>DelayLayer</code></a> applied directly to the input, producing a vector containing the current input and a fixed number of past inputs,</li><li>a <a href="../layers/#ReservoirComputing.NonlinearFeaturesLayer"><code>NonlinearFeaturesLayer</code></a> that applies user-provided functions to this delayed vector and concatenates the results, and</li><li>a <a href="../layers/#ReservoirComputing.LinearReadout"><code>LinearReadout</code></a> mapping the resulting feature vector to outputs.</li></ol><p><strong>Arguments</strong></p><ul><li><code>in_dims</code>: Input dimension.</li><li><code>out_dims</code>: Output dimension.</li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>num_delays</code>: Number of past input vectors to include. The internal <a href="../layers/#ReservoirComputing.DelayLayer"><code>DelayLayer</code></a> outputs a vector of length <code>(num_delays + 1) * in_dims</code> (current input plus <code>num_delays</code> past inputs). Default: <code>2</code>.</li><li><code>stride</code>: Delay stride in layer calls. The delay buffer is updated only when the internal clock is a multiple of <code>stride</code>. Default: <code>1</code>.</li><li><code>init_delay</code>: Initializer (or tuple of initializers) for the delay history, passed to <a href="../layers/#ReservoirComputing.DelayLayer"><code>DelayLayer</code></a>. Each initializer function is called as <code>init(rng, in_dims, 1)</code> to fill one delay column. Default: <code>zeros32</code>.</li><li><code>features</code>: A function or tuple of functions <code>(f₁, f₂, ...)</code> used by <a href="../layers/#ReservoirComputing.NonlinearFeaturesLayer"><code>NonlinearFeaturesLayer</code></a>. Each <code>f</code> is called as <code>f(x)</code> where <code>x</code> is the delayed input vector. By default it is assumed that each <code>f</code> returns a vector of the same length as <code>x</code> when <code>ro_dims</code> is not provided. Default: empty <code>()</code>.</li><li><code>include_input</code>: Whether to include the raw delayed input vector itself as the first block of the feature vector (passed to <a href="../layers/#ReservoirComputing.NonlinearFeaturesLayer"><code>NonlinearFeaturesLayer</code></a>). Default: <code>true</code>.</li><li><code>state_modifiers</code>: Extra layers applied after the <code>NonlinearFeaturesLayer</code> and before the readout. Accepts a single layer, an <code>AbstractVector</code>, or a <code>Tuple</code>. Default: empty <code>()</code>.</li><li><code>readout_activation</code>: Activation for the linear readout. Default: <code>identity</code>.</li><li><code>ro_dims</code>: Input dimension of the readout. If <code>nothing</code> (default), it is <em>estimated</em> under the assumption that each feature function returns a vector with the same length as the delayed input. In that case, <code>ro_dims ≈ (num_delays + 1) * in_dims * n_blocks</code>, where <code>n_blocks</code> is the number of concatenated vectors (original delayed input if <code>include_input=true</code> plus one block per feature function). If your feature functions change the length (e.g. constant features, higher-order polynomial expansions with cross terms), you should pass <code>ro_dims</code> explicitly.</li></ul><p><strong>Inputs</strong></p><ul><li><code>x :: AbstractArray (in_dims, batch)</code> or <code>(in_dims,)</code></li></ul><p><strong>Returns</strong></p><ul><li>Output <code>y :: (out_dims, batch)</code> (or <code>(out_dims,)</code> for vector input).</li><li>Updated layer state (NamedTuple).</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/SciML/ReservoirComputing.jl/blob/dc3fb6dcb873ce0f7f264010f03ef2343047ee2e/src/models/ngrc.jl#L1-L62">source</a></section></details></article><article><details class="docstring" open="true"><summary id="ReservoirComputing.polynomial_monomials"><a class="docstring-binding" href="#ReservoirComputing.polynomial_monomials"><code>ReservoirComputing.polynomial_monomials</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">polynomial_monomials(input_vector;
    degrees = 1:2)</code></pre><p>Generate all unordered polynomial monomials of the entries in <code>input_vector</code> for the given set of degrees.</p><p>For each <code>d</code> in <code>degrees</code>, this function produces all degree-<code>d</code> monomials of the form</p><ul><li>degree 1: <code>x₁, x₂, …</code></li><li>degree 2: <code>x₁², x₁x₂, x₁x₃, x₂², …</code></li><li>degree 3: <code>x₁³, x₁²x₂, x₁x₂x₃, x₂³, …</code></li></ul><p>where combinations are taken with repetition and in non-decreasing index order. This means that, for example, <code>x₁x₂</code> and <code>x₂x₁</code> are represented only once.</p><p>The returned vector is a flat list of all such products, in a deterministic order determined by the recursive enumeration.</p><p><strong>Arguments</strong></p><ul><li><code>input_vector</code> Input vector whose entries define the variables used to build monomials.</li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>degrees</code>: An iterable of positive integers specifying which monomial degrees to generate. Each degree less than <code>1</code> is skipped. Default: <code>1:2</code>.</li></ul><p><strong>Returns</strong></p><ul><li><code>output_monomials</code> a vector of the same type as <code>input_vector</code> containing all generated monomials, concatenated across the requested degrees, in a deterministic order.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/SciML/ReservoirComputing.jl/blob/dc3fb6dcb873ce0f7f264010f03ef2343047ee2e/src/models/ngrc.jl#L119-L155">source</a></section></details></article><h3 id="Utilities"><a class="docs-heading-anchor" href="#Utilities">Utilities</a><a id="Utilities-1"></a><a class="docs-heading-anchor-permalink" href="#Utilities" title="Permalink"></a></h3><article><details class="docstring" open="true"><summary id="ReservoirComputing.resetcarry!"><a class="docstring-binding" href="#ReservoirComputing.resetcarry!"><code>ReservoirComputing.resetcarry!</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">resetcarry!(rng, rc::ReservoirComputer, st; init_carry=nothing)
resetcarry!(rng, rc::ReservoirComputer, ps, st; init_carry=nothing)</code></pre><p>Reset (or set) the hidden-state carry of a model in the echo state network family.</p><p>If an existing carry is present in <code>st.cell.carry</code>, its leading dimension is used to infer the state size. Otherwise the reservoir output size is taken from <code>rc.reservoir.cell.out_dims</code>. When <code>init_carry=nothing</code>, the carry is cleared; the initializer from the struct construction will then be used. When a function is provided, it is called to create a new initial hidden state.</p><p><strong>Arguments</strong></p><ul><li><code>rng</code>: Random number generator (used if a new carry is sampled/created).</li><li><code>rc</code>: A reservoir computing network model.</li><li><code>st</code>: Current model states.</li><li><code>ps</code>: Optional model parameters. Returned unchanged.</li></ul><p><strong>Keyword arguments</strong></p><ul><li><code>init_carry</code>: Controls the initialization of the new carry.<ul><li><code>nothing</code> (default): remove/clear the carry (forces the cell to reinitialize from its own <code>init_state</code> on next use).</li><li><code>f</code>: a function following standard from <a href="https://lux.csail.mit.edu/stable/api/Building_Blocks/WeightInitializers">WeightInitializers.jl</a></li></ul></li></ul><p><strong>Returns</strong></p><ul><li><code>resetcarry!(rng, rc, st; ...) -&gt; st′</code>: Updated states with <code>st′.cell.carry</code> set to <code>nothing</code> or <code>(h0,)</code>.</li><li><code>resetcarry!(rng, rc, ps, st; ...) -&gt; (ps, st′)</code>: Same as above, but also returns the unchanged <code>ps</code> for convenience.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/SciML/ReservoirComputing.jl/blob/dc3fb6dcb873ce0f7f264010f03ef2343047ee2e/src/reservoircomputer.jl#L149-L183">source</a></section></details></article><h2 id="Reservoir-Computing-with-Cellular-Automata"><a class="docs-heading-anchor" href="#Reservoir-Computing-with-Cellular-Automata">Reservoir Computing with Cellular Automata</a><a id="Reservoir-Computing-with-Cellular-Automata-1"></a><a class="docs-heading-anchor-permalink" href="#Reservoir-Computing-with-Cellular-Automata" title="Permalink"></a></h2><article><details class="docstring" open="true"><summary id="ReservoirComputing.RECA"><a class="docstring-binding" href="#ReservoirComputing.RECA"><code>ReservoirComputing.RECA</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">RECA(in_dims, out_dims, automaton;
    input_encoding=RandomMapping(),
    generations=8, state_modifiers=(),
    readout_activation=identity)</code></pre><p>Construct a cellular–automata reservoir model.</p><p>At each time step the input vector is randomly embedded into a Cellular Automaton (CA) lattice, the CA is evolved for <code>generations</code> steps, and the flattened evolution (excluding the initial row) is used as the reservoir state. A linear <a href="../layers/#ReservoirComputing.LinearReadout"><code>LinearReadout</code></a> maps these features to <code>out_dims</code>.</p><div class="admonition is-info" id="Note-1e88af8f9ecaf524"><header class="admonition-header">Note<a class="admonition-anchor" href="#Note-1e88af8f9ecaf524" title="Permalink"></a></header><div class="admonition-body"><p>This constructor is only available when the <code>CellularAutomata.jl</code> package is loaded.</p></div></div><p><strong>Arguments</strong></p><ul><li><code>in_dims</code>: Number of input features (rows of training data).</li><li><code>out_dims</code>: Number of output features (rows of target data).</li><li><code>automaton</code>: A CA rule/object from <code>CellularAutomata.jl</code> (e.g. <code>DCA(90)</code>, <code>DCA(30)</code>, …).</li></ul><p><strong>Keyword Arguments</strong></p><ul><li><code>input_encoding</code>: Random embedding spec with fields <code>permutations</code> and <code>expansion_size</code>. Default is <code>RandomMapping()</code>.</li><li><code>generations</code>: Number of CA generations to evolve per time step. Default is 8.</li><li><code>state_modifiers</code>: Optional tuple/vector of additional layers applied after the CA cell and before the readout (e.g., <code>NLAT2()</code>, <code>Pad(1.0)</code>, custom transforms, etc.). Functions are wrapped automatically. Default is none.</li><li><code>readout_activation</code>: Activation applied by the readout Default is <code>identity</code>.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/SciML/ReservoirComputing.jl/blob/dc3fb6dcb873ce0f7f264010f03ef2343047ee2e/src/extensions/reca.jl#L150-L187">source</a></section></details></article><p>The input encodings are the equivalent of the input matrices of the ESNs. These are the available encodings:</p><article><details class="docstring" open="true"><summary id="ReservoirComputing.RandomMapping"><a class="docstring-binding" href="#ReservoirComputing.RandomMapping"><code>ReservoirComputing.RandomMapping</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">RandomMapping(permutations, expansion_size)
RandomMapping(permutations; expansion_size=40)
RandomMapping(;permutations=8, expansion_size=40)</code></pre><p>Specify the <strong>random input embedding</strong> used by the Cellular Automata reservoir. Each time step, the input vector of length <code>in_dims</code> is randomly placed into a larger 1D lattice of length <code>expansion_size</code>, and this is repeated for <code>permutations</code> independent lattices (blocks). The concatenation of these blocks forms the CA initial condition of length: <code>ca_size = expansion_size * permutations</code>. The detail of this implementation can be found in (<a href="../../references/#Nichele2017">Nichele and Molund, 2017</a>).</p><p><strong>Arguments</strong></p><ul><li><code>permutations</code>: number of independent random maps (blocks). Larger values increase feature diversity and <code>ca_size</code> proportionally.</li><li><code>expansion_size</code>: width of each block (the size of a single CA lattice). Larger values increase the spatial resolution and both <code>ca_size</code> and <code>states_size</code>.</li></ul><p><strong>Usage</strong></p><p>This is a <strong>configuration object</strong>; it does not perform the mapping by itself. Create the concrete tables with <code>create_encoding</code> and pass them to <a href="../layers/#ReservoirComputing.RECACell"><code>RECACell</code></a>:</p><pre><code class="language-julia hljs">using ReservoirComputing, CellularAutomata, Random

in_dims = 4
generations = 8
mapping = RandomMapping(permutations = 8, expansion_size = 40)

enc = ReservoirComputing.create_encoding(mapping, in_dims, generations)  # → RandomMaps
cell = RECACell(DCA(90), enc)

rc = ReservoirChain(
    StatefulLayer(cell),
    LinearReadout(enc.states_size =&gt; in_dims; include_collect = true)
)</code></pre><p>Or let <a href="#ReservoirComputing.RECA"><code>RECA</code></a> do this for you:</p><pre><code class="language-julia hljs">rc = RECA(in_dims = 4, out_dims = 4, DCA(90);
    input_encoding = RandomMapping(permutations = 8, expansion_size = 40),
    generations = 8)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/SciML/ReservoirComputing.jl/blob/dc3fb6dcb873ce0f7f264010f03ef2343047ee2e/src/extensions/reca.jl#L4-L53">source</a></section></details></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../layers/">« Layers</a><a class="docs-footer-nextpage" href="../utils/">Utilities »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.16.1 on <span class="colophon-date" title="Wednesday 21 January 2026 13:32">Wednesday 21 January 2026</span>. Using Julia version 1.12.4.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
