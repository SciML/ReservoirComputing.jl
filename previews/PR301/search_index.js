var documenterSearchIndex = {"docs":
[{"location":"api/inits/low_connectivity/#low_connectivity","page":"low_connectivity","title":"low_connectivity","text":"","category":"section"},{"location":"api/inits/low_connectivity/#ReservoirComputing.low_connectivity","page":"low_connectivity","title":"ReservoirComputing.low_connectivity","text":"low_connectivity([rng], [T], dims...;\n                 return_sparse = false, connected=false,\n                 in_degree = 1, radius = 1.0, cut_cycle = false)\n\nConstruct an internal reservoir connectivity matrix with low connectivity.\n\nThis function creates a square reservoir matrix with the specified in-degree for each node (Griffith et al., 2019). When in_degree is 1, the function can enforce a fully connected cycle if connected is true; otherwise, it generates a random connectivity pattern.\n\nArguments\n\nrng: Random number generator. Default is Utils.default_rng() from WeightInitializers.\nT: Type of the elements in the reservoir matrix. Default is Float32.\ndims: Dimensions of the reservoir matrix.\n\nKeyword Arguments\n\nreturn_sparse: If true, the function returns the reservoir matrix as a sparse matrix. Default is false.\nconnected: For in_degree == 1, if true a connected cycle is enforced. Default is false.\nin_degree: The number of incoming connections per node. Must not exceed the number of nodes. Default is 1.\nradius: The desired spectral radius of the reservoir. Defaults to 1.0.\ncut_cycle: If true, removes one edge from the cycle to cut it. Default is false.\n\n\n\n\n\n","category":"function"},{"location":"api/inits/low_connectivity/#References","page":"low_connectivity","title":"References","text":"","category":"section"},{"location":"api/inits/low_connectivity/","page":"low_connectivity","title":"low_connectivity","text":"Griffith, A.; Pomerance, A. and Gauthier, D. J. (2019). Forecasting chaotic systems with very low connectivity reservoir computers. Chaos: An Interdisciplinary Journal of Nonlinear Science 29.\n\n\n\n","category":"page"},{"location":"tutorials/reca/#Reservoir-Computing-using-Cellular-Automata","page":"Reservoir Computing with Cellular Automata","title":"Reservoir Computing using Cellular Automata","text":"","category":"section"},{"location":"tutorials/reca/","page":"Reservoir Computing with Cellular Automata","title":"Reservoir Computing with Cellular Automata","text":"We showcase how to use reservoir computing models with cellular automata (ReCA) with ReservoirComputing.jl. While introduced in (Yilmaz, 2014) (Margem and Yilmaz, 2017), the implementation in this package follows (Nichele and Molund, 2017). To showcase ReCA models we show how to solve the 5 bit memory task.","category":"page"},{"location":"tutorials/reca/#5-bit-memory-task","page":"Reservoir Computing with Cellular Automata","title":"5 bit memory task","text":"","category":"section"},{"location":"tutorials/reca/","page":"Reservoir Computing with Cellular Automata","title":"Reservoir Computing with Cellular Automata","text":"We read the data can be read as follows:","category":"page"},{"location":"tutorials/reca/","page":"Reservoir Computing with Cellular Automata","title":"Reservoir Computing with Cellular Automata","text":"using DelimitedFiles\n\ninput = readdlm(\"./5bitinput.txt\", ',', Float64)\noutput = readdlm(\"./5bitoutput.txt\", ',', Float64)","category":"page"},{"location":"tutorials/reca/","page":"Reservoir Computing with Cellular Automata","title":"Reservoir Computing with Cellular Automata","text":"To use a ReCA model, it is necessary to define the rule one intends to use. To do so, ReservoirComputing.jl leverages CellularAutomata.jl that needs to be called as well to define the RECA struct:","category":"page"},{"location":"tutorials/reca/","page":"Reservoir Computing with Cellular Automata","title":"Reservoir Computing with Cellular Automata","text":"using ReservoirComputing, CellularAutomata, Random\nRandom.seed!(42)\nrng = MersenneTwister(17)\n\nca = DCA(90)","category":"page"},{"location":"tutorials/reca/","page":"Reservoir Computing with Cellular Automata","title":"Reservoir Computing with Cellular Automata","text":"To define the ReCA model, it suffices to call:","category":"page"},{"location":"tutorials/reca/","page":"Reservoir Computing with Cellular Automata","title":"Reservoir Computing with Cellular Automata","text":"reca = RECA(4, 4, DCA(90);\n           generations=16,\n           input_encoding=RandomMapping(16, 40))\nps, st = setup(rng, reca)","category":"page"},{"location":"tutorials/reca/","page":"Reservoir Computing with Cellular Automata","title":"Reservoir Computing with Cellular Automata","text":"After this, the training can be performed with the chosen method.","category":"page"},{"location":"tutorials/reca/","page":"Reservoir Computing with Cellular Automata","title":"Reservoir Computing with Cellular Automata","text":"ps, st = train!(reca, input, output, ps, st, StandardRidge(0.00001))","category":"page"},{"location":"tutorials/reca/","page":"Reservoir Computing with Cellular Automata","title":"Reservoir Computing with Cellular Automata","text":"We are going to test the recall ability of the model, feeding the input data and investigating whether the predicted output equals the output data.","category":"page"},{"location":"tutorials/reca/","page":"Reservoir Computing with Cellular Automata","title":"Reservoir Computing with Cellular Automata","text":"st0 = resetcarry!(rng, reca, st) #reset the first ca state\npred_out, st = predict(reca, input, ps, st0)\nfinal_pred = convert(AbstractArray{Float32}, pred_out .> 0.5)\nfinal_pred == output","category":"page"},{"location":"api/inits/backward_connection!/#backward_connection!","page":"backward_connection!","title":"backward_connection!","text":"","category":"section"},{"location":"api/inits/backward_connection!/#ReservoirComputing.backward_connection!","page":"backward_connection!","title":"ReservoirComputing.backward_connection!","text":"backward_connection!([rng], reservoir_matrix, weight, shift;\n    sampling_type=:no_sample, irrational=pi, start=1,\n    p=0.5)\n\nAdds a backward connection in the reservoir_matrix, with given shift and weight. The weight can be a single number or an array.\n\nArguments\n\nrng: Random number generator. Default is Utils.default_rng() from WeightInitializers.\nreservoir_matrix: matrix to be changed.\nweight: weight to add as a backward connection. Can be either a single number or an array.\nshift: How far the backward connection will be from the diagonal.\n\nKeyword arguments\n\nsampling_type: Sampling that decides the distribution of weight negative numbers. If set to :no_sample the sign is unchanged. If set to :bernoulli_sample! then each weight can be positive with a probability set by positive_prob. If set to :irrational_sample! the weight is negative if the decimal number of the irrational number chosen is odd. If set to :regular_sample!, each weight will be assigned a negative sign after the chosen strides. strides can be a single number or an array. Default is :no_sample.\npositive_prob: probability of the weight being positive when sampling_type is set to :bernoulli_sample!. Default is 0.5.\nirrational: Irrational number whose decimals decide the sign of weight. Default is pi.\nstart: Which place after the decimal point the counting starts for the irrational sign counting. Default is 1.\nstrides: number of strides for assigning negative value to a weight. It can be an integer or an array. Default is 2.\n\nExamples\n\njulia> matrix = zeros(Float32, 5, 5)\n5×5 Matrix{Float32}:\n 0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n\njulia> backward_connection!(matrix, 3.0, 1)\n5×5 Matrix{Float32}:\n 0.0  3.0  0.0  0.0  0.0\n 0.0  0.0  3.0  0.0  0.0\n 0.0  0.0  0.0  3.0  0.0\n 0.0  0.0  0.0  0.0  3.0\n 0.0  0.0  0.0  0.0  0.0\n\njulia> backward_connection!(matrix, 3.0, 1; sampling_type = :bernoulli_sample!)\n5×5 Matrix{Float32}:\n 0.0  3.0   0.0  0.0   0.0\n 0.0  0.0  -3.0  0.0   0.0\n 0.0  0.0   0.0  3.0   0.0\n 0.0  0.0   0.0  0.0  -3.0\n 0.0  0.0   0.0  0.0   0.0\n\n\n\n\n\n","category":"function"},{"location":"tutorials/scratch/#Building-a-model-from-scratch","page":"Building a model from scratch","title":"Building a model from scratch","text":"","category":"section"},{"location":"tutorials/scratch/","page":"Building a model from scratch","title":"Building a model from scratch","text":"ReservoirComputing.jl provides utilities to build reservoir reservoir computing models from scratch. In this tutorial we are going to build an echo state network (ESN) and showcase how this custom implementation is equivalent to the provided model (minus some comfort utilities)","category":"page"},{"location":"tutorials/scratch/#Using-provided-layers:-ReservoirChain,-ESNCell,-and-LinearReadout","page":"Building a model from scratch","title":"Using provided layers: ReservoirChain, ESNCell, and LinearReadout","text":"","category":"section"},{"location":"tutorials/scratch/","page":"Building a model from scratch","title":"Building a model from scratch","text":"The library provides a ReservoirChain, which is virtually equivalent to Lux's Chain. Passing layers, or functions, to the chain will concatenate them, and will allow the flow of the input data through the model.","category":"page"},{"location":"tutorials/scratch/","page":"Building a model from scratch","title":"Building a model from scratch","text":"To build an ESN we also need a ESNCell to provide the ESN forward pass. However, the cell is stateless, so to keep the memoruy of the input we need to wrap it in a StatefulLayer, which saves the internal state in the model states st and feeds it to the cell in the next step.","category":"page"},{"location":"tutorials/scratch/","page":"Building a model from scratch","title":"Building a model from scratch","text":"Finally, we need the trainable readout for the reservoir computing. The library provides LinearReadout, a dense layer the weights of which will be trained using linear regression.","category":"page"},{"location":"tutorials/scratch/","page":"Building a model from scratch","title":"Building a model from scratch","text":"Putting it all together we get the following","category":"page"},{"location":"tutorials/scratch/","page":"Building a model from scratch","title":"Building a model from scratch","text":"using ReservoirComputing\n\nesn_scratch = ReservoirChain(\n    StatefulLayer(\n        ESNCell(3=>50)\n    ),\n    LinearReadout(50=>1)\n)","category":"page"},{"location":"tutorials/scratch/","page":"Building a model from scratch","title":"Building a model from scratch","text":"Now, this implementation, elements naming aside, is completely equivalent to the following","category":"page"},{"location":"tutorials/scratch/","page":"Building a model from scratch","title":"Building a model from scratch","text":"esn = ESN(3, 50, 1)","category":"page"},{"location":"tutorials/scratch/","page":"Building a model from scratch","title":"Building a model from scratch","text":"and we can check it initializing the two models and comparing, for instance, the weights of the input layer:","category":"page"},{"location":"tutorials/scratch/","page":"Building a model from scratch","title":"Building a model from scratch","text":"using Random\nRandom.seed!(43)\n\nrng = MersenneTwister(17)\nps_s, st_s = setup(rng, esn_scratch)\n\nrng = MersenneTwister(17)\nps, st = setup(rng, esn)\n\nps_s.layer_1.input_matrix == ps.reservoir.input_matrix","category":"page"},{"location":"tutorials/scratch/","page":"Building a model from scratch","title":"Building a model from scratch","text":"Both the models can be trained using train!, and predictions can be obtained with predict. The internal states collected for linear regression are computed by traversing the ReservoirChain, and stopping right before the LinearReadout.","category":"page"},{"location":"tutorials/scratch/#Manual-state-collection-with-Collect","page":"Building a model from scratch","title":"Manual state collection with Collect","text":"","category":"section"},{"location":"tutorials/scratch/","page":"Building a model from scratch","title":"Building a model from scratch","text":"For more complicated models usually you would want to control when the state collection happens. In a ReservoirChain, the collection of states is controlled by the layer Collect. The role of this layer is to tell the collectstates function where to stop for state collection. All the readout layers have a include_collect=true keyword, which forces a Collect layer bvefore the readout. The model we wrote before can be written as","category":"page"},{"location":"tutorials/scratch/","page":"Building a model from scratch","title":"Building a model from scratch","text":"esn_scratch = ReservoirChain(\n    StatefulLayer(\n        ESNCell(3=>50)\n    ),\n    Collect(),\n    LinearReadout(50=>1; include_collect=false)\n)","category":"page"},{"location":"tutorials/scratch/","page":"Building a model from scratch","title":"Building a model from scratch","text":"to make the collection explicit. This layer is useful in case one needs to build more complicated models such as a DeepESN. We can build a deep model in multiple ways:","category":"page"},{"location":"tutorials/scratch/","page":"Building a model from scratch","title":"Building a model from scratch","text":"deepesn_scratch = ReservoirChain(\n    StatefulLayer(\n        ESNCell(3=>50)\n    ),\n    StatefulLayer(\n        ESNCell(50=>50)\n    ),\n    StatefulLayer(\n        ESNCell(50=>50)\n    ),\n    Collect(),\n    LinearReadout(50=>1; include_collect=false)\n)","category":"page"},{"location":"tutorials/scratch/","page":"Building a model from scratch","title":"Building a model from scratch","text":"this first approach is the one provided by default in the library through DeepESN. However, you could want the state collection to be after each cell","category":"page"},{"location":"tutorials/scratch/","page":"Building a model from scratch","title":"Building a model from scratch","text":"deepesn_scratch = ReservoirChain(\n    StatefulLayer(\n        ESNCell(3=>50)\n    ),\n    Collect(),\n    StatefulLayer(\n        ESNCell(50=>50)\n    ),\n    Collect(),\n    StatefulLayer(\n        ESNCell(50=>50)\n    ),\n    Collect(),\n    LinearReadout(50=>1; include_collect=false)\n)","category":"page"},{"location":"tutorials/scratch/","page":"Building a model from scratch","title":"Building a model from scratch","text":"With this approach, the resulting state will be a concatenation of the states at each Collect point. So the resulting states for this architecture will be vector of size 150.","category":"page"},{"location":"tutorials/scratch/","page":"Building a model from scratch","title":"Building a model from scratch","text":"ps, st = setup(rng, deepesn_scratch)\nstates, st = collectstates(deepesn_scratch, rand(3, 300), ps, st)\nsize(states[:,1])","category":"page"},{"location":"tutorials/scratch/","page":"Building a model from scratch","title":"Building a model from scratch","text":"This allows for even more complex constructions, where the state collection follows specific patterns","category":"page"},{"location":"tutorials/scratch/","page":"Building a model from scratch","title":"Building a model from scratch","text":"deepesn_scratch = ReservoirChain(\n    StatefulLayer(\n        ESNCell(3=>50)\n    ),\n    StatefulLayer(\n        ESNCell(50=>50)\n    ),\n    Collect(),\n    StatefulLayer(\n        ESNCell(50=>50)\n    ),\n    Collect(),\n    LinearReadout(50=>1; include_collect=false)\n)","category":"page"},{"location":"tutorials/scratch/","page":"Building a model from scratch","title":"Building a model from scratch","text":"Here, for instance, we have a Collect after the first two cells and then one at the very end. You can see how the size of the states is now 100:","category":"page"},{"location":"tutorials/scratch/","page":"Building a model from scratch","title":"Building a model from scratch","text":"ps, st = setup(rng, deepesn_scratch)\nstates, st = collectstates(deepesn_scratch, rand(3, 300), ps, st)\nsize(states[:,1])","category":"page"},{"location":"tutorials/scratch/","page":"Building a model from scratch","title":"Building a model from scratch","text":"Similar approaches could be leveraged, for instance, when the data show multiscale dynamics that require specific modeling approaches.","category":"page"},{"location":"api/inits/delay_line!/#delay_line!","page":"delay_line!","title":"delay_line!","text":"","category":"section"},{"location":"api/inits/delay_line!/#ReservoirComputing.delay_line!","page":"delay_line!","title":"ReservoirComputing.delay_line!","text":"delay_line!([rng], reservoir_matrix, weight, shift;\n    sampling_type=:no_sample, irrational=pi, start=1,\n    p=0.5)\n\nAdds a delay line in the reservoir_matrix, with given shift and weight. The weight can be a single number or an array.\n\nArguments\n\nrng: Random number generator. Default is Utils.default_rng() from WeightInitializers.\nreservoir_matrix: matrix to be changed.\nweight: weight to add as a delay line. Can be either a single number or an array.\nshift: How far the delay line will be from the diagonal.\n\nKeyword arguments\n\nsampling_type: Sampling that decides the distribution of weight negative numbers. If set to :no_sample the sign is unchanged. If set to :bernoulli_sample! then each weight can be positive with a probability set by positive_prob. If set to :irrational_sample! the weight is negative if the decimal number of the irrational number chosen is odd. If set to :regular_sample!, each weight will be assigned a negative sign after the chosen strides. strides can be a single number or an array. Default is :no_sample.\npositive_prob: probability of the weight being positive when sampling_type is set to :bernoulli_sample!. Default is 0.5.\nirrational: Irrational number whose decimals decide the sign of weight. Default is pi.\nstart: Which place after the decimal point the counting starts for the irrational sign counting. Default is 1.\nstrides: number of strides for assigning negative value to a weight. It can be an integer or an array. Default is 2.\n\nExamples\n\njulia> matrix = zeros(Float32, 5, 5)\n5×5 Matrix{Float32}:\n 0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n\njulia> delay_line!(matrix, 5.0, 2)\n5×5 Matrix{Float32}:\n 0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n 5.0  0.0  0.0  0.0  0.0\n 0.0  5.0  0.0  0.0  0.0\n 0.0  0.0  5.0  0.0  0.0\n\n julia> delay_line!(matrix, 5.0, 2; sampling_type=:bernoulli_sample!)\n5×5 Matrix{Float32}:\n 0.0   0.0  0.0  0.0  0.0\n 0.0   0.0  0.0  0.0  0.0\n 5.0   0.0  0.0  0.0  0.0\n 0.0  -5.0  0.0  0.0  0.0\n 0.0   0.0  5.0  0.0  0.0\n\n\n\n\n\n","category":"function"},{"location":"tutorials/deep_esn/#Deep-Echo-State-Networks","page":"Deep Echo State Networks","title":"Deep Echo State Networks","text":"","category":"section"},{"location":"tutorials/deep_esn/","page":"Deep Echo State Networks","title":"Deep Echo State Networks","text":"In this example we showcase how to build a deep echo state network (DeepESN) following the work of (Gallicchio and Micheli, 2017). The DeepESN stacks reservoirs on top of each other, feeding the output from one into the next. In the version implemented in ReservoirComputing.jl the final state is the state used for training.","category":"page"},{"location":"tutorials/deep_esn/#Lorenz-Example","page":"Deep Echo State Networks","title":"Lorenz Example","text":"","category":"section"},{"location":"tutorials/deep_esn/","page":"Deep Echo State Networks","title":"Deep Echo State Networks","text":"We are going to reuse the Lorenz data used in the Lorenz System Forecasting example.","category":"page"},{"location":"tutorials/deep_esn/","page":"Deep Echo State Networks","title":"Deep Echo State Networks","text":"using OrdinaryDiffEq\n\n#define lorenz system\nfunction lorenz!(du, u, p, t)\n    du[1] = 10.0 * (u[2] - u[1])\n    du[2] = u[1] * (28.0 - u[3]) - u[2]\n    du[3] = u[1] * u[2] - (8 / 3) * u[3]\nend\n\n#solve and take data\nprob = ODEProblem(lorenz!, [1.0, 0.0, 0.0], (0.0, 200.0))\ndata = solve(prob, ABM54(); dt=0.02)\ndata = reduce(hcat, data.u)\n\n#determine shift length, training length and prediction length\nshift = 300\ntrain_len = 5000\npredict_len = 1250\n\n#split the data accordingly\ninput_data = data[:, shift:(shift + train_len - 1)]\ntarget_data = data[:, (shift + 1):(shift + train_len)]\ntest_data = data[:, (shift + train_len + 1):(shift + train_len + predict_len)]","category":"page"},{"location":"tutorials/deep_esn/","page":"Deep Echo State Networks","title":"Deep Echo State Networks","text":"The call for the DeepESN works similarly to the ESN. The only difference is that the reservoir (and corresponding kwargs) can be fed as an array.","category":"page"},{"location":"tutorials/deep_esn/","page":"Deep Echo State Networks","title":"Deep Echo State Networks","text":"using ReservoirComputing\ninput_size = 3\nres_size = 300\ndesn = DeepESN(input_size, [res_size, res_size], input_size;\n    init_reservoir=rand_sparse(; radius=1.2, sparsity=6/300),\n    state_modifiers=[NLAT2, ExtendedSquare]\n)\n","category":"page"},{"location":"tutorials/deep_esn/","page":"Deep Echo State Networks","title":"Deep Echo State Networks","text":"The training and prediction follow the usual framework:","category":"page"},{"location":"tutorials/deep_esn/","page":"Deep Echo State Networks","title":"Deep Echo State Networks","text":"using Random\nRandom.seed!(42)\nrng = MersenneTwister(17)\n\nps, st = setup(rng, desn)\nps, st = train!(desn, input_data, target_data, ps, st)\n\noutput, st = predict(desn, 1250, ps, st; initialdata=test_data[:, 1])","category":"page"},{"location":"tutorials/deep_esn/","page":"Deep Echo State Networks","title":"Deep Echo State Networks","text":"Plotting the results:","category":"page"},{"location":"tutorials/deep_esn/","page":"Deep Echo State Networks","title":"Deep Echo State Networks","text":"using Plots\n\nts = 0.0:0.02:200.0\nlorenz_maxlyap = 0.9056\npredict_ts = ts[(shift + train_len + 1):(shift + train_len + predict_len)]\nlyap_time = (predict_ts .- predict_ts[1]) * (1 / lorenz_maxlyap)\n\np1 = plot(lyap_time, [test_data[1, :] output[1, :]]; label=[\"actual\" \"predicted\"],\n    ylabel=\"x(t)\", linewidth=2.5, xticks=false, yticks=-15:15:15);\np2 = plot(lyap_time, [test_data[2, :] output[2, :]]; label=[\"actual\" \"predicted\"],\n    ylabel=\"y(t)\", linewidth=2.5, xticks=false, yticks=-20:20:20);\np3 = plot(lyap_time, [test_data[3, :] output[3, :]]; label=[\"actual\" \"predicted\"],\n    ylabel=\"z(t)\", linewidth=2.5, xlabel=\"max(λ)*t\", yticks=10:15:40);\n\nplot(p1, p2, p3; plot_title=\"Lorenz System Coordinates\",\n    layout=(3, 1), xtickfontsize=12, ytickfontsize=12, xguidefontsize=15,\n    yguidefontsize=15,\n    legendfontsize=12, titlefontsize=20)","category":"page"},{"location":"tutorials/deep_esn/#References","page":"Deep Echo State Networks","title":"References","text":"","category":"section"},{"location":"tutorials/deep_esn/","page":"Deep Echo State Networks","title":"Deep Echo State Networks","text":"Gallicchio, C. and Micheli, A. (2017). Deep echo state network (deepesn): A brief survey, arXiv preprint arXiv:1712.04323.\n\n\n\n","category":"page"},{"location":"tutorials/lorenz_basic/#Lorenz-System-Forecasting","page":"Chaos forecasting with an ESN","title":"Lorenz System Forecasting","text":"","category":"section"},{"location":"tutorials/lorenz_basic/","page":"Chaos forecasting with an ESN","title":"Chaos forecasting with an ESN","text":"This example expands on the readme Lorenz system forecasting to showcase how to use models and methods provided in the library for Echo State Networks.","category":"page"},{"location":"tutorials/lorenz_basic/#Generating-the-data","page":"Chaos forecasting with an ESN","title":"Generating the data","text":"","category":"section"},{"location":"tutorials/lorenz_basic/","page":"Chaos forecasting with an ESN","title":"Chaos forecasting with an ESN","text":"Starting off the workflow, the first step is to obtain the data. We use OrdinaryDiffEq to derive the Lorenz system data. The data is passed to the model as a matrix, where the columns represent the time steps.","category":"page"},{"location":"tutorials/lorenz_basic/","page":"Chaos forecasting with an ESN","title":"Chaos forecasting with an ESN","text":"using OrdinaryDiffEq\n\n#define lorenz system\nfunction lorenz!(du, u, p, t)\n    du[1] = 10.0 * (u[2] - u[1])\n    du[2] = u[1] * (28.0 - u[3]) - u[2]\n    du[3] = u[1] * u[2] - (8 / 3) * u[3]\nend\n\n#solve and take data\nprob = ODEProblem(lorenz!, [1.0, 0.0, 0.0], (0.0, 200.0))\ndata = solve(prob, ABM54(); dt=0.02)\ndata = reduce(hcat, data.u)","category":"page"},{"location":"tutorials/lorenz_basic/","page":"Chaos forecasting with an ESN","title":"Chaos forecasting with an ESN","text":"Now we split the data in training and testing. To do an autoregressive forecast we want the model to be trained on the next step, so we are going to shift the target data by one. Additionally, we discard the transient period.","category":"page"},{"location":"tutorials/lorenz_basic/","page":"Chaos forecasting with an ESN","title":"Chaos forecasting with an ESN","text":"#determine shift length, training length and prediction length\nshift = 300\ntrain_len = 5000\npredict_len = 1250\n\n#split the data accordingly\ninput_data = data[:, shift:(shift + train_len - 1)]\ntarget_data = data[:, (shift + 1):(shift + train_len)]\ntest_data = data[:, (shift + train_len + 1):(shift + train_len + predict_len)]","category":"page"},{"location":"tutorials/lorenz_basic/","page":"Chaos forecasting with an ESN","title":"Chaos forecasting with an ESN","text":"It is important to notice that the data needs to be formatted in a matrix with the features as rows and time steps as columns as in this example This is needed even if the time series consists of single values.","category":"page"},{"location":"tutorials/lorenz_basic/#Building-the-Echo-State-Network","page":"Chaos forecasting with an ESN","title":"Building the Echo State Network","text":"","category":"section"},{"location":"tutorials/lorenz_basic/","page":"Chaos forecasting with an ESN","title":"Chaos forecasting with an ESN","text":"Once the data is ready, it is possible to define the parameters for the ESN and the ESN struct itself. In this example, the values from (Pathak et al., 2017) are loosely followed as general guidelines.","category":"page"},{"location":"tutorials/lorenz_basic/","page":"Chaos forecasting with an ESN","title":"Chaos forecasting with an ESN","text":"using ReservoirComputing\n\n#define ESN parameters\nres_size = 300\nin_size = 3\nres_radius = 1.2\nres_sparsity = 6 / 300\ninput_scaling = 0.1\n\n#build ESN struct\nesn = ESN(in_size, res_size, in_size; #autoregressive so in_size = out_size\n    init_reservoir = rand_sparse(; radius = res_radius, sparsity = res_sparsity),\n    init_input = weighted_init(; scaling = input_scaling),\n    state_modifiers = NLAT2\n)","category":"page"},{"location":"tutorials/lorenz_basic/","page":"Chaos forecasting with an ESN","title":"Chaos forecasting with an ESN","text":"In this case, a size of 300 has been chosen, so the reservoir matrix will be 300 x 300. However, this is not always the case, since some input layer constructions can modify the dimensions of the reservoir. Please make sure to read the API documentation of the initializer you intend to use if you think that is cause of errors.","category":"page"},{"location":"tutorials/lorenz_basic/","page":"Chaos forecasting with an ESN","title":"Chaos forecasting with an ESN","text":"The res_radius determines the scaling of the spectral radius of the reservoir matrix; a proper scaling is necessary to assure the Echo State Property. The default value in the rand_sparse method is 1.0 in accordance with the most commonly followed guidelines found in the literature (see (Lukoševičius, 2012) and references therein).","category":"page"},{"location":"tutorials/lorenz_basic/","page":"Chaos forecasting with an ESN","title":"Chaos forecasting with an ESN","text":"The value of input_scaling determines the upper and lower bounds of the uniform distribution of the weights in the weighted_init. The value of 0.1 represents the default. The default input layer is the scaled_rand, a dense matrix. The details of the weighted version can be found in (Lu et al., 2017), for this example, this version returns the best results.","category":"page"},{"location":"tutorials/lorenz_basic/#Training-and-Prediction","page":"Chaos forecasting with an ESN","title":"Training and Prediction","text":"","category":"section"},{"location":"tutorials/lorenz_basic/","page":"Chaos forecasting with an ESN","title":"Chaos forecasting with an ESN","text":"Training for ESNs usually means solving a linear regression. The libbrary supports solvers from 'MLILinearModels.jl', in addition to a custom implementation of ridge regression. In this example we will use the latter.","category":"page"},{"location":"tutorials/lorenz_basic/","page":"Chaos forecasting with an ESN","title":"Chaos forecasting with an ESN","text":"Since ReservoirComputing.jl builds on LuxCore.jl we first need to setup the state and the parameters","category":"page"},{"location":"tutorials/lorenz_basic/","page":"Chaos forecasting with an ESN","title":"Chaos forecasting with an ESN","text":"using Random\nRandom.seed!(42)\nrng = MersenneTwister(17)\n\nps, st = setup(rng, esn)","category":"page"},{"location":"tutorials/lorenz_basic/","page":"Chaos forecasting with an ESN","title":"Chaos forecasting with an ESN","text":"Now we can proceed with training the ESN model. Usually an initial transient is discarded, to account for the dynamics of the ESN to settle. This can be done by passing the washout keyword argument to train.","category":"page"},{"location":"tutorials/lorenz_basic/","page":"Chaos forecasting with an ESN","title":"Chaos forecasting with an ESN","text":"#define training method\ntraining_method = StandardRidge(0.0)\n\nps, st = train!(esn, input_data, target_data, ps, st, training_method)","category":"page"},{"location":"tutorials/lorenz_basic/","page":"Chaos forecasting with an ESN","title":"Chaos forecasting with an ESN","text":"ps now contains the trained parameters for the ESN.","category":"page"},{"location":"tutorials/lorenz_basic/","page":"Chaos forecasting with an ESN","title":"Chaos forecasting with an ESN","text":"info: Returning training states\nThe ESN states are internally used the training, however they are not returned by default. To inspect the states, it is necessary to set the boolean keyword argument return_states as true in the train! call.","category":"page"},{"location":"tutorials/lorenz_basic/","page":"Chaos forecasting with an ESN","title":"Chaos forecasting with an ESN","text":"ReservoirComputing.jl provides additional utilities functions for autoregressive forecasting:","category":"page"},{"location":"tutorials/lorenz_basic/","page":"Chaos forecasting with an ESN","title":"Chaos forecasting with an ESN","text":"output, st = predict(esn, predict_len, ps, st; initialdata=test_data[:, 1])","category":"page"},{"location":"tutorials/lorenz_basic/","page":"Chaos forecasting with an ESN","title":"Chaos forecasting with an ESN","text":"To inspect the results, they can easily be plotted using an external library. In this case, we will use Plots.jl:","category":"page"},{"location":"tutorials/lorenz_basic/","page":"Chaos forecasting with an ESN","title":"Chaos forecasting with an ESN","text":"using Plots, Plots.PlotMeasures\n\nts = 0.0:0.02:200.0\nlorenz_maxlyap = 0.9056\npredict_ts = ts[(shift + train_len + 1):(shift + train_len + predict_len)]\nlyap_time = (predict_ts .- predict_ts[1]) * (1 / lorenz_maxlyap)\n\np1 = plot(lyap_time, [test_data[1, :] output[1, :]]; label=[\"actual\" \"predicted\"],\n    ylabel=\"x(t)\", linewidth=2.5, xticks=false, yticks=-15:15:15);\np2 = plot(lyap_time, [test_data[2, :] output[2, :]]; label=[\"actual\" \"predicted\"],\n    ylabel=\"y(t)\", linewidth=2.5, xticks=false, yticks=-20:20:20);\np3 = plot(lyap_time, [test_data[3, :] output[3, :]]; label=[\"actual\" \"predicted\"],\n    ylabel=\"z(t)\", linewidth=2.5, xlabel=\"max(λ)*t\", yticks=10:15:40);\n\nplot(p1, p2, p3; plot_title=\"Lorenz System Coordinates\",\n    layout=(3, 1), xtickfontsize=12, ytickfontsize=12, xguidefontsize=15,\n    yguidefontsize=15,\n    legendfontsize=12, titlefontsize=20)","category":"page"},{"location":"tutorials/lorenz_basic/#References","page":"Chaos forecasting with an ESN","title":"References","text":"","category":"section"},{"location":"tutorials/lorenz_basic/","page":"Chaos forecasting with an ESN","title":"Chaos forecasting with an ESN","text":"Lu, Z.; Pathak, J.; Hunt, B.; Girvan, M.; Brockett, R. and Ott, E. (2017). Reservoir observers: Model-free inference of unmeasured variables in chaotic systems. Chaos: An Interdisciplinary Journal of Nonlinear Science 27.\n\n\n\nLukoševičius, M. (2012). A Practical Guide to Applying Echo State Networks. In: Neural Networks: Tricks of the Trade (Springer Berlin Heidelberg); pp. 659–686.\n\n\n\nPathak, J.; Lu, Z.; Hunt, B. R.; Girvan, M. and Ott, E. (2017). Using machine learning to replicate chaotic attractors and calculate Lyapunov exponents from data. Chaos: An Interdisciplinary Journal of Nonlinear Science 27.\n\n\n\n","category":"page"},{"location":"api/inits/delay_line_backward/#delay*line*backward","page":"delaylinebackward","title":"delaylinebackward","text":"","category":"section"},{"location":"api/inits/delay_line_backward/#ReservoirComputing.delay_line_backward","page":"delaylinebackward","title":"ReservoirComputing.delay_line_backward","text":"delay_line_backward([rng], [T], dims...;\n    weight=0.1, fb_weight=0.1, return_sparse=false,\n    delay_kwargs=(), fb_kwargs=())\n\nCreate a delay line backward reservoir with the specified by dims and weights. Creates a matrix with backward connections as described in (Rodan and Tino, 2011).\n\nArguments\n\nrng: Random number generator. Default is Utils.default_rng() from WeightInitializers.\nT: Type of the elements in the reservoir matrix. Default is Float32.\ndims: Dimensions of the reservoir matrix.\n\nKeyword arguments\n\nweight: The weight determines the absolute value of forward connections in the reservoir. This can be provided as a single value or an array. In case it is provided as an array please make sure that the length of the array matches the length of the sub-diagonal you want to populate. Default is 0.1\nfb_weight: Determines the absolute value of backward connections in the reservoir. This can be provided as a single value or an array. In case it is provided as an array please make sure that the length of the array matches the length of the sub-diagonal you want to populate. Default is 0.1.\nfb_shift: How far the backward connection will be from the diagonal. Default is 2.\nreturn_sparse: flag for returning a sparse matrix. Default is false.\ndelay_kwargs and fb_kwargs: named tuples that control the kwargs for the delay line weight and feedback weights respectively. The kwargs are as follows:\nsampling_type: Sampling that decides the distribution of weight negative numbers. If set to :no_sample the sign is unchanged. If set to :bernoulli_sample! then each weight can be positive with a probability set by positive_prob. If set to :irrational_sample! the weight is negative if the decimal number of the irrational number chosen is odd. If set to :regular_sample!, each weight will be assigned a negative sign after the chosen strides. strides can be a single number or an array. Default is :no_sample.\npositive_prob: probability of the weight being positive when sampling_type is set to :bernoulli_sample!. Default is 0.5.\nirrational: Irrational number whose decimals decide the sign of weight. Default is pi.\nstart: Which place after the decimal point the counting starts for the irrational sign counting. Default is 1.\nstrides: number of strides for assigning negative value to a weight. It can be an integer or an array. Default is 2.\n\nExamples\n\njulia> res_matrix = delay_line_backward(5, 5)\n5×5 Matrix{Float32}:\n 0.0  0.1  0.0  0.0  0.0\n 0.1  0.0  0.1  0.0  0.0\n 0.0  0.1  0.0  0.1  0.0\n 0.0  0.0  0.1  0.0  0.1\n 0.0  0.0  0.0  0.1  0.0\n\njulia> res_matrix = delay_line_backward(Float16, 5, 5)\n5×5 Matrix{Float16}:\n 0.0  0.1  0.0  0.0  0.0\n 0.1  0.0  0.1  0.0  0.0\n 0.0  0.1  0.0  0.1  0.0\n 0.0  0.0  0.1  0.0  0.1\n 0.0  0.0  0.0  0.1  0.0\n\n\n\n\n\n","category":"function"},{"location":"api/inits/delay_line_backward/#References","page":"delaylinebackward","title":"References","text":"","category":"section"},{"location":"api/inits/delay_line_backward/","page":"delaylinebackward","title":"delaylinebackward","text":"Rodan, A. and Tino, P. (2011). Minimum Complexity Echo State Network. IEEE Transactions on Neural Networks 22, 131–144.\n\n\n\n","category":"page"},{"location":"api/inits/block_diagonal/#block_diagonal","page":"block_diagonal","title":"block_diagonal","text":"","category":"section"},{"location":"api/inits/block_diagonal/#ReservoirComputing.block_diagonal","page":"block_diagonal","title":"ReservoirComputing.block_diagonal","text":"block_diagonal([rng], [T], dims...;\n    weight=1, block_size=1,\n    return_sparse=false)\n\nCreates a block‐diagonal matrix consisting of square blocks of size block_size along the main diagonal (Ma et al., 2023). Each block may be filled with\n\na single scalar\na vector of per‐block weights (length = number of blocks)\n\nEquations\n\nW_ij =\nbegincases\n    w_b  textif leftlfloorfraci-1srightrfloor = leftlfloorfracj-1srightrfloor = b\n           s = textblock_size b=0dotsnb-1 \n    0    textotherwise\nendcases\n\nArguments\n\nrng: Random number generator. Default is Utils.default_rng().\nT: Element type of the matrix. Default is Float32.\ndims: Dimensions of the output matrix (must be two-dimensional).\n\nKeyword arguments\n\nweight:\nscalar: every block is filled with that value\nvector: length = number of blocks, one constant per block\nDefault is 1.0.\nblock_size: Size(s) of each square block on the diagonal. Default is 1.0.\nreturn_sparse: If true, returns the matrix as sparse. SparseArrays.jl must be lodead. Default is false.\n\nExamples\n\n# 4×4 with two 2×2 blocks of 1.0\njulia> W1 = block_diagonal(4, 4; block_size=2)\n4×4 Matrix{Float32}:\n 1.0  1.0  0.0  0.0\n 1.0  1.0  0.0  0.0\n 0.0  0.0  1.0  1.0\n 0.0  0.0  1.0  1.0\n\n# per-block weights [0.5, 2.0]\njulia> W2 = block_diagonal(4, 4; block_size=2, weight=[0.5, 2.0])\n4×4 Matrix{Float32}:\n 0.5  0.5  0.0  0.0\n 0.5  0.5  0.0  0.0\n 0.0  0.0  2.0  2.0\n 0.0  0.0  2.0  2.0\n\n\n\n\n\n","category":"function"},{"location":"api/inits/block_diagonal/#References","page":"block_diagonal","title":"References","text":"","category":"section"},{"location":"api/inits/block_diagonal/","page":"block_diagonal","title":"block_diagonal","text":"Ma, H.; Prosperino, D.; Haluszczynski, A. and Räth, C. (2023). Efficient forecasting of chaotic systems with block-diagonal and binary reservoir computing. Chaos: An Interdisciplinary Journal of Nonlinear Science 33.\n\n\n\n","category":"page"},{"location":"api/inits/#Echo-State-Networks-Initializers","page":"Initializers","title":"Echo State Networks Initializers","text":"","category":"section"},{"location":"api/inits/#Input-layers","page":"Initializers","title":"Input layers","text":"","category":"section"},{"location":"api/inits/","page":"Initializers","title":"Initializers","text":"chebyshev_mapping\ninformed_init\nlogistic_mapping\nminimal_init\nmodified_lm\nscaled_rand\nweighted_init\nweighted_minimal","category":"page"},{"location":"api/inits/#Reservoirs","page":"Initializers","title":"Reservoirs","text":"","category":"section"},{"location":"api/inits/","page":"Initializers","title":"Initializers","text":"block_diagonal\nchaotic_init\ncycle_jumps\ndelay_line\ndelaylinebackward\ndouble_cycle\nforward_connection\nlow_connectivity\npseudo_svd\nrand_sparse\nselfloop_cycle\nselfloopdelaylinebackward\nselfloopfeedbackcycle\nselfloopforwardconnection\nsimple_cycle\ntruedoublecycle","category":"page"},{"location":"api/inits/#Building-functions","page":"Initializers","title":"Building functions","text":"","category":"section"},{"location":"api/inits/","page":"Initializers","title":"Initializers","text":"add_jumps!\nbackward_connection!\ndelay_line!\nreversesimplecycle!\nscale_radius!\nself_loop!\nsimple_cycle!","category":"page"},{"location":"api/inits/weighted_init/#weighted_init","page":"weighted_init","title":"weighted_init","text":"","category":"section"},{"location":"api/inits/weighted_init/#ReservoirComputing.weighted_init","page":"weighted_init","title":"ReservoirComputing.weighted_init","text":"weighted_init([rng], [T], dims...;\n    scaling=0.1, return_sparse=false)\n\nCreate and return a matrix representing a weighted input layer. This initializer generates a weighted input matrix with random non-zero elements distributed uniformly within the range [-scaling, scaling] (Lu et al., 2017).\n\nArguments\n\nrng: Random number generator. Default is Utils.default_rng() from WeightInitializers.\nT: Type of the elements in the reservoir matrix. Default is Float32.\ndims: Dimensions of the matrix. Should follow res_size x in_size.\n\nKeyword arguments\n\nscaling: The scaling factor for the weight distribution. Defaults to 0.1.\nreturn_sparse: flag for returning a sparse matrix. Default is false.\n\nExamples\n\njulia> res_input = weighted_init(8, 3)\n6×3 Matrix{Float32}:\n  0.0452399   0.0          0.0\n -0.0348047   0.0          0.0\n  0.0        -0.0386004    0.0\n  0.0         0.00981022   0.0\n  0.0         0.0          0.0577838\n  0.0         0.0         -0.0562827\n\n\n\n\n\n","category":"function"},{"location":"api/inits/weighted_init/#References","page":"weighted_init","title":"References","text":"","category":"section"},{"location":"api/inits/weighted_init/","page":"weighted_init","title":"weighted_init","text":"Lu, Z.; Pathak, J.; Hunt, B.; Girvan, M.; Brockett, R. and Ott, E. (2017). Reservoir observers: Model-free inference of unmeasured variables in chaotic systems. Chaos: An Interdisciplinary Journal of Nonlinear Science 27.\n\n\n\n","category":"page"},{"location":"api/inits/double_cycle/#double_cycle","page":"double_cycle","title":"double_cycle","text":"","category":"section"},{"location":"api/inits/double_cycle/#ReservoirComputing.double_cycle","page":"double_cycle","title":"ReservoirComputing.double_cycle","text":"double_cycle([rng], [T], dims...;\n    cycle_weight=0.1, second_cycle_weight=0.1,\n    return_sparse=false)\n\nCreates a double cycle reservoir (Fu et al., 2023).\n\nArguments\n\nrng: Random number generator. Default is Utils.default_rng() from WeightInitializers.\nT: Type of the elements in the reservoir matrix. Default is Float32.\ndims: Dimensions of the reservoir matrix.\n\nKeyword arguments\n\ncycle_weight: Weight of the upper cycle connections in the reservoir matrix. Default is 0.1.\nsecond_cycle_weight: Weight of the lower cycle connections in the reservoir matrix. Default is 0.1.\nreturn_sparse: flag for returning a sparse matrix. Default is false.\n\nExamples\n\njulia> reservoir_matrix = double_cycle(5, 5; cycle_weight = 0.1, second_cycle_weight = 0.3)\n5×5 Matrix{Float32}:\n 0.0  0.3  0.0  0.0  0.3\n 0.1  0.0  0.3  0.0  0.0\n 0.0  0.1  0.0  0.3  0.0\n 0.0  0.0  0.1  0.0  0.3\n 0.1  0.0  0.0  0.1  0.0\n\n\n\n\n\n","category":"function"},{"location":"api/inits/double_cycle/#References","page":"double_cycle","title":"References","text":"","category":"section"},{"location":"api/inits/double_cycle/","page":"double_cycle","title":"double_cycle","text":"Fu, J.; Li, G.; Tang, J.; Xia, L.; Wang, L. and Duan, S. (2023). A double-cycle echo state network topology for time series prediction. Chaos: An Interdisciplinary Journal of Nonlinear Science 33.\n\n\n\n","category":"page"},{"location":"api/inits/reverse_simple_cycle!/#reverse*simple*cycle!","page":"reversesimplecycle!","title":"reversesimplecycle!","text":"","category":"section"},{"location":"api/inits/reverse_simple_cycle!/#ReservoirComputing.reverse_simple_cycle!","page":"reversesimplecycle!","title":"ReservoirComputing.reverse_simple_cycle!","text":"reverse_simple_cycle!([rng], reservoir_matrix, weight;\n    sampling_type=:no_sample, irrational=pi, start=1,\n    p=0.5)\n\nAdds a reverse simple cycle in the reservoir_matrix, with given weight. The weight can be a single number or an array.\n\nArguments\n\nrng: Random number generator. Default is Utils.default_rng() from WeightInitializers.\nreservoir_matrix: matrix to be changed.\nweight: weight to add as a simple cycle. Can be either a single number or an array.\n\nKeyword arguments\n\nsampling_type: Sampling that decides the distribution of weight negative numbers. If set to :no_sample the sign is unchanged. If set to :bernoulli_sample! then each weight can be positive with a probability set by positive_prob. If set to :irrational_sample! the weight is negative if the decimal number of the irrational number chosen is odd. If set to :regular_sample!, each weight will be assigned a negative sign after the chosen strides. strides can be a single number or an array. Default is :no_sample.\npositive_prob: probability of the weight being positive when sampling_type is set to :bernoulli_sample!. Default is 0.5.\nirrational: Irrational number whose decimals decide the sign of weight. Default is pi.\nstart: Which place after the decimal point the counting starts for the irrational sign counting. Default is 1.\nstrides: number of strides for assigning negative value to a weight. It can be an integer or an array. Default is 2.\n\nExamples\n\njulia> matrix = zeros(Float32, 5, 5)\n5×5 Matrix{Float32}:\n 0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n\njulia> reverse_simple_cycle!(matrix, 1.0; sampling_type = :regular_sample!)\n5×5 Matrix{Float32}:\n 0.0  -1.0  0.0   0.0  0.0\n 0.0   0.0  1.0   0.0  0.0\n 0.0   0.0  0.0  -1.0  0.0\n 0.0   0.0  0.0   0.0  1.0\n 1.0   0.0  0.0   0.0  0.0\n\n\n\n\n\n","category":"function"},{"location":"api/train/#Train","page":"Train","title":"Train","text":"","category":"section"},{"location":"api/train/#ReservoirComputing.train!","page":"Train","title":"ReservoirComputing.train!","text":"train!(rc, train_data, target_data, ps, st,\n       train_method=StandardRidge(0.0);\n       washout=0, return_states=false)\n\nTrains a given reservoir computing by creating the reservoir states from train_data, and then fiting the readout layer using target_data as target. The learned weights/layer are written into ps.\n\nArguments\n\nrc: A reservoir computing model, either provided by ReservoirComputing.jl or built with ReservoirChain. Must contain a trainable layer (for example LinearReadout), and a collection point Collect.\ntrain_data: input sequence where columns are time steps.\ntarget_data: targets aligned with train_data.\nps: model parameters.\nst: model states.\ntrain_method: training algorithm. Default is StandardRidge.\n\nKeyword arguments\n\nwashout: number of initial time steps to discard (applied equally to features and targets). Default 0.\nreturn_states: if true, also returns the feature matrix used for the fit.\nkwargs...: additional keyword arguments for the training algorithm, if needed. Defaults vary according to the different training method.\n\nReturns\n\n(ps, st): updated model parameters and states.\n(ps, st), states: If return_states=true.\n\nNotes\n\nFeatures are produced by collectstates(rc, train_data, ps, st). If you rely on the implicit collection of a LinearReadout, make sure that readout was created with include_collect=true, or insert an explicit Collect() earlier in the ReservoirChain.\n\n\n\n\n\n","category":"function"},{"location":"api/train/#ReservoirComputing.train","page":"Train","title":"ReservoirComputing.train","text":"train(train_method, states, target_data; kwargs...)\n\nLower level training hook to fit a readout from precomputed reservoir features and given targets.\n\nDispatching on this method with different training methods allows one to hook directly into train! without additional changes.\n\nArguments\n\ntrain_method: An object describing the training algorithm and its hyperparameters (e.g. regularization strength, solver choice, constraints).\nstates: Feature matrix with reservoir states (ie. obtained with collectstates). Shape (n_features, T), where T is the number of samples (e.g. time steps).\ntarget_data: Target matrix aligned with states. Shape (n_outputs, T).\n\nReturns\n\noutput_weights: Trained readout. Should be a forward method to be hooked into a layer. For instance, in case of linear regression output_weights is a mtrix consumable by LinearReadout.\n\nNotes\n\nAny sequence pre-processing (e.g. washout) should be handled by the caller before invoking train. See train! for an end-to-end workflow.\nFor very long T, consider chunked or iterative solvers to reduce memory usage.\nIf your approach returns additional artifacts (e.g. diagnostics), prefer storing them inside train_method or exposing a separate API; keep train’s return value as the forward method only.\n\n\n\n\n\n","category":"function"},{"location":"api/train/#Training-methods","page":"Train","title":"Training methods","text":"","category":"section"},{"location":"api/train/#ReservoirComputing.StandardRidge","page":"Train","title":"ReservoirComputing.StandardRidge","text":"StandardRidge([Type], [reg])\n\nRidge regression method.\n\nEquations\n\nmathbfw = (mathbfX^top mathbfX +\nlambda mathbfI)^-1 mathbfX^top mathbfy\n\nArguments\n\nType: type of the regularization argument. Default is inferred internally, there's usually no need to tweak this\nreg: regularization coefficient. Default is set to 0.0 (linear regression).\n\n\n\n\n\n","category":"type"},{"location":"api/layers/#Layers","page":"Layers","title":"Layers","text":"","category":"section"},{"location":"api/layers/#Base-Layers","page":"Layers","title":"Base Layers","text":"","category":"section"},{"location":"api/layers/#ReservoirComputing.ReservoirComputer","page":"Layers","title":"ReservoirComputing.ReservoirComputer","text":"ReservoirComputer(reservoir, states_modifiers, readout)\n\nGeneric reservoir-computing container that wires together:\n\na reservoir (any Lux-compatible layer producing features),\nzero or more states_modifiers applied sequentially to the reservoir features,\na readout layer (typically LinearReadout).\n\nThe container exposes a standard (x, ps, st) -> (y, st′) interface and utility functions to initialize parameters/states, stream sequences to collect features, and install trained readout weights.\n\nArguments\n\nreservoir: a layer that consumes inputs and produces feature vectors.\nstates_modifiers: a tuple (or vector converted to Tuple) of layers applied after the reservoir (may be empty).\nreadout: the final trainable layer mapping features to outputs.\n\nInputs\n\nx: input to the reservoir (shape determined by the reservoir).\nps: reservoir computing parameters.\nst: reservoir computing states.\n\nReturns\n\n(y, st′) where y is the readout output and st′ contains the updated states of the reservoir, modifiers, and readout.\n\n\n\n\n\n","category":"type"},{"location":"api/layers/#ReservoirComputing.ReservoirChain","page":"Layers","title":"ReservoirComputing.ReservoirChain","text":"ReservoirChain(layers...; name=nothing)\nReservoirChain(xs::AbstractVector; name=nothing)\nReservoirChain(nt::NamedTuple; name=nothing)\nReservoirChain(; name=nothing, kwargs...)\n\nA lightweight, Lux-compatible container that composes a sequence of layers and executes them in order. The implementation of ReservoirChain is equivalent to Lux's own Chain.\n\nConstruction\n\nYou can build a chain from:\n\nPositional layers: ReservoirChain(l1, l2, ...)\nA vector of layers: ReservoirChain([l1, l2, ...])\nA named tuple of layers: ReservoirChain((; layer_a=l1, layer_b=l2))\nKeywords (sugar for a named tuple): ReservoirChain(; layer_a=l1, layer_b=l2)\n\nIn all cases, function objects are automatically wrapped via WrappedFunction so they can participate like regular layers. If a LinearReadout with include_collect=true is present, the chain automatically inserts a Collect layer immediately before that readout.\n\nUse name to optionally tag the chain instance.\n\nInputs\n\n(x, ps, st) where:\n\nx: input to the first layer.\nps: parameters as a named tuple with the same fields and order as the chain's layers.\nst: states as a named tuple with the same fields and order as the chain's layers.\n\nThe call (c::ReservoirChain)(x, ps, st) forwards x through each layer: (x, ps_i, st_i) -> (x_next, st_i′) and returns the final output and the updated states for every layer.\n\nReturns\n\n(y, st′) where y is the output of the last layer and st′ is a named tuple collecting the updated states for each layer.\n\nParameters\n\nA NamedTuple whose fields correspond 1:1 with the layers. Each field holds the parameters for that layer.\nField names are generated as :layer_1, :layer_2, ... when constructed positionally, or preserved when you pass a NamedTuple/keyword constructor.\n\nStates\n\nA NamedTuple whose fields correspond 1:1 with the layers. Each field holds the state for that layer.\n\nLayer access & indexing\n\nc[i]: get the i-th layer (1-based).\nc[indices]: return a new ReservoirChain formed by selecting a subset of layers.\ngetproperty(c, :layer_k): access layer k by its generated/explicit name.\nlength(c), firstindex(c), lastindex(c): standard collection interfaces.\n\nNotes\n\nFunction wrapping: Any plain Function in the constructor is wrapped as WrappedFunction(f). Non-layer, non-function objects will error.\nAuto-collect for readouts: When a LinearReadout has include_collect=true, the constructor expands it to (Collect(), readout) so that downstream tooling can capture features consistently.\n\n\n\n\n\n","category":"type"},{"location":"api/layers/#ReservoirComputing.Collect","page":"Layers","title":"ReservoirComputing.Collect","text":"Collect()\n\nMarker layer that passes data through unchanged but marks a feature checkpoint for collectstates. At each time step, whenever a Collect is encountered in the chain, the current vector is recorded as part of the feature vector used to train the readout. If multiple Collect layers exist, their vectors are concatenated with vcat in order of appearance.\n\nArguments\n\nNone.\n\nKeyword arguments\n\nNone.\n\nInputs\n\nx :: AbstractArray (d, batch) — the current tensor flowing through the chain.\n\nReturns\n\n(x, st) — the same tensor x and the unchanged state st.\n\nParameters\n\nNone.\n\nStates\n\nNone.\n\nNotes\n\nWhen used with a single Collect before a LinearReadout, training uses exactly the tensor right before the readout (e.g., the reservoir state).\nWith multiple Collect layers (e.g., after different submodules), the per-step features are vcat-ed in chain order to form one feature vector.\nIf the readout is constructed with include_collect=true, an implicit collection point is assumed immediately before the readout. Use an explicit Collect only when you want to control where/what is collected (or to stack multiple features).\n```julia rc = ReservoirChain(         StatefulLayer(ESNCell(3 => 300)),         NLAT2(),         Collect(), # <– collect the 300-dim reservoir after NLAT2         LinearReadout(300 => 3; include_collect=false) # <– toggle off the default Collect()     )\n\n```\n\n\n\n\n\n","category":"type"},{"location":"api/layers/#ReservoirComputing.StatefulLayer","page":"Layers","title":"ReservoirComputing.StatefulLayer","text":"StatefulLayer(cell::AbstractReservoirRecurrentCell)\n\nA lightweight wrapper that makes a recurrent cell carry its input state to the next step.\n\nArguments\n\ncell: AbstractReservoirRecurrentCell (e.g. ESNCell).\n\nStates\n\ncell: internal states for the wrapped cell (e.g., RNG replicas, etc.).\ncarry: the per-sequence hidden state; initialized to nothing.\n\n\n\n\n\n","category":"type"},{"location":"api/layers/#Readout-Layers","page":"Layers","title":"Readout Layers","text":"","category":"section"},{"location":"api/layers/#ReservoirComputing.LinearReadout","page":"Layers","title":"ReservoirComputing.LinearReadout","text":"LinearReadout(in_dims => out_dims, [activation];\n        use_bias=false, include_collect=true)\n\nLinear readout layer with optional bias and elementwise activation. Intended as the final, trainable mapping from collected features (e.g., reservoir state) to outputs. When include_collect=true, training will collect features immediately before this layer (logically inserting a Collect right before it).\n\nEquation\n\nmathbfy = psileft(mathbfWmathbfz + mathbfbright)\n\nArguments\n\nin_dims: Input/feature dimension (e.g., reservoir size).\nout_dims: Output dimension (e.g., number of targets).\nactivation: Elementwise output nonlinearity. Default: identity.\n\nKeyword arguments\n\nuse_bias: Include an additive bias vector b. Default: false.\ninclude_collect: If true (default), training collects features immediately before this layer (as if a Collect were inserted right before it).\n\nParameters\n\nweight :: (out_dims × in_dims)\nbias   :: (out_dims,) — present only if use_bias=true\n\nStates\n\nNone.\n\nNotes\n\nIn ESN workflows, readout weights are typically replaced via ridge regression in train!. Therefore, how LinearReadout gets initialized is of no consequence. Additionally, the dimensions will also not be taken into account, as train! will replace the weights.\nIf you set include_collect=false, make sure a Collect appears earlier in the chain. Otherwise training may operate on the post-readout signal, which is usually unintended.\n\n\n\n\n\n","category":"type"},{"location":"api/layers/#ReservoirComputing.SVMReadout","page":"Layers","title":"ReservoirComputing.SVMReadout","text":"SVMReadout(in_dims => out_dims;\n    include_collect=true, kwargs...)\n\nReadout layer based on support vector machines. Requires LIBSVM.jl.\n\nArguments\n\nin_dims: Input/feature dimension (e.g., reservoir size).\nout_dims: Output dimension (e.g., number of targets).\n\nKeyword arguments\n\ninclude_collect: If true (default), training collects features immediately before this layer (as if a Collect() were inserted right before it).\nkwags: specific keyword arguments for LIBSVM elements.\n\n\n\n\n\n","category":"type"},{"location":"api/layers/#Echo-State-Networks","page":"Layers","title":"Echo State Networks","text":"","category":"section"},{"location":"api/layers/#ReservoirComputing.ESNCell","page":"Layers","title":"ReservoirComputing.ESNCell","text":"ESNCell(in_dims => out_dims, [activation];\n    use_bias=false, init_bias=rand32,\n    init_reservoir=rand_sparse, init_input=scaled_rand,\n    init_state=randn32, leak_coefficient=1.0)\n\nEcho State Network (ESN) recurrent cell with optional leaky integration.\n\nEquations\n\nbeginaligned\n    tildemathbfh(t) = phileft(mathbfW_inmathbfx(t) +\n        mathbfW_resmathbfh(t-1) + mathbfbright) \n    mathbfh(t) = (1-alpha)mathbfh(t-1) + alphatildemathbfh(t)\nendaligned\n\nArguments\n\nin_dims: Input dimension.\nout_dims: Reservoir (hidden state) dimension.\nactivation: Activation function. Default: tanh.\n\nKeyword arguments\n\nuse_bias: Whether to include a bias term. Default: false.\ninit_bias: Initializer for the bias. Used only if use_bias=true.   Default is rand32.\ninit_reservoir: Initializer for the reservoir matrix W_res. Default is rand_sparse.\ninit_input: Initializer for the input matrix W_in. Default is scaled_rand.\ninit_state: Initializer for the hidden state when an external state is not provided. Default is randn32.\nleak_coefficient: Leak rate α ∈ (0,1]. Default: 1.0.\n\nInputs\n\nCase 1: x :: AbstractArray (in_dims, batch) A fresh state is created via init_state; the call is forwarded to Case 2.\nCase 2: (x, (h,)) where h :: AbstractArray (out_dims, batch) Computes the update and returns the new state.\n\nIn both cases, the forward returns ((h_new, (h_new,)), st_out) where st_out contains any updated internal state.\n\nReturns\n\nOutput/hidden state h_new :: out_dims and state tuple (h_new,).\nUpdated layer state (NamedTuple).\n\nParameters\n\nCreated by initialparameters(rng, esn):\n\ninput_matrix :: (out_dims × in_dims) — W_in\nreservoir_matrix :: (out_dims × out_dims) — W_res\nbias :: (out_dims,) — present only if use_bias=true\n\nStates\n\nCreated by initialstates(rng, esn):\n\nrng: a replicated RNG used to sample initial hidden states when needed.\n\n\n\n\n\n","category":"type"},{"location":"api/layers/#Reservoir-computing-with-cellular-automata","page":"Layers","title":"Reservoir computing with cellular automata","text":"","category":"section"},{"location":"api/layers/#ReservoirComputing.RECACell","page":"Layers","title":"ReservoirComputing.RECACell","text":"RECACell(automaton, enc::RandomMaps)\n\nCellular Automata (CA)–based reservoir recurrent cell. At each time step, the input vector is randomly embedded into a CA configuration, the CA is evolved for a fixed number of generations, and the flattened CA evolution is emitted as the reservoir state. The last CA configuration is carried to the next step. For more details please refer to (Nichele and Molund, 2017), and (Yilmaz, 2014).\n\nArguments\n\nautomaton: A cellular automaton rule/object from CellularAutomata.jl (e.g., DCA(90), DCA(30), …).\nenc: Precomputed random-mapping/encoding metadata given as a RandomMapping.\n\nInputs\n\nCase A: a single input vector x with length in_dims. The cell internally uses the stored CA state (st.ca) as the previous configuration.\nCase B: a tuple (x, (ca,)) where x is as above and ca has length enc.ca_size.\n\nComputation\n\nRandom embedding of x into a CA initial condition c₀ using enc.maps across enc.permutations blocks of length enc.expansion_size.\nCA evolution for G = enc.generations steps with the given automaton, producing an evolution matrix E ∈ ℝ^{(G+1) × ca_size} where E[1,:] = c₀ and E[t+1,:] = F(E[t,:]).\nFeature vector is the flattened stack of E[2:end, :] (dropping the initial row), shaped as a column vector of length enc.states_size.\nCarry is the final CA configuration E[end, :].\n\nReturns\n\nOutput: (h, (caₙ,)) where\nh has length enc.states_size (the CA features),\ncaₙ has length enc.ca_size (next carry).\nUpdated (unchanged) cell state (parameters-free layer state).\n\nParameters & State\n\nParameters: none\nState: (ca = zeros(Float32, enc.ca_size))\n\n\n\n\n\n","category":"type"},{"location":"api/inits/minimal_init/#minimal_init","page":"minimal_init","title":"minimal_init","text":"","category":"section"},{"location":"api/inits/minimal_init/#ReservoirComputing.minimal_init","page":"minimal_init","title":"ReservoirComputing.minimal_init","text":"minimal_init([rng], [T], dims...;\n    sampling_type=:bernoulli_sample!, weight=0.1, irrational=pi,\n    start=1, p=0.5)\n\nCreate a layer matrix with uniform weights determined by weight (Rodan and Tino, 2011). The sign difference is randomly determined by the sampling chosen.\n\nArguments\n\nrng: Random number generator. Default is Utils.default_rng() from WeightInitializers.\nT: Type of the elements in the reservoir matrix. Default is Float32.\ndims: Dimensions of the matrix. Should follow res_size x in_size.\n\nKeyword arguments\n\nweight: The weight used to fill the layer matrix. Default is 0.1.\nsampling_type: Sampling that decides the distribution of weight negative numbers. If set to :no_sample the sign is unchanged. If set to :bernoulli_sample! then each weight can be positive with a probability set by positive_prob. If set to :irrational_sample! the weight is negative if the decimal number of the irrational number chosen is odd. If set to :regular_sample!, each weight will be assigned a negative sign after the chosen strides. strides can be a single number or an array. Default is :no_sample.\npositive_prob: probability of the weight being positive when sampling_type is set to :bernoulli_sample!. Default is 0.5.\nirrational: Irrational number whose decimals decide the sign of weight. Default is pi.\nstart: Which place after the decimal point the counting starts for the irrational sign counting. Default is 1.\nstrides: number of strides for assigning negative value to a weight. It can be an integer or an array. Default is 2.\n\nExamples\n\njulia> res_input = minimal_init(8, 3)\n8×3 Matrix{Float32}:\n  0.1  -0.1   0.1\n -0.1   0.1   0.1\n -0.1  -0.1   0.1\n -0.1  -0.1  -0.1\n  0.1   0.1   0.1\n -0.1  -0.1  -0.1\n -0.1  -0.1   0.1\n  0.1  -0.1   0.1\n\njulia> res_input = minimal_init(8, 3; sampling_type = :irrational)\n8×3 Matrix{Float32}:\n -0.1   0.1  -0.1\n  0.1  -0.1  -0.1\n  0.1   0.1  -0.1\n  0.1   0.1   0.1\n -0.1  -0.1  -0.1\n  0.1   0.1   0.1\n  0.1   0.1  -0.1\n -0.1   0.1  -0.1\n\njulia> res_input = minimal_init(8, 3; p = 0.1) # lower p -> more negative signs\n8×3 Matrix{Float32}:\n -0.1  -0.1  -0.1\n -0.1  -0.1  -0.1\n -0.1  -0.1  -0.1\n -0.1  -0.1  -0.1\n  0.1  -0.1  -0.1\n -0.1  -0.1  -0.1\n -0.1  -0.1  -0.1\n -0.1  -0.1  -0.1\n\njulia> res_input = minimal_init(8, 3; p = 0.8)# higher p -> more positive signs\n8×3 Matrix{Float32}:\n  0.1   0.1  0.1\n -0.1   0.1  0.1\n -0.1   0.1  0.1\n  0.1   0.1  0.1\n  0.1   0.1  0.1\n  0.1  -0.1  0.1\n -0.1   0.1  0.1\n  0.1   0.1  0.1\n\n\n\n\n\n","category":"function"},{"location":"api/inits/minimal_init/#References","page":"minimal_init","title":"References","text":"","category":"section"},{"location":"api/inits/minimal_init/","page":"minimal_init","title":"minimal_init","text":"Rodan, A. and Tino, P. (2011). Minimum Complexity Echo State Network. IEEE Transactions on Neural Networks 22, 131–144.\n\n\n\n","category":"page"},{"location":"api/inits/chaotic_init/#chaotic_init","page":"chaotic_init","title":"chaotic_init","text":"","category":"section"},{"location":"api/inits/chaotic_init/#ReservoirComputing.chaotic_init","page":"chaotic_init","title":"ReservoirComputing.chaotic_init","text":"chaotic_init([rng], [T], dims...;\n    extra_edge_probability=T(0.1), spectral_radius=one(T),\n    return_sparse=false)\n\nConstruct a chaotic reservoir matrix using a digital chaotic system (Xie et al., 2024).\n\nThe matrix topology is derived from a strongly connected adjacency matrix based on a digital chaotic system operating at finite precision. If the requested matrix order does not exactly match a valid order the closest valid order is used.\n\nArguments\n\nrng: Random number generator. Default is Utils.default_rng() from WeightInitializers.\nT: Type of the elements in the reservoir matrix. Default is Float32.\ndims: Dimensions of the reservoir matrix.\n\nKeyword arguments\n\nextra_edge_probability: Probability of adding extra random edges in the adjacency matrix to enhance connectivity. Default is 0.1.\ndesired_spectral_radius: The target spectral radius for the reservoir matrix. Default is one.\nreturn_sparse: If true, the function returns the reservoir matrix as a sparse matrix. Default is false.\n\nExamples\n\njulia> res_matrix = chaotic_init(8, 8)\n┌ Warning:\n│\n│     Adjusting reservoir matrix order:\n│         from 8 (requested) to 4\n│     based on computed bit precision = 1.\n│\n└ @ ReservoirComputing ~/.julia/dev/ReservoirComputing/src/esn/esn_inits.jl:805\n4×4 SparseArrays.SparseMatrixCSC{Float32, Int64} with 6 stored entries:\n   ⋅        -0.600945   ⋅          ⋅\n   ⋅          ⋅        0.132667   2.21354\n   ⋅        -2.60383    ⋅        -2.90391\n -0.578156    ⋅         ⋅          ⋅\n\n\n\n\n\n","category":"function"},{"location":"api/inits/chaotic_init/#References","page":"chaotic_init","title":"References","text":"","category":"section"},{"location":"api/inits/chaotic_init/","page":"chaotic_init","title":"chaotic_init","text":"Xie, M.; Wang, Q. and Yu, S. (2024). Time Series Prediction of ESN Based on Chebyshev Mapping and Strongly Connected Topology. Neural Processing Letters 56.\n\n\n\n","category":"page"},{"location":"api/inits/forward_connection/#forward_connection","page":"forward_connection","title":"forward_connection","text":"","category":"section"},{"location":"api/inits/forward_connection/#ReservoirComputing.forward_connection","page":"forward_connection","title":"ReservoirComputing.forward_connection","text":"forward_connection([rng], [T], dims...;\n    weight=0.1, selfloop_weight=0.1,\n    return_sparse=false)\n\nCreates a reservoir based on a forward connection of weights (Elsarraj et al., 2019).\n\nThis architecture is referred to as TP5 in the original paper.\n\nEquations\n\nW_ij =\nbegincases\n    r  textif  j = i - 2 text for  i = 3 dots N \n    0  textotherwise\nendcases\n\nArguments\n\nrng: Random number generator. Default is Utils.default_rng() from WeightInitializers.\nT: Type of the elements in the reservoir matrix. Default is Float32.\ndims: Dimensions of the reservoir matrix.\n\nKeyword arguments\n\nweight: Weight of the cycle connections in the reservoir matrix. This can be provided as a single value or an array. In case it is provided as an array please make sure that the length of the array matches the length of the sub-diagonal you want to populate. Default is 0.1.\nreturn_sparse: flag for returning a sparse matrix. Default is false.\nsampling_type: Sampling that decides the distribution of weight negative numbers. If set to :no_sample the sign is unchanged. If set to :bernoulli_sample! then each weight can be positive with a probability set by positive_prob. If set to :irrational_sample! the weight is negative if the decimal number of the irrational number chosen is odd. If set to :regular_sample!, each weight will be assigned a negative sign after the chosen strides. strides can be a single number or an array. Default is :no_sample.\npositive_prob: probability of the weight being positive when sampling_type is set to :bernoulli_sample!. Default is 0.5.\nirrational: Irrational number whose decimals decide the sign of weight. Default is pi.\nstart: Which place after the decimal point the counting starts for the irrational sign counting. Default is 1.\nstrides: number of strides for assigning negative value to a weight. It can be an integer or an array. Default is 2.\n\nExamples\n\njulia> reservoir_matrix = forward_connection(5, 5)\n5×5 Matrix{Float32}:\n 0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n 0.1  0.0  0.0  0.0  0.0\n 0.0  0.1  0.0  0.0  0.0\n 0.0  0.0  0.1  0.0  0.0\n\njulia> reservoir_matrix = forward_connection(5, 5; weight=0.5)\n5×5 Matrix{Float32}:\n 0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n 0.5  0.0  0.0  0.0  0.0\n 0.0  0.5  0.0  0.0  0.0\n 0.0  0.0  0.5  0.0  0.0\n\n\n\n\n\n","category":"function"},{"location":"api/inits/forward_connection/#References","page":"forward_connection","title":"References","text":"","category":"section"},{"location":"api/inits/forward_connection/","page":"forward_connection","title":"forward_connection","text":"Elsarraj, D.; Qisi, M. A.; Rodan, A.; Obeid, N.; Sharieh, A. and Faris, H. (2019). Demystifying echo state network with deterministic simple topologies. International Journal of Computational Science and Engineering 19, 407–417.\n\n\n\n","category":"page"},{"location":"api/inits/simple_cycle/#simple_cycle","page":"simple_cycle","title":"simple_cycle","text":"","category":"section"},{"location":"api/inits/simple_cycle/#ReservoirComputing.simple_cycle","page":"simple_cycle","title":"ReservoirComputing.simple_cycle","text":"simple_cycle([rng], [T], dims...;\n    weight=0.1, return_sparse=false,\n    kwargs...)\n\nCreate a simple cycle reservoir (Rodan and Tino, 2011).\n\nArguments\n\nrng: Random number generator. Default is Utils.default_rng() from WeightInitializers.\nT: Type of the elements in the reservoir matrix. Default is Float32.\ndims: Dimensions of the reservoir matrix.\n\nKeyword arguments\n\nweight: Weight of the connections in the reservoir matrix. This can be provided as a single value or an array. In case it is provided as an array please make sure that the length of the array matches the length of the cycle you want to populate. Default is 0.1.\nreturn_sparse: flag for returning a sparse matrix. Default is false.\nsampling_type: Sampling that decides the distribution of weight negative numbers. If set to :no_sample the sign is unchanged. If set to :bernoulli_sample! then each weight can be positive with a probability set by positive_prob. If set to :irrational_sample! the weight is negative if the decimal number of the irrational number chosen is odd. If set to :regular_sample!, each weight will be assigned a negative sign after the chosen strides. strides can be a single number or an array. Default is :no_sample.\npositive_prob: probability of the weight being positive when sampling_type is set to :bernoulli_sample!. Default is 0.5.\nirrational: Irrational number whose decimals decide the sign of weight. Default is pi.\nstart: Which place after the decimal point the counting starts for the irrational sign counting. Default is 1.\nstrides: number of strides for assigning negative value to a weight. It can be an integer or an array. Default is 2.\n\nExamples\n\njulia> res_matrix = simple_cycle(5, 5)\n5×5 Matrix{Float32}:\n 0.0  0.0  0.0  0.0  0.1\n 0.1  0.0  0.0  0.0  0.0\n 0.0  0.1  0.0  0.0  0.0\n 0.0  0.0  0.1  0.0  0.0\n 0.0  0.0  0.0  0.1  0.0\n\njulia> res_matrix = simple_cycle(5, 5; weight = 11)\n5×5 Matrix{Float32}:\n  0.0   0.0   0.0   0.0  11.0\n 11.0   0.0   0.0   0.0   0.0\n  0.0  11.0   0.0   0.0   0.0\n  0.0   0.0  11.0   0.0   0.0\n  0.0   0.0   0.0  11.0   0.0\n\n\n\n\n\n","category":"function"},{"location":"api/inits/simple_cycle/#References","page":"simple_cycle","title":"References","text":"","category":"section"},{"location":"api/inits/simple_cycle/","page":"simple_cycle","title":"simple_cycle","text":"Rodan, A. and Tino, P. (2011). Minimum Complexity Echo State Network. IEEE Transactions on Neural Networks 22, 131–144.\n\n\n\n","category":"page"},{"location":"#ReservoirComputing.jl","page":"ReservoirComputing.jl","title":"ReservoirComputing.jl","text":"","category":"section"},{"location":"","page":"ReservoirComputing.jl","title":"ReservoirComputing.jl","text":"ReservoirComputing.jl is a versatile and user-friendly Julia package designed for the implementation of Reservoir Computing models, such as Echo State Networks (ESNs). Reservoir Computing expands the input data into a higher-dimensional space, leveraging regression techniques for effective model training. This approach can be thought as a kernel method with an explicit kernel trick.","category":"page"},{"location":"","page":"ReservoirComputing.jl","title":"ReservoirComputing.jl","text":"info: Introductory material\nThis library assumes some basic knowledge of Reservoir Computing. For a good introduction, we suggest the following papers: the first two are the seminal papers about ESN and liquid state machines, the others are in-depth review papers that should cover all the needed information. For the majority of the algorithms implemented in this library we cited in the documentation the original work introducing them. If you ever are in doubt about a method or a function just type ? function in the Julia REPL to read the relevant notes.Jaeger, Herbert: The “echo state” approach to analyzing and training   recurrent neural networks-with an erratum note.\nMaass W, Natschläger T, Markram H: Real-time computing without   stable states: a new framework for neural computation based on   perturbations.\nLukoševičius, Mantas: A practical guide to applying echo state networks.   Neural networks: Tricks of the trade.\nLukoševičius, Mantas, and Herbert Jaeger: Reservoir computing approaches   to recurrent neural network training.","category":"page"},{"location":"","page":"ReservoirComputing.jl","title":"ReservoirComputing.jl","text":"info: Performance tip\nFor faster computations on the CPU it is suggested to add using MKL to the script. For clarity's sake this library will not be indicated  under every example in the documentation.","category":"page"},{"location":"#Installation","page":"ReservoirComputing.jl","title":"Installation","text":"","category":"section"},{"location":"","page":"ReservoirComputing.jl","title":"ReservoirComputing.jl","text":"To install ReservoirComputing.jl, ensure you have Julia version 1.10 or higher. Follow these steps:","category":"page"},{"location":"","page":"ReservoirComputing.jl","title":"ReservoirComputing.jl","text":"1. Open the Julia command line.\n2. Enter the Pkg REPL mode by pressing ].\n3. Type `add ReservoirComputing` and press Enter.","category":"page"},{"location":"","page":"ReservoirComputing.jl","title":"ReservoirComputing.jl","text":"For a more customized installation or to contribute to the package, consider cloning the repository:","category":"page"},{"location":"","page":"ReservoirComputing.jl","title":"ReservoirComputing.jl","text":"using Pkg\nPkg.clone(\"https://github.com/SciML/ReservoirComputing.jl.git\")","category":"page"},{"location":"","page":"ReservoirComputing.jl","title":"ReservoirComputing.jl","text":"or dev the package.","category":"page"},{"location":"#Features-Overview","page":"ReservoirComputing.jl","title":"Features Overview","text":"","category":"section"},{"location":"","page":"ReservoirComputing.jl","title":"ReservoirComputing.jl","text":"Multiple Training Algorithms: Supports Ridge Regression, Linear Models,   and LIBSVM regression methods for Reservoir Computing models.\nDiverse Prediction Methods: Offers both generative and predictive methods   for Reservoir Computing predictions.\nModifiable Training and Prediction: Allows modifications in Reservoir   Computing states, such as state extension, padding, and combination methods.\nNon-linear Algorithm Options: Includes options for non-linear   modifications in algorithms.\nEcho State Networks (ESNs): Features various input layers, reservoirs,   and methods for driving ESN reservoir states.\nCellular Automata-Based Reservoir Computing: Introduces models based   on one-dimensional Cellular Automata for Reservoir Computing.","category":"page"},{"location":"#Contributing","page":"ReservoirComputing.jl","title":"Contributing","text":"","category":"section"},{"location":"","page":"ReservoirComputing.jl","title":"ReservoirComputing.jl","text":"Contributions to ReservoirComputing.jl are highly encouraged and appreciated. Whether it's through implementing new RC model variations, enhancing documentation, adding examples, or any improvement, your contribution is valuable. We welcome posts of relevant papers or ideas in the issues section. For deeper insights into the library's functionality, the API section in the documentation is a great resource. For any queries not suited for issues, please reach out to the lead developers via Slack or email.","category":"page"},{"location":"#Citing","page":"ReservoirComputing.jl","title":"Citing","text":"","category":"section"},{"location":"","page":"ReservoirComputing.jl","title":"ReservoirComputing.jl","text":"If you use ReservoirComputing.jl in your work, we kindly ask you to cite it. Here is the BibTeX entry for your convenience:","category":"page"},{"location":"","page":"ReservoirComputing.jl","title":"ReservoirComputing.jl","text":"@article{JMLR:v23:22-0611,\n  author  = {Francesco Martinuzzi and Chris Rackauckas and Anas Abdelrehim and Miguel D. Mahecha and Karin Mora},\n  title   = {ReservoirComputing.jl: An Efficient and Modular Library for Reservoir Computing Models},\n  journal = {Journal of Machine Learning Research},\n  year    = {2022},\n  volume  = {23},\n  number  = {288},\n  pages   = {1--8},\n  url     = {http://jmlr.org/papers/v23/22-0611.html}\n}","category":"page"},{"location":"#Reproducibility","page":"ReservoirComputing.jl","title":"Reproducibility","text":"","category":"section"},{"location":"","page":"ReservoirComputing.jl","title":"ReservoirComputing.jl","text":"<details><summary>The documentation of this SciML package was built using these direct dependencies,</summary>","category":"page"},{"location":"","page":"ReservoirComputing.jl","title":"ReservoirComputing.jl","text":"using Pkg # hide\nPkg.status() # hide","category":"page"},{"location":"","page":"ReservoirComputing.jl","title":"ReservoirComputing.jl","text":"</details>","category":"page"},{"location":"","page":"ReservoirComputing.jl","title":"ReservoirComputing.jl","text":"<details><summary>and using this machine and Julia version.</summary>","category":"page"},{"location":"","page":"ReservoirComputing.jl","title":"ReservoirComputing.jl","text":"using InteractiveUtils # hide\nversioninfo() # hide","category":"page"},{"location":"","page":"ReservoirComputing.jl","title":"ReservoirComputing.jl","text":"</details>","category":"page"},{"location":"","page":"ReservoirComputing.jl","title":"ReservoirComputing.jl","text":"<details><summary>A more complete overview of all dependencies and their versions is also provided.</summary>","category":"page"},{"location":"","page":"ReservoirComputing.jl","title":"ReservoirComputing.jl","text":"using Pkg # hide\nPkg.status(; mode=PKGMODE_MANIFEST) # hide","category":"page"},{"location":"","page":"ReservoirComputing.jl","title":"ReservoirComputing.jl","text":"</details>","category":"page"},{"location":"","page":"ReservoirComputing.jl","title":"ReservoirComputing.jl","text":"using TOML\nusing Markdown\nversion = TOML.parse(read(\"../../Project.toml\", String))[\"version\"]\nname = TOML.parse(read(\"../../Project.toml\", String))[\"name\"]\nlink_manifest = \"https://github.com/SciML/\" * name * \".jl/tree/gh-pages/v\" * version *\n                \"/assets/Manifest.toml\"\nlink_project = \"https://github.com/SciML/\" * name * \".jl/tree/gh-pages/v\" * version *\n               \"/assets/Project.toml\"\nMarkdown.parse(\"\"\"You can also download the\n[manifest]($link_manifest)\nfile and the\n[project]($link_project)\nfile.\n\"\"\")","category":"page"},{"location":"api/models/#Models","page":"Models","title":"Models","text":"","category":"section"},{"location":"api/models/#Echo-State-Networks","page":"Models","title":"Echo State Networks","text":"","category":"section"},{"location":"api/models/#ReservoirComputing.ESN","page":"Models","title":"ReservoirComputing.ESN","text":"ESN(in_dims, res_dims, out_dims, activation=tanh;\n    leak_coefficient=1.0, init_reservoir=rand_sparse, init_input=scaled_rand,\n    init_bias=zeros32, init_state=randn32, use_bias=false,\n    state_modifiers=(), readout_activation=identity)\n\nEcho State Network (ESN): a reservoir (recurrent) layer followed by an optional sequence of state-modifier layers and a linear readout.\n\nESN composes:\n\na stateful ESNCell (reservoir),\nzero or more state_modifiers applied to the reservoir state, and\na LinearReadout mapping reservoir features to outputs.\n\nEquations\n\nFor input \\mathbf{x}(t) ∈ \\mathbb{R}^{in\\_dims}, reservoir state \\mathbf{h}(t) ∈ \\mathbb{R}^{res\\_dims}, and output \\mathbf{y}(t) ∈ \\mathbb{R}^{out\\_dims}:\n\nbeginaligned\n    tildemathbfh(t) = phileft(mathbfW_inmathbfx(t) +\n        mathbfW_resmathbfh(t-1) + mathbfbright) \n    mathbfh(t) = (1-alpha)mathbfh(t-1) + alphatildemathbfh(t) \n    mathbfz(t) = psileft(mathrmModsbig(mathbfh(t)big)right) \n    mathbfy(t) = rholeft(mathbfW_outmathbfz(t) + mathbfb_outright)\nendaligned\n\nArguments\n\nin_dims: Input dimension.\nres_dims: Reservoir (hidden state) dimension.\nout_dims: Output dimension.\nactivation: Reservoir activation (for ESNCell). Default: tanh.\n\nKeyword arguments\n\nReservoir (passed to ESNCell):\n\nleak_coefficient: Leak rate α ∈ (0,1]. Default: 1.0.\ninit_reservoir: Initializer for W_res. Default: rand_sparse.\ninit_input: Initializer for W_in. Default: scaled_rand.\ninit_bias: Initializer for reservoir bias (used iff use_bias=true). Default: zeros32.\ninit_state: Initializer used when an external state is not provided. Default: randn32.\nuse_bias: Whether the reservoir uses a bias term. Default: false.\n\nComposition:\n\nstate_modifiers: A layer or collection of layers applied to the reservoir state before the readout. Accepts a single layer, an AbstractVector, or a Tuple. Default: empty ().\nreadout_activation: Activation for the linear readout. Default: identity.\n\nInputs\n\nx :: AbstractArray (in_dims, batch)\n\nReturns\n\nOutput y :: (out_dims, batch).\nUpdated layer state (NamedTuple).\n\nParameters\n\nreservoir — parameters of the internal ESNCell, including:\ninput_matrix :: (res_dims × in_dims) — W_in\nreservoir_matrix :: (res_dims × res_dims) — W_res\nbias :: (res_dims,) — present only if use_bias=true\nstates_modifiers — a Tuple with parameters for each modifier layer (may be empty).\nreadout — parameters of LinearReadout, typically:\nweight :: (out_dims × res_dims) — W_out\nbias :: (out_dims,) — b_out (if the readout uses bias)\n\nExact field names for modifiers/readout follow their respective layer definitions.\n\nStates\n\nreservoir — states for the internal ESNCell (e.g. rng used to sample initial hidden states).\nstates_modifiers — a Tuple with states for each modifier layer.\nreadout — states for LinearReadout.\n\n\n\n\n\n","category":"function"},{"location":"api/models/#ReservoirComputing.DeepESN","page":"Models","title":"ReservoirComputing.DeepESN","text":"DeepESN(in_dims, res_dims, out_dims,\n        activation=tanh; depth=2, leak_coefficient=1.0, init_reservoir=rand_sparse,\n        init_input=scaled_rand, init_bias=zeros32, init_state=randn32,\n        use_bias=false, state_modifiers=(), readout_activation=identity)\n\nDeep Echo State Network (DeepESN): a stack of stateful ESNCell layers (optionally with per-layer state modifiers) followed by a linear readout.\n\nDeepESN composes, for L = length(res_dims) layers:\n\na sequence of stateful ESNCell with widths res_dims[ℓ],\nzero or more per-layer state_modifiers[ℓ] applied to the layer's state, and\na final LinearReadout from the last layer's features to the output.\n\nEquations\n\nFor input \\mathbf{x}(t) ∈ \\mathbb{R}^{in\\_dims}, per-layer reservoir states \\mathbf{h}^{(\\ell)}(t) ∈ \\mathbb{R}^{res\\_dims[\\ell]} (\\ell = 1..L), and output \\mathbf{y}(t) ∈ \\mathbb{R}^{out\\_dims}:\n\n```math \\begin{aligned}     \\tilde{\\mathbf{h}}^{(1)}(t) &= \\phi1!\\left(         \\mathbf{W}^{(1)}{in}\\,\\mathbf{x}(t) + \\mathbf{W}^{(1)}{res}\\,\\mathbf{h}^{(1)}(t-1)         + \\mathbf{b}^{(1)}\\right) \\\n    \\mathbf{h}^{(1)}(t) &= (1-\\alpha1)\\,\\mathbf{h}^{(1)}(t-1) + \\alpha1\\,\\tilde{\\mathbf{h}}^{(1)}(t) \\\n    \\mathbf{u}^{(1)}(t) &= \\mathrm{Mods}1!\\big(\\mathbf{h}^{(1)}(t)\\big) \\\n    \\tilde{\\mathbf{h}}^{(\\ell)}(t) &= \\phi\\ell!\\left(         \\mathbf{W}^{(\\ell)}{in}\\,\\mathbf{u}^{(\\ell-1)}(t) +         \\mathbf{W}^{(\\ell)}{res}\\,\\mathbf{h}^{(\\ell)}(t-1) + \\mathbf{b}^{(\\ell)}\\right),         \\quad \\ell=2..L \\\n    \\mathbf{h}^{(\\ell)}(t) &= (1-\\alpha\\ell)\\,\\mathbf{h}^{(\\ell)}(t-1) + \\alpha\\ell\\,\\tilde{\\mathbf{h}}^{(\\ell)}(t),         \\quad \\ell=2..L \\\n    \\mathbf{u}^{(\\ell)}(t) &= \\mathrm{Mods}\\ell!\\big(\\mathbf{h}^{(\\ell)}(t)\\big), \\quad \\ell=2..L \\\n    \\mathbf{y}(t) &= \\rho!\\left(\\mathbf{W}{out}\\,\\mathbf{u}^{(L)}(t) + \\mathbf{b}{out}\\right) \\end{aligned}\n\nWhere\n\n\\mathbf{x}(t) ∈ ℝ^{in_dims × batch} — input at time t.\n\\mathbf{h}^{(\\ell)}(t) ∈ ℝ^{res_dims[ℓ] × batch} — hidden state of layer ℓ.\n\\tilde{\\mathbf{h}}^{(\\ell)}(t) — candidate state before leaky mixing.\n\\mathbf{u}^{(\\ell)}(t) — features after applying the ℓ-th state_modifiers (identity if none).\n\\mathbf{y}(t) ∈ ℝ^{out_dims × batch} — network output.\n\\mathbf{W}^{(\\ell)}_{in} ∈ ℝ^{res_dims[ℓ] × in\\_size[ℓ]} — input matrix at layer ℓ (in_size[1]=in_dims, in_size[ℓ]=res_dims[ℓ-1] for ℓ>1).\n\\mathbf{W}^{(\\ell)}_{res} ∈ ℝ^{res_dims[ℓ] × res_dims[ℓ]} — reservoir matrix at layer ℓ.\n\\mathbf{b}^{(\\ell)} ∈ ℝ^{res_dims[ℓ] × 1} — reservoir bias (broadcast over batch), present iff use_bias[ℓ]=true.\n\\mathbf{W}_{out} ∈ ℝ^{out_dims × res_dims[L]} — readout matrix.\n\\mathbf{b}_{out} ∈ ℝ^{out_dims × 1} — readout bias (if used by the readout).\n\\phi_\\ell — activation of layer ℓ (activation[ℓ], default tanh).\n\\alpha_\\ell ∈ (0,1] — leak coefficient of layer ℓ (leak_coefficient[ℓ]).\n\\mathrm{Mods}_\\ell(·) — composition of modifiers for layer ℓ (may be empty).\n\\rho — readout activation (readout_activation, default identity).\n\nArguments\n\nin_dims: Input dimension.\nres_dims: Vector of reservoir (hidden) dimensions per layer; its length sets the depth L.\nout_dims: Output dimension.\nactivation: Reservoir activation(s). Either a single function (broadcast to all layers) or a vector/tuple of length L. Default: tanh.\n\nKeyword arguments\n\nPer-layer reservoir options (passed to each ESNCell):\n\nleak_coefficient: Leak rate(s) α_ℓ ∈ (0,1]. Scalar or length-L collection. Default: 1.0.\ninit_reservoir: Initializer(s) for W_res^{(ℓ)}. Scalar or length-L. Default: rand_sparse.\ninit_input: Initializer(s) for W_in^{(ℓ)}. Scalar or length-L. Default: scaled_rand.\ninit_bias: Initializer(s) for reservoir bias (used iff use_bias[ℓ]=true). Scalar or length-L. Default: zeros32.\ninit_state: Initializer(s) used when an external state is not provided. Scalar or length-L. Default: randn32.\nuse_bias: Whether each reservoir uses a bias term. Boolean scalar or length-L. Default: false.\ndepth: Depth of the DeepESN. If the reservoir size is given as a number instead of a vector, this parameter controls the depth of the model. Default is 2.\n\nComposition:\n\nstate_modifiers: Per-layer modifier(s) applied to each layer’s state before it feeds into the next layer (and the readout for the last layer). Accepts nothing, a single layer, a vector/tuple of length L, or per-layer collections. Defaults to no modifiers.\nreadout_activation: Activation for the final linear readout. Default: identity.\n\nInputs\n\nx :: AbstractArray (in_dims, batch)\n\nReturns\n\nOutput y :: (out_dims, batch).\nUpdated layer state (NamedTuple) containing states for all cells, modifiers, and readout.\n\nParameters\n\ncells :: NTuple{L,NamedTuple} — parameters for each ESNCell, including:\ninput_matrix :: (res_dims[ℓ] × in_size[ℓ]) — W_in^{(ℓ)}\nreservoir_matrix :: (res_dims[ℓ] × res_dims[ℓ]) — W_res^{(ℓ)}\nbias :: (res_dims[ℓ],) — present only if use_bias[ℓ]=true\nstates_modifiers :: NTuple{L,Tuple} — per-layer tuples of modifier parameters (empty tuples if none).\nreadout — parameters of LinearReadout, typically:\nweight :: (out_dims × res_dims[L]) — W_out\nbias :: (out_dims,) — b_out (if the readout uses bias)\n\nExact field names for modifiers/readout follow their respective layer definitions.\n\nStates\n\ncells :: NTuple{L,NamedTuple} — states for each ESNCell.\nstates_modifiers :: NTuple{L,Tuple} — per-layer tuples of modifier states.\nreadout — states for LinearReadout.\n\n\n\n\n\n","category":"type"},{"location":"api/models/#ReservoirComputing.HybridESN","page":"Models","title":"ReservoirComputing.HybridESN","text":"HybridESN(km, km_dims, in_dims, res_dims, out_dims, [activation];\n    state_modifiers=(), readout_activation=identity,\n    include_collect=true, kwargs...)\n\nHybrid Echo State Network (HybridESN): an Echo State Network augmented with a knowledge model whose outputs are concatenated to the ESN’s input and used throughout the reservoir and readout computations.\n\nHybridESN composes:\n\na knowledge model km producing auxiliary features from the input,\na stateful ESNCell that receives the concatenated input [km(x(t)); x(t)],\nzero or more state_modifiers applied to the reservoir state, and\na LinearReadout mapping the combined features [km(x(t)); h*(t)] to the output.\n\nArguments\n\nkm: Knowledge model applied to the input (e.g. a physical model, neural   submodule, or differentiable function). May be a WrappedFunction or any   callable layer.\nkm_dims: Output dimension of the knowledge model km.\nin_dims: Input dimension.\nres_dims: Reservoir (hidden state) dimension.\nout_dims: Output dimension.\nactivation: Reservoir activation (for ESNCell). Default: tanh.\n\nKeyword arguments\n\nleak_coefficient: Leak rate α ∈ (0,1]. Default: 1.0.\ninit_reservoir: Initializer for W_res. Default: rand_sparse.\ninit_input: Initializer for W_in. Default: scaled_rand.\ninit_bias: Initializer for reservoir bias (used if use_bias=true).   Default: zeros32.\ninit_state: Initializer used when an external state is not provided.   Default: randn32.\nuse_bias: Whether the reservoir uses a bias term. Default: false.\nstate_modifiers: A layer or collection of layers applied to the reservoir   state before the readout. Accepts a single layer, an AbstractVector, or a   Tuple. Default: empty ().\nreadout_activation: Activation for the linear readout. Default: identity.\ninclude_collect: Whether the readout should include collection mode.   Default: true.\n\nInputs\n\nx :: AbstractArray (in_dims, batch)\n\nReturns\n\nOutput y :: (out_dims, batch).\nUpdated layer state (NamedTuple).\n\nParameters\n\nknowledge_model — parameters of the knowledge model km.\ncell — parameters of the internal ESNCell, including:\ninput_matrix :: (res_dims × (in_dims + km_dims)) — W_in\nreservoir_matrix :: (res_dims × res_dims) — W_res\nbias :: (res_dims,) — present only if use_bias=true\nstates_modifiers — a Tuple with parameters for each modifier layer (may be empty).\nreadout — parameters of LinearReadout, typically:\nweight :: (out_dims × (res_dims + km_dims)) — W_out\nbias :: (out_dims,) — b_out (if the readout uses bias)\n\nExact field names for modifiers/readout follow their respective layer definitions.\n\nStates\n\nCreated by initialstates(rng, hesn):\n\nknowledge_model — states for the internal knowledge model.\ncell — states for the internal ESNCell.\nstates_modifiers — a Tuple with states for each modifier layer.\nreadout — states for LinearReadout.\n\n\n\n\n\n","category":"type"},{"location":"api/models/#Utilities","page":"Models","title":"Utilities","text":"","category":"section"},{"location":"api/models/#ReservoirComputing.resetcarry!","page":"Models","title":"ReservoirComputing.resetcarry!","text":"resetcarry!(rng, rc::ReservoirComputer, st; init_carry=nothing)\nresetcarry!(rng, rc::ReservoirComputer, ps, st; init_carry=nothing)\n\nReset (or set) the hidden-state carry of a model in the echo state network family.\n\nIf an existing carry is present in st.cell.carry, its leading dimension is used to infer the state size. Otherwise the reservoir output size is taken from rc.reservoir.cell.out_dims. When init_carry=nothing, the carry is cleared; the initializer from the struct construction will then be used. When a function is provided, it is called to create a new initial hidden state.\n\nArguments\n\nrng: Random number generator (used if a new carry is sampled/created).\nrc: A reservoir computing network model.\nst: Current model states.\nps: Optional model parameters. Returned unchanged.\n\nKeyword arguments\n\ninit_carry: Controls the initialization of the new carry.\nnothing (default): remove/clear the carry (forces the cell to reinitialize from its own init_state on next use).\nf: a function following standard from WeightInitializers.jl\n\nReturns\n\nresetcarry!(rng, rc, st; ...) -> st′: Updated states with st′.cell.carry set to nothing or (h0,).\nresetcarry!(rng, rc, ps, st; ...) -> (ps, st′): Same as above, but also returns the unchanged ps for convenience.\n\n\n\n\n\n","category":"function"},{"location":"api/models/#Reservoir-Computing-with-Cellular-Automata","page":"Models","title":"Reservoir Computing with Cellular Automata","text":"","category":"section"},{"location":"api/models/#ReservoirComputing.RECA","page":"Models","title":"ReservoirComputing.RECA","text":"RECA(in_dims, out_dims, automaton;\n    input_encoding=RandomMapping(),\n    generations=8, state_modifiers=(),\n    readout_activation=identity)\n\nConstruct a cellular–automata reservoir model.\n\nAt each time step the input vector is randomly embedded into a Cellular Automaton (CA) lattice, the CA is evolved for generations steps, and the flattened evolution (excluding the initial row) is used as the reservoir state. A linear LinearReadout maps these features to out_dims.\n\nnote: Note\nThis constructor is only available when the CellularAutomata.jl package is loaded.\n\nArguments\n\nin_dims: Number of input features (rows of training data).\nout_dims: Number of output features (rows of target data).\nautomaton: A CA rule/object from CellularAutomata.jl (e.g. DCA(90), DCA(30), …).\n\nKeyword Arguments\n\ninput_encoding: Random embedding spec with fields permutations and expansion_size. Default is RandomMapping().\ngenerations: Number of CA generations to evolve per time step. Default is 8.\nstate_modifiers: Optional tuple/vector of additional layers applied after the CA cell and before the readout (e.g., NLAT2(), Pad(1.0), custom transforms, etc.). Functions are wrapped automatically. Default is none.\nreadout_activation: Activation applied by the readout Default is identity.\n\n\n\n\n\n","category":"function"},{"location":"api/models/","page":"Models","title":"Models","text":"The input encodings are the equivalent of the input matrices of the ESNs. These are the available encodings:","category":"page"},{"location":"api/models/#ReservoirComputing.RandomMapping","page":"Models","title":"ReservoirComputing.RandomMapping","text":"RandomMapping(permutations, expansion_size)\nRandomMapping(permutations; expansion_size=40)\nRandomMapping(;permutations=8, expansion_size=40)\n\nSpecify the random input embedding used by the Cellular Automata reservoir. Each time step, the input vector of length in_dims is randomly placed into a larger 1D lattice of length expansion_size, and this is repeated for permutations independent lattices (blocks). The concatenation of these blocks forms the CA initial condition of length: ca_size = expansion_size * permutations. The detail of this implementation can be found in (Nichele and Molund, 2017).\n\nArguments\n\npermutations: number of independent random maps (blocks). Larger values increase feature diversity and ca_size proportionally.\nexpansion_size: width of each block (the size of a single CA lattice). Larger values increase the spatial resolution and both ca_size and states_size.\n\nUsage\n\nThis is a configuration object; it does not perform the mapping by itself. Create the concrete tables with create_encoding and pass them to RECACell:\n\nusing ReservoirComputing, CellularAutomata, Random\n\nin_dims = 4\ngenerations = 8\nmapping = RandomMapping(permutations = 8, expansion_size = 40)\n\nenc = ReservoirComputing.create_encoding(mapping, in_dims, generations)  # → RandomMaps\ncell = RECACell(DCA(90), enc)\n\nrc = ReservoirChain(\n    StatefulLayer(cell),\n    LinearReadout(enc.states_size => in_dims; include_collect = true)\n)\n\nOr let RECA do this for you:\n\nrc = RECA(in_dims = 4, out_dims = 4, DCA(90);\n    input_encoding = RandomMapping(permutations = 8, expansion_size = 40),\n    generations = 8)\n\n\n\n\n\n","category":"type"},{"location":"references/#References","page":"References","title":"References","text":"","category":"section"},{"location":"references/","page":"References","title":"References","text":"Barbosa, W. A.; Griffith, A.; Rowlands, G. E.; Govia, L. C.; Ribeill, G. J.; Nguyen, M.-H.; Ohki, T. A. and Gauthier, D. J. (2021). Symmetry-aware reservoir computing. Physical Review E 104.\n\n\n\nChattopadhyay, A.; Hassanzadeh, P. and Subramanian, D. (2020). Data-driven predictions of a multiscale Lorenz 96 chaotic system using machine-learning methods: reservoir computing,  artificial neural network,  and long short-term memory network. Nonlinear Processes in Geophysics 27, 373–389.\n\n\n\nElsarraj, D.; Qisi, M. A.; Rodan, A.; Obeid, N.; Sharieh, A. and Faris, H. (2019). Demystifying echo state network with deterministic simple topologies. International Journal of Computational Science and Engineering 19, 407–417.\n\n\n\nFu, J.; Li, G.; Tang, J.; Xia, L.; Wang, L. and Duan, S. (2023). A double-cycle echo state network topology for time series prediction. Chaos: An Interdisciplinary Journal of Nonlinear Science 33.\n\n\n\nGallicchio, C. and Micheli, A. (2017). Deep echo state network (deepesn): A brief survey, arXiv preprint arXiv:1712.04323.\n\n\n\nGriffith, A.; Pomerance, A. and Gauthier, D. J. (2019). Forecasting chaotic systems with very low connectivity reservoir computers. Chaos: An Interdisciplinary Journal of Nonlinear Science 29.\n\n\n\nHerteux, J. and Räth, C. (2020). Breaking symmetries of the reservoir equations in echo state networks. Chaos: An Interdisciplinary Journal of Nonlinear Science 30.\n\n\n\nLu, Z.; Pathak, J.; Hunt, B.; Girvan, M.; Brockett, R. and Ott, E. (2017). Reservoir observers: Model-free inference of unmeasured variables in chaotic systems. Chaos: An Interdisciplinary Journal of Nonlinear Science 27.\n\n\n\nLukoševičius, M. (2012). A Practical Guide to Applying Echo State Networks. In: Neural Networks: Tricks of the Trade (Springer Berlin Heidelberg); pp. 659–686.\n\n\n\nMa, H.; Prosperino, D.; Haluszczynski, A. and Räth, C. (2023). Efficient forecasting of chaotic systems with block-diagonal and binary reservoir computing. Chaos: An Interdisciplinary Journal of Nonlinear Science 33.\n\n\n\nMargem, M. and Yilmaz, O. (2017). An experimental study on cellular automata reservoir in pathological sequence learning tasks.\n\n\n\nNichele, S. and Molund, A. (2017). Deep reservoir computing using cellular automata, arXiv preprint arXiv:1703.02806.\n\n\n\nPathak, J.; Lu, Z.; Hunt, B. R.; Girvan, M. and Ott, E. (2017). Using machine learning to replicate chaotic attractors and calculate Lyapunov exponents from data. Chaos: An Interdisciplinary Journal of Nonlinear Science 27.\n\n\n\nPathak, J.; Wikner, A.; Fussell, R.; Chandra, S.; Hunt, B. R.; Girvan, M. and Ott, E. (2018). Hybrid forecasting of chaotic processes: Using machine learning in conjunction with a knowledge-based model. Chaos: An Interdisciplinary Journal of Nonlinear Science 28.\n\n\n\nRodan, A. and Tino, P. (2011). Minimum Complexity Echo State Network. IEEE Transactions on Neural Networks 22, 131–144.\n\n\n\nRodan, A. and Tiňo, P. (2012). Simple Deterministically Constructed Cycle Reservoirs with Regular Jumps. Neural Computation 24, 1822–1852.\n\n\n\nViehweg, J.; Poll, C. and Mäder, P. (2025). Deterministic Reservoir Computing for Chaotic Time Series Prediction, arXiv preprint arXiv:2501.15615.\n\n\n\nWang, H.; Liu, Y.; Lu, P.; Luo, Y.; Wang, D. and Xu, X. (2022). Echo state network with logistic mapping and bias dropout for time series prediction. Neurocomputing 489, 196–210.\n\n\n\nXie, M.; Wang, Q. and Yu, S. (2024). Time Series Prediction of ESN Based on Chebyshev Mapping and Strongly Connected Topology. Neural Processing Letters 56.\n\n\n\nYang, C.; Qiao, J.; Han, H. and Wang, L. (2018). Design of polynomial echo state networks for time series prediction. Neurocomputing 290, 148–160.\n\n\n\nYilmaz, O. (2014). Reservoir computing using cellular automata, arXiv preprint arXiv:1410.0162.\n\n\n\n","category":"page"},{"location":"api/inits/simple_cycle!/#simple_cycle!","page":"simple_cycle!","title":"simple_cycle!","text":"","category":"section"},{"location":"api/inits/simple_cycle!/#ReservoirComputing.simple_cycle!","page":"simple_cycle!","title":"ReservoirComputing.simple_cycle!","text":"simple_cycle!([rng], reservoir_matrix, weight;\n    sampling_type=:no_sample, irrational=pi, start=1,\n    p=0.5)\n\nAdds a simple cycle in the reservoir_matrix, with given weight. The weight can be a single number or an array.\n\nArguments\n\nrng: Random number generator. Default is Utils.default_rng() from WeightInitializers.\nreservoir_matrix: matrix to be changed.\nweight: weight to add as a simple cycle. Can be either a single number or an array.\n\nKeyword arguments\n\nsampling_type: Sampling that decides the distribution of weight negative numbers. If set to :no_sample the sign is unchanged. If set to :bernoulli_sample! then each weight can be positive with a probability set by positive_prob. If set to :irrational_sample! the weight is negative if the decimal number of the irrational number chosen is odd. If set to :regular_sample!, each weight will be assigned a negative sign after the chosen strides. strides can be a single number or an array. Default is :no_sample.\npositive_prob: probability of the weight being positive when sampling_type is set to :bernoulli_sample!. Default is 0.5.\nirrational: Irrational number whose decimals decide the sign of weight. Default is pi.\nstart: Which place after the decimal point the counting starts for the irrational sign counting. Default is 1.\nstrides: number of strides for assigning negative value to a weight. It can be an integer or an array. Default is 2.\n\nExamples\n\njulia> matrix = zeros(Float32, 5, 5)\n5×5 Matrix{Float32}:\n 0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n\njulia> simple_cycle!(matrix, 1.0; sampling_type = :irrational_sample!)\n5×5 Matrix{Float32}:\n  0.0  0.0   0.0   0.0  -1.0\n -1.0  0.0   0.0   0.0   0.0\n  0.0  1.0   0.0   0.0   0.0\n  0.0  0.0  -1.0   0.0   0.0\n  0.0  0.0   0.0  -1.0   0.0\n\n\n\n\n\n","category":"function"},{"location":"api/inits/selfloop_forward_connection/#selfloop*forward*connection","page":"selfloopforwardconnection","title":"selfloopforwardconnection","text":"","category":"section"},{"location":"api/inits/selfloop_forward_connection/#ReservoirComputing.selfloop_forward_connection","page":"selfloopforwardconnection","title":"ReservoirComputing.selfloop_forward_connection","text":"selfloop_forward_connection([rng], [T], dims...;\n    weight=0.1, selfloop_weight=0.1,\n    return_sparse=false, selfloop_kwargs=(),\n    delay_kwargs=())\n\nCreates a reservoir based on a forward connection of weights between even nodes with the addition of self loops (Elsarraj et al., 2019).\n\nThis architecture is referred to as TP4 in the original paper.\n\nEquations\n\nW_ij =\nbegincases\n    ll  textif  i = j text for  i = 1 dots N \n    r  textif  j = i - 2 text for  i = 3 dots N \n    0  textotherwise\nendcases\n\nArguments\n\nrng: Random number generator. Default is Utils.default_rng() from WeightInitializers.\nT: Type of the elements in the reservoir matrix. Default is Float32.\ndims: Dimensions of the reservoir matrix.\n\nKeyword arguments\n\nweight: Weight of the cycle connections in the reservoir matrix. This can be provided as a single value or an array. In case it is provided as an array please make sure that the length of the array matches the length of the cycle you want to populate. Default is 0.1.\nselfloop_weight: Weight of the self loops in the reservoir matrix. This can be provided as a single value or an array. In case it is provided as an array please make sure that the length of the array matches the length of the diagonal you want to populate. Default is 0.1.\nreturn_sparse: flag for returning a sparse matrix. Default is false.\ndelay_kwargs and selfloop_kwargs: named tuples that control the kwargs for the delay line weight and self loop weights respectively. The kwargs are as follows:\nsampling_type: Sampling that decides the distribution of weight negative numbers.   If set to :no_sample the sign is unchanged. If set to :bernoulli_sample! then each   weight can be positive with a probability set by positive_prob. If set to   :irrational_sample! the weight is negative if the decimal number of the   irrational number chosen is odd. If set to :regular_sample!, each weight will be   assigned a negative sign after the chosen strides. strides can be a single   number or an array. Default is :no_sample.\npositive_prob: probability of the weight being positive when sampling_type is set to :bernoulli_sample!. Default is 0.5.\nirrational: Irrational number whose decimals decide the sign of weight. Default is pi.\nstart: Which place after the decimal point the counting starts for the irrational sign counting. Default is 1.\nstrides: number of strides for assigning negative value to a weight. It can be an integer or an array. Default is 2.\n\nExamples\n\njulia> reservoir_matrix = selfloop_forward_connection(5, 5)\n5×5 Matrix{Float32}:\n 0.1  0.0  0.0  0.0  0.0\n 0.0  0.1  0.0  0.0  0.0\n 0.1  0.0  0.1  0.0  0.0\n 0.0  0.1  0.0  0.1  0.0\n 0.0  0.0  0.1  0.0  0.1\n\njulia> reservoir_matrix = selfloop_forward_connection(5, 5; weight=0.5)\n5×5 Matrix{Float32}:\n 0.1  0.0  0.0  0.0  0.0\n 0.0  0.1  0.0  0.0  0.0\n 0.5  0.0  0.1  0.0  0.0\n 0.0  0.5  0.0  0.1  0.0\n 0.0  0.0  0.5  0.0  0.1\n\n\n\n\n\n","category":"function"},{"location":"api/inits/selfloop_forward_connection/#References","page":"selfloopforwardconnection","title":"References","text":"","category":"section"},{"location":"api/inits/selfloop_forward_connection/","page":"selfloopforwardconnection","title":"selfloopforwardconnection","text":"Elsarraj, D.; Qisi, M. A.; Rodan, A.; Obeid, N.; Sharieh, A. and Faris, H. (2019). Demystifying echo state network with deterministic simple topologies. International Journal of Computational Science and Engineering 19, 407–417.\n\n\n\n","category":"page"},{"location":"api/predict/#Predict","page":"Predict","title":"Predict","text":"","category":"section"},{"location":"api/predict/#ReservoirComputing.predict","page":"Predict","title":"ReservoirComputing.predict","text":"predict(rc, steps::Integer, ps, st; initialdata=nothing)\npredict(rc, data::AbstractMatrix, ps, st)\n\nRun the model either in (1) closed-loop (auto-regressive) mode for a fixed number of steps, or in (2) teacher-forced (point-by-point) mode over a given input sequence.\n\n1) Auto-regressive rollout\n\nBehavior\n\nRolls the model forward for steps time steps.\nAt each step, the model’s output becomes the next input.\n\nArguments\n\nrc: The reservoir chain / model.\nsteps: Number of time steps to generate.\nps: Model parameters.\nst: Model states.\n\nKeyword Arguments\n\ninitialdata=nothing: Column vector used as the first input. Has to be provided.\n\nReturns\n\noutput: Generated outputs of shape (out_dims, steps).\nst: Final model state after steps steps.\n\n2) Teacher-forced / point-by-point\n\nFeeds each column of data as input; the model state is threaded across time, and an output is produced for each input column.\n\nArguments\n\nrc: The reservoir chain / model.\ndata: Input sequence of shape (in_dims, T) (columns are time).\nps: Model parameters.\nst: Model states.\n\nReturns\n\noutput: Outputs for each input column, shape (out_dims, T).\nst: Updated minal model states.\n\n\n\n\n\n","category":"function"},{"location":"api/inits/cycle_jumps/#cycle_jumps","page":"cycle_jumps","title":"cycle_jumps","text":"","category":"section"},{"location":"api/inits/cycle_jumps/#ReservoirComputing.cycle_jumps","page":"cycle_jumps","title":"ReservoirComputing.cycle_jumps","text":"cycle_jumps([rng], [T], dims...;\n    cycle_weight=0.1, jump_weight=0.1, jump_size=3, return_sparse=false,\n    cycle_kwargs=(), jump_kwargs=())\n\nCreate a cycle jumps reservoir (Rodan and Tiňo, 2012).\n\nArguments\n\nrng: Random number generator. Default is Utils.default_rng() from WeightInitializers.\nT: Type of the elements in the reservoir matrix. Default is Float32.\ndims: Dimensions of the reservoir matrix.\n\nKeyword arguments\n\ncycle_weight:  The weight of cycle connections. This can be provided as a single value or an array. In case it is provided as an array please make sure that the length of the array matches the length of the cycle you want to populate. Default is 0.1.\njump_weight: The weight of jump connections. This can be provided as a single value or an array. In case it is provided as an array please make sure that the length of the array matches the length of the jumps you want to populate. Default is 0.1.\njump_size:  The number of steps between jump connections. Default is 3.\nreturn_sparse: flag for returning a sparse matrix. Default is false.\ncycle_kwargs and jump_kwargs: named tuples that control the kwargs for the cycle and jump weights respectively. The kwargs are as follows:\nsampling_type: Sampling that decides the distribution of weight negative numbers. If set to :no_sample the sign is unchanged. If set to :bernoulli_sample! then each weight can be positive with a probability set by positive_prob. If set to :irrational_sample! the weight is negative if the decimal number of the irrational number chosen is odd. If set to :regular_sample!, each weight will be assigned a negative sign after the chosen strides. strides can be a single number or an array. Default is :no_sample.\npositive_prob: probability of the weight being positive when sampling_type is set to :bernoulli_sample!. Default is 0.5.\nirrational: Irrational number whose decimals decide the sign of weight. Default is pi.\nstart: Which place after the decimal point the counting starts for the irrational sign counting. Default is 1.\nstrides: number of strides for assigning negative value to a weight. It can be an integer or an array. Default is 2.\n\nExamples\n\njulia> res_matrix = cycle_jumps(5, 5)\n5×5 Matrix{Float32}:\n 0.0  0.0  0.0  0.1  0.1\n 0.1  0.0  0.0  0.0  0.0\n 0.0  0.1  0.0  0.0  0.0\n 0.1  0.0  0.1  0.0  0.0\n 0.0  0.0  0.0  0.1  0.0\n\njulia> res_matrix = cycle_jumps(5, 5; jump_size = 2)\n5×5 Matrix{Float32}:\n 0.0  0.0  0.1  0.0  0.1\n 0.1  0.0  0.0  0.0  0.0\n 0.1  0.1  0.0  0.0  0.1\n 0.0  0.0  0.1  0.0  0.0\n 0.0  0.0  0.1  0.1  0.0\n\n\n\n\n\n","category":"function"},{"location":"api/inits/cycle_jumps/#References","page":"cycle_jumps","title":"References","text":"","category":"section"},{"location":"api/inits/cycle_jumps/","page":"cycle_jumps","title":"cycle_jumps","text":"Rodan, A. and Tiňo, P. (2012). Simple Deterministically Constructed Cycle Reservoirs with Regular Jumps. Neural Computation 24, 1822–1852.\n\n\n\n","category":"page"},{"location":"api/inits/scaled_rand/#scaled_rand","page":"scaled_rand","title":"scaled_rand","text":"","category":"section"},{"location":"api/inits/scaled_rand/#ReservoirComputing.scaled_rand","page":"scaled_rand","title":"ReservoirComputing.scaled_rand","text":"scaled_rand([rng], [T], dims...;\n    scaling=0.1)\n\nCreate and return a matrix with random values, uniformly distributed within a range defined by scaling.\n\nArguments\n\nrng: Random number generator. Default is Utils.default_rng() from WeightInitializers.\nT: Type of the elements in the reservoir matrix. Default is Float32.\ndims: Dimensions of the matrix. Should follow res_size x in_size.\n\nKeyword arguments\n\nscaling: A scaling factor to define the range of the uniform distribution. The factor can be passed in three different ways:\nA single number. In this case, the matrix elements will be randomly chosen from the range [-scaling, scaling]. Default option, with a the scaling value set to 0.1.\nA tuple (lower, upper). The values define the range of the distribution.\nA vector. In this case, the columns will be scaled individually by the entries of the vector. The entries can be numbers or tuples, which will mirror the behavior described above.\n\nExamples\n\njulia> res_input = scaled_rand(8, 3)\n8×3 Matrix{Float32}:\n -0.0669356  -0.0292692  -0.0188943\n  0.0159724   0.004071   -0.0737949\n  0.026355   -0.0191563   0.0714962\n -0.0177412   0.0279123   0.0892906\n -0.0184405   0.0567368   0.0190222\n  0.0944272   0.0679244   0.0148647\n -0.0799005  -0.0891089  -0.0444782\n -0.0970182   0.0934286   0.03553\n\njulia> tt = scaled_rand(5, 3, scaling = (0.1, 0.15))\n5×3 Matrix{Float32}:\n  0.13631   0.110929  0.116177\n  0.116299  0.136038  0.119713\n  0.11535   0.144712  0.110029\n  0.127453  0.12657   0.147656\n  0.139446  0.117656  0.104712\n\nExample with vector:\n\njulia> tt = scaled_rand(5, 3, scaling = [0.1, 0.2, 0.3])\n5×3 Matrix{Float32}:\n  0.0452399   -0.112565   -0.105874\n -0.0348047    0.0883044  -0.0634468\n -0.0386004    0.157698   -0.179648\n  0.00981022   0.012559    0.271875\n  0.0577838   -0.0587553  -0.243451\n\njulia> tt = scaled_rand(5, 3, scaling = [(0.1, 0.2), (-0.2, -0.1), (0.3, 0.5)])\n5×3 Matrix{Float32}:\n  0.17262   -0.178141  0.364709\n  0.132598  -0.127924  0.378851\n  0.1307    -0.110575  0.340117\n  0.154905  -0.14686   0.490625\n  0.178892  -0.164689  0.31885\n\n\n\n\n\n","category":"function"},{"location":"api/inits/rand_sparse/#rand_sparse","page":"rand_sparse","title":"rand_sparse","text":"","category":"section"},{"location":"api/inits/rand_sparse/#ReservoirComputing.rand_sparse","page":"rand_sparse","title":"ReservoirComputing.rand_sparse","text":"rand_sparse([rng], [T], dims...;\n    radius=1.0, sparsity=0.1, std=1.0, return_sparse=false)\n\nCreate and return a random sparse reservoir matrix. The matrix will be of size specified by dims, with specified sparsity and scaled spectral radius according to radius.\n\nArguments\n\nrng: Random number generator. Default is Utils.default_rng() from WeightInitializers.\nT: Type of the elements in the reservoir matrix. Default is Float32.\ndims: Dimensions of the reservoir matrix.\n\nKeyword arguments\n\nradius: The desired spectral radius of the reservoir. Defaults to 1.0.\nsparsity: The sparsity level of the reservoir matrix, controlling the fraction of zero elements. Defaults to 0.1.\nreturn_sparse: flag for returning a sparse matrix. Default is false.\n\nExamples\n\njulia> res_matrix = rand_sparse(5, 5; sparsity = 0.5)\n5×5 Matrix{Float32}:\n 0.0        0.0        0.0        0.0      0.0\n 0.0        0.794565   0.0        0.26164  0.0\n 0.0        0.0       -0.931294   0.0      0.553706\n 0.723235  -0.524727   0.0        0.0      0.0\n 1.23723    0.0        0.181824  -1.5478   0.465328\n\n\n\n\n\n","category":"function"},{"location":"api/inits/chebyshev_mapping/#chebyshev_mapping","page":"chebyshev_mapping","title":"chebyshev_mapping","text":"","category":"section"},{"location":"api/inits/chebyshev_mapping/#ReservoirComputing.chebyshev_mapping","page":"chebyshev_mapping","title":"ReservoirComputing.chebyshev_mapping","text":"chebyshev_mapping([rng], [T], dims...;\n    amplitude=one(T), sine_divisor=one(T),\n    chebyshev_parameter=one(T), return_sparse=false)\n\nGenerate a Chebyshev-mapped matrix (Xie et al., 2024). The first row is initialized using a sine function and subsequent rows are iteratively generated via the Chebyshev mapping. The first row is defined as:\n\n    W1 j = textamplitude cdot sin(j cdot pi  (textsine_divisor\n        cdot textn_cols))\n\nfor j = 1, 2, …, ncols (with ncols typically equal to K+1, where K is the number of input layer neurons). Subsequent rows are generated by applying the mapping:\n\n    Wi+1 j = cos( textchebyshev_parameter cdot acos(Wpi j))\n\nArguments\n\nrng: Random number generator. Default is Utils.default_rng() from WeightInitializers.\nT: Type of the elements in the reservoir matrix. Default is Float32.\ndims: Dimensions of the matrix. Should follow res_size x in_size. res_size is assumed to be K+1.\n\nKeyword arguments\n\namplitude: Scaling factor used to initialize the first row. This parameter adjusts the amplitude of the sine function. Default value is one.\nsine_divisor: Divisor applied in the sine function's phase. Default value is one.\nchebyshev_parameter: Control parameter for the Chebyshev mapping in subsequent rows. This parameter influences the distribution of the matrix elements. Default is one.\nreturn_sparse: If true, the function returns the matrix as a sparse matrix. Default is false.\n\nExamples\n\njulia> input_matrix = chebyshev_mapping(10, 3)\n10×3 Matrix{Float32}:\n 0.866025  0.866025   1.22465f-16\n 0.866025  0.866025  -4.37114f-8\n 0.866025  0.866025  -4.37114f-8\n 0.866025  0.866025  -4.37114f-8\n 0.866025  0.866025  -4.37114f-8\n 0.866025  0.866025  -4.37114f-8\n 0.866025  0.866025  -4.37114f-8\n 0.866025  0.866025  -4.37114f-8\n 0.866025  0.866025  -4.37114f-8\n 0.866025  0.866025  -4.37114f-8\n\n\n\n\n\n","category":"function"},{"location":"api/inits/chebyshev_mapping/#References","page":"chebyshev_mapping","title":"References","text":"","category":"section"},{"location":"api/inits/chebyshev_mapping/","page":"chebyshev_mapping","title":"chebyshev_mapping","text":"Xie, M.; Wang, Q. and Yu, S. (2024). Time Series Prediction of ESN Based on Chebyshev Mapping and Strongly Connected Topology. Neural Processing Letters 56.\n\n\n\n","category":"page"},{"location":"api/inits/pseudo_svd/#pseudo_svd","page":"pseudo_svd","title":"pseudo_svd","text":"","category":"section"},{"location":"api/inits/pseudo_svd/#ReservoirComputing.pseudo_svd","page":"pseudo_svd","title":"ReservoirComputing.pseudo_svd","text":"pseudo_svd([rng], [T], dims...;\n    max_value=1.0, sparsity=0.1, sorted=true, reverse_sort=false,\n    return_sparse=false)\n\nReturns an initializer to build a sparse reservoir matrix with the given sparsity by using a pseudo-SVD approach as described in (Yang et al., 2018).\n\nArguments\n\nrng: Random number generator. Default is Utils.default_rng() from WeightInitializers.\nT: Type of the elements in the reservoir matrix. Default is Float32.\ndims: Dimensions of the reservoir matrix.\n\nKeyword arguments\n\nmax_value: The maximum absolute value of elements in the matrix. Default is 1.0\nsparsity: The desired sparsity level of the reservoir matrix. Default is 0.1\nsorted: A boolean indicating whether to sort the singular values before creating the diagonal matrix. Default is true.\nreverse_sort: A boolean indicating whether to reverse the sorted singular values. Default is false.\nreturn_sparse: flag for returning a sparse matrix. Default is false.\nreturn_diag: flag for returning a Diagonal matrix. If both return_diag and return_sparse are set to true priority is given to return_diag. Default is false.\n\nExamples\n\njulia> res_matrix = pseudo_svd(5, 5)\n5×5 Matrix{Float32}:\n 0.306998  0.0       0.0       0.0       0.0\n 0.0       0.325977  0.0       0.0       0.0\n 0.0       0.0       0.549051  0.0       0.0\n 0.0       0.0       0.0       0.726199  0.0\n 0.0       0.0       0.0       0.0       1.0\n\n\n\n\n\n","category":"function"},{"location":"api/inits/pseudo_svd/#References","page":"pseudo_svd","title":"References","text":"","category":"section"},{"location":"api/inits/pseudo_svd/","page":"pseudo_svd","title":"pseudo_svd","text":"Yang, C.; Qiao, J.; Han, H. and Wang, L. (2018). Design of polynomial echo state networks for time series prediction. Neurocomputing 290, 148–160.\n\n\n\n","category":"page"},{"location":"api/inits/selfloop_delayline_backward/#selfloop*delayline*backward","page":"selfloopdelaylinebackward","title":"selfloopdelaylinebackward","text":"","category":"section"},{"location":"api/inits/selfloop_delayline_backward/#ReservoirComputing.selfloop_delayline_backward","page":"selfloopdelaylinebackward","title":"ReservoirComputing.selfloop_delayline_backward","text":"selfloop_delayline_backward([rng], [T], dims...;\n    weight=0.1, selfloop_weight=0.1, fb_weight=0.1,\n    fb_shift=2, return_sparse=false, fb_kwargs=(),\n    selfloop_kwargs=(), delay_kwargs=())\n\nCreates a reservoir based on a delay line with the addition of self loops and backward connections shifted by one (Elsarraj et al., 2019).\n\nThis architecture is referred to as TP3 in the original paper.\n\nEquations\n\nW_ij =\nbegincases\n    ll  textif  i = j text for  i = 1 dots N \n    r  textif  j = i - 1 text for  i = 2 dots N \n    r  textif  j = i - 2 text for  i = 3 dots N \n    0  textotherwise\nendcases\n\nArguments\n\nrng: Random number generator. Default is Utils.default_rng() from WeightInitializers.\nT: Type of the elements in the reservoir matrix. Default is Float32.\ndims: Dimensions of the reservoir matrix.\n\nKeyword arguments\n\nweight: Weight of the cycle connections in the reservoir matrix. This can be provided as a single value or an array. In case it is provided as an array please make sure that the length of the array matches the length of the cycle you want to populate. Default is 0.1.\nselfloop_weight: Weight of the self loops in the reservoir matrix. This can be provided as a single value or an array. In case it is provided as an array please make sure that the length of the array matches the length of the diagonal you want to populate. Default is 0.1.\nfb_weight: Weight of the feedback in the reservoir matrix. This can be provided as a single value or an array. In case it is provided as an array please make sure that the length of the array matches the length of the diagonal you want to populate. Default is 0.1.\nfb_shift: How far the backward connection will be from the diagonal. Default is 2.\nreturn_sparse: flag for returning a sparse matrix. Default is false.\ndelay_kwargs, selfloop_kwargs, and fb_kwargs: named tuples that control the kwargs for the weights generation. The kwargs are as follows:\nsampling_type: Sampling that decides the distribution of weight negative numbers.   If set to :no_sample the sign is unchanged. If set to :bernoulli_sample! then each   weight can be positive with a probability set by positive_prob. If set to   :irrational_sample! the weight is negative if the decimal number of the   irrational number chosen is odd. If set to :regular_sample!, each weight will be   assigned a negative sign after the chosen strides. strides can be a single   number or an array. Default is :no_sample.\npositive_prob: probability of the weight being positive when sampling_type is set to :bernoulli_sample!. Default is 0.5.\nirrational: Irrational number whose decimals decide the sign of weight. Default is pi.\nstart: Which place after the decimal point the counting starts for the irrational sign counting. Default is 1.\nstrides: number of strides for assigning negative value to a weight. It can be an integer or an array. Default is 2.\n\nExamples\n\njulia> reservoir_matrix = selfloop_delayline_backward(5, 5)\n5×5 Matrix{Float32}:\n 0.1  0.0  0.1  0.0  0.0\n 0.1  0.1  0.0  0.1  0.0\n 0.0  0.1  0.1  0.0  0.1\n 0.0  0.0  0.1  0.1  0.0\n 0.0  0.0  0.0  0.1  0.1\n\njulia> reservoir_matrix = selfloop_delayline_backward(5, 5; weight=0.3)\n5×5 Matrix{Float32}:\n 0.1  0.0  0.3  0.0  0.0\n 0.3  0.1  0.0  0.3  0.0\n 0.0  0.3  0.1  0.0  0.3\n 0.0  0.0  0.3  0.1  0.0\n 0.0  0.0  0.0  0.3  0.1\n\n\n\n\n\n","category":"function"},{"location":"api/inits/selfloop_delayline_backward/#References","page":"selfloopdelaylinebackward","title":"References","text":"","category":"section"},{"location":"api/inits/selfloop_delayline_backward/","page":"selfloopdelaylinebackward","title":"selfloopdelaylinebackward","text":"Elsarraj, D.; Qisi, M. A.; Rodan, A.; Obeid, N.; Sharieh, A. and Faris, H. (2019). Demystifying echo state network with deterministic simple topologies. International Journal of Computational Science and Engineering 19, 407–417.\n\n\n\n","category":"page"},{"location":"api/inits/weighted_minimal/#weighted_minimal","page":"weighted_minimal","title":"weighted_minimal","text":"","category":"section"},{"location":"api/inits/weighted_minimal/#ReservoirComputing.weighted_minimal","page":"weighted_minimal","title":"ReservoirComputing.weighted_minimal","text":"weighted_minimal([rng], [T], dims...;\n    weight=0.1, return_sparse=false,\n    sampling_type=:no_sample)\n\nCreate and return a minimal weighted input layer matrix. This initializer generates a weighted input matrix with equal, deterministic elements in the same construction as [weighted_minimal](@ref), inspired by (Lu et al., 2017).\n\nPlease note that this initializer computes its own reservoir size! If the computed reservoir size is different than the provided one it will raise a warning.\n\nArguments\n\nrng: Random number generator. Default is Utils.default_rng() from WeightInitializers.\nT: Type of the elements in the reservoir matrix. Default is Float32.\ndims: Dimensions of the matrix. Should follow res_size x in_size.\n\nKeyword arguments\n\nweight: The value for all the weights in the input matrix. Defaults to 0.1.\nreturn_sparse: flag for returning a sparse matrix. Default is false.\nsampling_type: Sampling that decides the distribution of weight negative numbers. If set to :no_sample the sign is unchanged. If set to :bernoulli_sample! then each weight can be positive with a probability set by positive_prob. If set to :irrational_sample! the weight is negative if the decimal number of the irrational number chosen is odd. If set to :regular_sample!, each weight will be assigned a negative sign after the chosen strides. strides can be a single number or an array. Default is :no_sample.\npositive_prob: probability of the weight being positive when sampling_type is set to :bernoulli_sample!. Default is 0.5.\nirrational: Irrational number whose decimals decide the sign of weight. Default is pi.\nstart: Which place after the decimal point the counting starts for the irrational sign counting. Default is 1.\nstrides: number of strides for assigning negative value to a weight. It can be an integer or an array. Default is 2.\n\nExamples\n\njulia> res_input = weighted_minimal(8, 3)\n┌ Warning: Reservoir size has changed!\n│\n│     Computed reservoir size (6) does not equal the provided reservoir size (8).\n│\n│     Using computed value (6). Make sure to modify the reservoir initializer accordingly.\n│\n└ @ ReservoirComputing ~/.julia/dev/ReservoirComputing/src/esn/esn_inits.jl:159\n6×3 Matrix{Float32}:\n 0.1  0.0  0.0\n 0.1  0.0  0.0\n 0.0  0.1  0.0\n 0.0  0.1  0.0\n 0.0  0.0  0.1\n 0.0  0.0  0.1\n\njulia> res_input = weighted_minimal(9, 3; weight = 0.99)\n9×3 Matrix{Float32}:\n 0.99  0.0   0.0\n 0.99  0.0   0.0\n 0.99  0.0   0.0\n 0.0   0.99  0.0\n 0.0   0.99  0.0\n 0.0   0.99  0.0\n 0.0   0.0   0.99\n 0.0   0.0   0.99\n 0.0   0.0   0.99\n\njulia> res_input = weighted_minimal(9, 3; sampling_type = :bernoulli_sample!)\n9×3 Matrix{Float32}:\n  0.1  -0.0  -0.0\n -0.1  -0.0  -0.0\n  0.1  -0.0   0.0\n -0.0   0.1   0.0\n  0.0   0.1  -0.0\n  0.0   0.1   0.0\n -0.0  -0.0  -0.1\n -0.0  -0.0   0.1\n  0.0  -0.0   0.1\n\n\n\n\n\n","category":"function"},{"location":"api/inits/weighted_minimal/#References","page":"weighted_minimal","title":"References","text":"","category":"section"},{"location":"api/inits/weighted_minimal/","page":"weighted_minimal","title":"weighted_minimal","text":"Lu, Z.; Pathak, J.; Hunt, B.; Girvan, M.; Brockett, R. and Ott, E. (2017). Reservoir observers: Model-free inference of unmeasured variables in chaotic systems. Chaos: An Interdisciplinary Journal of Nonlinear Science 27.\n\n\n\n","category":"page"},{"location":"api/inits/modified_lm/#modified_lm","page":"modified_lm","title":"modified_lm","text":"","category":"section"},{"location":"api/inits/modified_lm/#ReservoirComputing.modified_lm","page":"modified_lm","title":"ReservoirComputing.modified_lm","text":"modified_lm([rng], [T], dims...;\n    factor, amplitude=0.3, sine_divisor=5.9, logistic_parameter=2.35,\n    return_sparse=false)\n\nGenerate a input weight matrix based on the logistic mapping (Viehweg et al., 2025). Thematrix is built so that each input is transformed into a high-dimensional feature space via a recursive logistic map. For each input, a chain of weights is generated as follows:\n\nThe first element of the chain is initialized using a sine function:\n\n      W1j = textamplitude cdot sin( (j cdot pi) \n          (textfactor cdot textn cdot textsine_divisor) )\n\nwhere j is the index corresponding to the input and n is the number of inputs.\n\nSubsequent elements are recursively computed using the logistic mapping:\n\n      Wi+1j = textlogistic_parameter cdot Wij cdot (1 - Wij)\n\nThe resulting matrix has dimensions (factor * in_size) x in_size, where in_size corresponds to the number of columns provided in dims. If the provided number of rows does not match factor * in_size the number of rows is overridden.\n\nArguments\n\nrng: Random number generator. Default is Utils.default_rng() from WeightInitializers.\nT: Type of the elements in the reservoir matrix. Default is Float32.\ndims: Dimensions of the matrix. Should follow res_size x in_size.\n\nKeyword arguments\n\nfactor: The number of logistic map iterations (chain length) per input, determining the number of rows per input.\namplitude: Scaling parameter A for the sine-based initialization of the first element in each logistic chain. Default is 0.3.\nsine_divisor: Parameter B used to adjust the phase in the sine initialization. Default is 5.9.\nlogistic_parameter: The parameter r in the logistic recurrence that governs the chain dynamics. Default is 2.35.\nreturn_sparse: If true, returns the resulting matrix as a sparse matrix. Default is false.\n\nExamples\n\njulia> modified_lm(20, 10; factor=2)\n20×10 SparseArrays.SparseMatrixCSC{Float32, Int64} with 18 stored entries:\n⎡⢠⠀⠀⠀⠀⎤\n⎢⠀⢣⠀⠀⠀⎥\n⎢⠀⠀⢣⠀⠀⎥\n⎢⠀⠀⠀⢣⠀⎥\n⎣⠀⠀⠀⠀⢣⎦\n\njulia> modified_lm(12, 4; factor=3)\n12×4 SparseArrays.SparseMatrixCSC{Float32, Int64} with 9 stored entries:\n  ⋅    ⋅          ⋅          ⋅\n  ⋅    ⋅          ⋅          ⋅\n  ⋅    ⋅          ⋅          ⋅\n  ⋅   0.0133075   ⋅          ⋅\n  ⋅   0.0308564   ⋅          ⋅\n  ⋅   0.070275    ⋅          ⋅\n  ⋅    ⋅         0.0265887   ⋅\n  ⋅    ⋅         0.0608222   ⋅\n  ⋅    ⋅         0.134239    ⋅\n  ⋅    ⋅          ⋅         0.0398177\n  ⋅    ⋅          ⋅         0.0898457\n  ⋅    ⋅          ⋅         0.192168\n\n\n\n\n\n\n","category":"function"},{"location":"api/inits/modified_lm/#References","page":"modified_lm","title":"References","text":"","category":"section"},{"location":"api/inits/modified_lm/","page":"modified_lm","title":"modified_lm","text":"Viehweg, J.; Poll, C. and Mäder, P. (2025). Deterministic Reservoir Computing for Chaotic Time Series Prediction, arXiv preprint arXiv:2501.15615.\n\n\n\n","category":"page"},{"location":"api/utils/#Utilities","page":"Utilities","title":"Utilities","text":"","category":"section"},{"location":"api/utils/#ReservoirComputing.collectstates","page":"Utilities","title":"ReservoirComputing.collectstates","text":"collectstates(rc, data, ps, st)\n\nRun the sequence data once through the reservoir chain rc, advancing the model state over time, and collect feature vectors at every Collect layer. If more than one Collect is encountered in a step, their vectors are concatenated with vcat in order of appearance. If no Collect is seen in a step, the feature defaults to the final vector exiting the chain for that time step.\n\nnote: Note\nIf your LinearReadout layer was created with include_collect=true (default behaviour), a collection point is placed immediately before the readout, so the collected features are the inputs to the readout.\n\nArguments\n\nrc: A ReservoirChain (or compatible AbstractLuxLayer with .layers).\ndata: Input sequence of shape (in_dims, T), where columns are time steps.\nps, st: Current parameters and state for rc.\n\nReturns\n\nstates: Reservoir states, i.e. a feature matrix with one column per time step. The feature dimension n_features equals the vertical concatenation of all vectors captured at Collect layers in that step.\nst: Updated model states.\n\n\n\n\n\n","category":"function"},{"location":"api/inits/selfloop_cycle/#selfloop_cycle","page":"selfloop_cycle","title":"selfloop_cycle","text":"","category":"section"},{"location":"api/inits/selfloop_cycle/#ReservoirComputing.selfloop_cycle","page":"selfloop_cycle","title":"ReservoirComputing.selfloop_cycle","text":"selfloop_cycle([rng], [T], dims...;\n    cycle_weight=0.1, selfloop_weight=0.1,\n    return_sparse=false, kwargs...)\n\nCreates a simple cycle reservoir with the addition of self loops (Elsarraj et al., 2019).\n\nThis architecture is referred to as TP1 in the original paper.\n\nEquations\n\nW_ij =\nbegincases\n    ll  textif  i = j \n    r  textif  j = i - 1 text for  i = 2 dots N \n    r  textif  i = 1 j = N \n    0  textotherwise\nendcases\n\nArguments\n\nrng: Random number generator. Default is Utils.default_rng() from WeightInitializers.\nT: Type of the elements in the reservoir matrix. Default is Float32.\ndims: Dimensions of the reservoir matrix.\n\nKeyword arguments\n\ncycle_weight: Weight of the cycle connections in the reservoir matrix. This can be provided as a single value or an array. In case it is provided as an array please make sure that the length of the array matches the length of the cycle you want to populate. Default is 0.1.\nselfloop_weight: Weight of the self loops in the reservoir matrix. This can be provided as a single value or an array. In case it is provided as an array please make sure that the length of the array matches the length of the diagonal you want to populate. Default is 0.1.\nreturn_sparse: flag for returning a sparse matrix. Default is false.\nsampling_type: Sampling that decides the distribution of weight negative numbers. If set to :no_sample the sign is unchanged. If set to :bernoulli_sample! then each weight can be positive with a probability set by positive_prob. If set to :irrational_sample! the weight is negative if the decimal number of the irrational number chosen is odd. If set to :regular_sample!, each weight will be assigned a negative sign after the chosen strides. strides can be a single number or an array. Default is :no_sample.\npositive_prob: probability of the weight being positive when sampling_type is set to :bernoulli_sample!. Default is 0.5.\nirrational: Irrational number whose decimals decide the sign of weight. Default is pi.\nstart: Which place after the decimal point the counting starts for the irrational sign counting. Default is 1.\nstrides: number of strides for assigning negative value to a weight. It can be an integer or an array. Default is 2.\n\nExamples\n\njulia> reservoir_matrix = selfloop_cycle(5, 5)\n5×5 Matrix{Float32}:\n 0.1  0.0  0.0  0.0  0.1\n 0.1  0.1  0.0  0.0  0.0\n 0.0  0.1  0.1  0.0  0.0\n 0.0  0.0  0.1  0.1  0.0\n 0.0  0.0  0.0  0.1  0.1\n\njulia> reservoir_matrix = selfloop_cycle(5, 5; weight=0.2, selfloop_weight=0.5)\n5×5 Matrix{Float32}:\n 0.5  0.0  0.0  0.0  0.2\n 0.2  0.5  0.0  0.0  0.0\n 0.0  0.2  0.5  0.0  0.0\n 0.0  0.0  0.2  0.5  0.0\n 0.0  0.0  0.0  0.2  0.5\n\n\n\n\n\n","category":"function"},{"location":"api/inits/selfloop_cycle/#References","page":"selfloop_cycle","title":"References","text":"","category":"section"},{"location":"api/inits/selfloop_cycle/","page":"selfloop_cycle","title":"selfloop_cycle","text":"Elsarraj, D.; Qisi, M. A.; Rodan, A.; Obeid, N.; Sharieh, A. and Faris, H. (2019). Demystifying echo state network with deterministic simple topologies. International Journal of Computational Science and Engineering 19, 407–417.\n\n\n\n","category":"page"},{"location":"tutorials/hybrid/#Hybrid-Echo-State-Networks","page":"Hybrid Echo State Networks","title":"Hybrid Echo State Networks","text":"","category":"section"},{"location":"api/inits/informed_init/#informed_init","page":"informed_init","title":"informed_init","text":"","category":"section"},{"location":"api/inits/informed_init/#ReservoirComputing.informed_init","page":"informed_init","title":"ReservoirComputing.informed_init","text":"informed_init([rng], [T], dims...;\n    scaling=0.1, model_in_size, gamma=0.5)\n\nCreate an input layer for informed echo state networks (Pathak et al., 2018).\n\nArguments\n\nrng: Random number generator. Default is Utils.default_rng() from WeightInitializers.\nT: Type of the elements in the reservoir matrix. Default is Float32.\ndims: Dimensions of the matrix. Should follow res_size x in_size.\n\nKeyword arguments\n\nscaling: The scaling factor for the input matrix. Default is 0.1.\nmodel_in_size: The size of the input model.\ngamma: The gamma value. Default is 0.5.\n\nExamples\n\n\n\n\n\n","category":"function"},{"location":"api/inits/informed_init/#References","page":"informed_init","title":"References","text":"","category":"section"},{"location":"api/inits/informed_init/","page":"informed_init","title":"informed_init","text":"Pathak, J.; Wikner, A.; Fussell, R.; Chandra, S.; Hunt, B. R.; Girvan, M. and Ott, E. (2018). Hybrid forecasting of chaotic processes: Using machine learning in conjunction with a knowledge-based model. Chaos: An Interdisciplinary Journal of Nonlinear Science 28.\n\n\n\n","category":"page"},{"location":"api/inits/self_loop!/#self_loop!","page":"self_loop!","title":"self_loop!","text":"","category":"section"},{"location":"api/inits/self_loop!/#ReservoirComputing.self_loop!","page":"self_loop!","title":"ReservoirComputing.self_loop!","text":"self_loop!([rng], reservoir_matrix, weight, jump_size;\n    sampling_type=:no_sample, irrational=pi, start=1,\n    positive_prob=0.5)\n\nAdds jumps to a given reservoir_matrix with chosen weight and determined jump_size. weight can be either a number or an array.\n\nArguments\n\nrng: Random number generator. Default is Utils.default_rng() from WeightInitializers.\nreservoir_matrix: matrix to be changed.\nweight: weight to add as a self loop. Can be either a single number or an array.\n\nKeyword arguments\n\nsampling_type: Sampling that decides the distribution of weight negative numbers. If set to :no_sample the sign is unchanged. If set to :bernoulli_sample! then each weight can be positive with a probability set by positive_prob. If set to :irrational_sample! the weight is negative if the decimal number of the irrational number chosen is odd. If set to :regular_sample!, each weight will be assigned a negative sign after the chosen strides. strides can be a single number or an array. Default is :no_sample.\npositive_prob: probability of the weight being positive when sampling_type is set to :bernoulli_sample!. Default is 0.5.\nirrational: Irrational number whose decimals decide the sign of weight. Default is pi.\nstart: Which place after the decimal point the counting starts for the irrational sign counting. Default is 1.\nstrides: number of strides for assigning negative value to a weight. It can be an integer or an array. Default is 2.\n\nExamples\n\njulia> matrix = zeros(Float32, 5, 5)\n5×5 Matrix{Float32}:\n 0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n\njulia> self_loop!(matrix, 1.0)\n5×5 Matrix{Float32}:\n  1.0  0.0   0.0   0.0   0.0\n  0.0  1.0   0.0   0.0   0.0\n  0.0  0.0   1.0   0.0   0.0\n  0.0  0.0   0.0   1.0   0.0\n  0.0  0.0   0.0   0.0   1.0\n\n\n\n\n\n","category":"function"},{"location":"api/inits/add_jumps!/#add_jumps!","page":"add_jumps!","title":"add_jumps!","text":"","category":"section"},{"location":"api/inits/add_jumps!/#ReservoirComputing.add_jumps!","page":"add_jumps!","title":"ReservoirComputing.add_jumps!","text":"add_jumps!([rng], reservoir_matrix, weight, jump_size;\n    sampling_type=:no_sample, irrational=pi, start=1,\n    positive_prob=0.5)\n\nAdds jumps to a given reservoir_matrix with chosen weight and determined jump_size. weight can be either a number or an array.\n\nArguments\n\nrng: Random number generator. Default is Utils.default_rng() from WeightInitializers.\nreservoir_matrix: matrix to be changed.\nweight: weight to add as a simple cycle. Can be either a single number or an array.\njump_size: size of the jump's distance.\n\nKeyword arguments\n\nsampling_type: Sampling that decides the distribution of weight negative numbers. If set to :no_sample the sign is unchanged. If set to :bernoulli_sample! then each weight can be positive with a probability set by positive_prob. If set to :irrational_sample! the weight is negative if the decimal number of the irrational number chosen is odd. If set to :regular_sample!, each weight will be assigned a negative sign after the chosen strides. strides can be a single number or an array. Default is :no_sample.\npositive_prob: probability of the weight being positive when sampling_type is set to :bernoulli_sample!. Default is 0.5.\nirrational: Irrational number whose decimals decide the sign of weight. Default is pi.\nstart: Which place after the decimal point the counting starts for the irrational sign counting. Default is 1.\nstrides: number of strides for assigning negative value to a weight. It can be an integer or an array. Default is 2.\n\nExamples\n\njulia> matrix = zeros(Float32, 5, 5)\n5×5 Matrix{Float32}:\n 0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n 0.0  0.0  0.0  0.0  0.0\n\njulia> add_jumps!(matrix, 1.0)\n5×5 Matrix{Float32}:\n  0.0  0.0   1.0   0.0   0.0\n  0.0  0.0   0.0   0.0   0.0\n  1.0  0.0   0.0   0.0   0.0\n  0.0  0.0   0.0   0.0   1.0\n  0.0  0.0   1.0   0.0   0.0\n\n\n\n\n\n","category":"function"},{"location":"api/inits/logistic_mapping/#logistic_mapping","page":"logistic_mapping","title":"logistic_mapping","text":"","category":"section"},{"location":"api/inits/logistic_mapping/#ReservoirComputing.logistic_mapping","page":"logistic_mapping","title":"ReservoirComputing.logistic_mapping","text":"logistic_mapping([rng], [T], dims...;\n    amplitude=0.3, sine_divisor=5.9, logistic_parameter=3.7,\n    return_sparse=false)\n\nGenerate an input weight matrix using a logistic mapping (Wang et al., 2022) The first row is initialized using a sine function:\n\n    W1 j = textamplitude cdot sin(j cdot pi \n        (textsine_divisor cdot in_size))\n\nfor each input index j, with in_size being the number of columns provided in dims. Subsequent rows are generated recursively using the logistic map recurrence:\n\n    Wi+1 j = textlogistic_parameter cdot W(i j) cdot (1 - Wi j)\n\nArguments\n\nrng: Random number generator. Default is Utils.default_rng() from WeightInitializers.\nT: Type of the elements in the reservoir matrix. Default is Float32.\ndims: Dimensions of the matrix. Should follow res_size x in_size.\n\nKeyword arguments\n\namplitude: Scaling parameter used in the sine initialization of the first row. Default is 0.3.\nsine_divisor: Parameter used to adjust the phase in the sine initialization. Default is 5.9.\nlogistic_parameter: The parameter in the logistic mapping recurrence that governs the dynamics. Default is 3.7.\nreturn_sparse: If true, returns the resulting matrix as a sparse matrix. Default is false.\n\nExamples\n\njulia> logistic_mapping(8, 3)\n8×3 Matrix{Float32}:\n 0.0529682  0.104272  0.1523\n 0.185602   0.345578  0.477687\n 0.559268   0.836769  0.923158\n 0.912003   0.50537   0.262468\n 0.296938   0.924893  0.716241\n 0.772434   0.257023  0.751987\n 0.650385   0.70656   0.69006\n 0.841322   0.767132  0.791346\n\n\n\n\n\n\n","category":"function"},{"location":"api/inits/logistic_mapping/#References","page":"logistic_mapping","title":"References","text":"","category":"section"},{"location":"api/inits/logistic_mapping/","page":"logistic_mapping","title":"logistic_mapping","text":"Wang, H.; Liu, Y.; Lu, P.; Luo, Y.; Wang, D. and Xu, X. (2022). Echo state network with logistic mapping and bias dropout for time series prediction. Neurocomputing 489, 196–210.\n\n\n\n","category":"page"},{"location":"api/states/#States-Modifications","page":"States","title":"States Modifications","text":"","category":"section"},{"location":"api/states/#ReservoirComputing.Pad","page":"States","title":"ReservoirComputing.Pad","text":"Pad(padding)\n\nPadding layer that adds padding (either a number or an array) at the end of a state.\n\nArguments\n\npadding: value to append. Default is 1.0.\n\n\n\n\n\n","category":"type"},{"location":"api/states/#ReservoirComputing.Extend","page":"States","title":"ReservoirComputing.Extend","text":"Pad(padding)\n\nWrapper layer that concatenates the reservoir state at that point with the input that it receives.\n\nArguments\n\nop: wrapped layer\n\nExamples\n\nesn = ReservoirChain(\n    Extend(\n        StatefulLayer(\n        ESNCell(\n        3 => 300; init_reservoir = rand_sparse(; radius = 1.2, sparsity = 6 / 300))\n    )\n    ),\n    NLAT2(),\n    LinearReadout(300 + 3 => 3)\n)\n\nIn this esample the input to Extend is the initial value fed to ReservoirChain. After Extend, the value in the chain will be the state returned by the StatefulLayer, vcated with the input.\n\n\n\n\n\n","category":"type"},{"location":"api/states/#ReservoirComputing.NLAT1","page":"States","title":"ReservoirComputing.NLAT1","text":"NLAT1(x)\n\nNLAT1 implements the T₁ transformation algorithm introduced in (Chattopadhyay et al., 2020) and (Pathak et al., 2017). The T₁ algorithm squares elements of the input array, targeting every second row.\n\ntilder_ij =\nbegincases\n    r_ij times r_ij  textif  j text is odd \n    r_ij  textif  j text is even\nendcases\n\nExample\n\njulia> x_old = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n10-element Vector{Int64}:\n 0\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n\njulia> n_new = NLAT1(x_old)\n10-element Vector{Int64}:\n  0\n  1\n  4\n  3\n 16\n  5\n 36\n  7\n 64\n  9\n\njulia> mat_old = [1  2  3;\n                   4  5  6;\n                   7  8  9;\n                  10 11 12;\n                  13 14 15;\n                  16 17 18;\n                  19 20 21]\n7×3 Matrix{Int64}:\n  1   2   3\n  4   5   6\n  7   8   9\n 10  11  12\n 13  14  15\n 16  17  18\n 19  20  21\n\njulia> mat_new = NLAT1(mat_old)\n7×3 Matrix{Int64}:\n   1    4    9\n   4    5    6\n  49   64   81\n  10   11   12\n 169  196  225\n  16   17   18\n 361  400  441\n\n\n\n\n\n\n","category":"function"},{"location":"api/states/#ReservoirComputing.NLAT2","page":"States","title":"ReservoirComputing.NLAT2","text":"NLAT2()\n\nNLAT2 implements the T₂ transformation algorithm as defined in (Chattopadhyay et al., 2020). This transformation algorithm modifies the reservoir states by multiplying each odd-indexed row (starting from the second row) with the product of its two preceding rows.\n\ntilder_ij =\nbegincases\n    r_ij-1 times r_ij-2  textif  j  1 text is odd \n    r_ij  textif  j text is 1 or even\nendcases\n\nExample\n\njulia> x_old = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n10-element Vector{Int64}:\n 0\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n\njulia> n_new = NLAT2(x_old)\n10-element Vector{Int64}:\n  0\n  1\n  0\n  3\n  6\n  5\n 20\n  7\n 42\n  9\n\njulia> mat_old = [1  2  3;\n                   4  5  6;\n                   7  8  9;\n                  10 11 12;\n                  13 14 15;\n                  16 17 18;\n                  19 20 21]\n7×3 Matrix{Int64}:\n  1   2   3\n  4   5   6\n  7   8   9\n 10  11  12\n 13  14  15\n 16  17  18\n 19  20  21\n\njulia> mat_new = NLAT2(mat_old)\n7×3 Matrix{Int64}:\n  1   2    3\n  4   5    6\n  4  10   18\n 10  11   12\n 70  88  108\n 16  17   18\n 19  20   21\n\n\n\n\n\n\n","category":"function"},{"location":"api/states/#ReservoirComputing.NLAT3","page":"States","title":"ReservoirComputing.NLAT3","text":"NLAT3()\n\nImplements the T₃ transformation algorithm as detailed in (Chattopadhyay et al., 2020). This algorithm modifies the reservoir's states by multiplying each odd-indexed row (beginning from the second row) with the product of the immediately preceding and the immediately following rows.\n\ntilder_ij =\nbegincases\nr_ij-1 times r_ij+1  textif  j  1 text is odd \nr_ij  textif  j = 1 text or even\nendcases\n\nExample\n\njulia> x_old = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n10-element Vector{Int64}:\n 0\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n\njulia> n_new = NLAT3(x_old)\n10-element Vector{Int64}:\n  0\n  1\n  3\n  3\n 15\n  5\n 35\n  7\n 63\n  9\n\njulia> mat_old = [1  2  3;\n                   4  5  6;\n                   7  8  9;\n                  10 11 12;\n                  13 14 15;\n                  16 17 18;\n                  19 20 21]\n7×3 Matrix{Int64}:\n  1   2   3\n  4   5   6\n  7   8   9\n 10  11  12\n 13  14  15\n 16  17  18\n 19  20  21\n\njulia> mat_new = NLAT3(mat_old)\n7×3 Matrix{Int64}:\n   1    2    3\n   4    5    6\n  40   55   72\n  10   11   12\n 160  187  216\n  16   17   18\n  19   20   21\n\n\n\n\n\n\n","category":"function"},{"location":"api/states/#ReservoirComputing.PartialSquare","page":"States","title":"ReservoirComputing.PartialSquare","text":"PartialSquare(eta)\n\nImplement a partial squaring of the states as described in (Barbosa et al., 2021).\n\nEquations\n\n    beginequation\n    g(r_i) =\n    begincases\n        r_i^2  textif  i leq eta_r N \n        r_i  textif  i  eta_r N\n    endcases\n    endequation\n\nExamples\n\njulia> ps = PartialSquare(0.6)\nPartialSquare(0.6)\n\n\njulia> x_old = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n10-element Vector{Int64}:\n 0\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n\njulia> x_new = ps(x_old)\n10-element Vector{Int64}:\n  0\n  1\n  4\n  9\n 16\n 25\n  6\n  7\n  8\n  9\n\n\n\n\n\n","category":"type"},{"location":"api/states/#ReservoirComputing.ExtendedSquare","page":"States","title":"ReservoirComputing.ExtendedSquare","text":"ExtendedSquare()\n\nExtension of the Lu initialization proposed in (Herteux and Räth, 2020). The state vector is extended with the squared elements of the initial state\n\nEquations\n\nbeginequation\n    vecx = x_1 x_2 dots x_N x_1^2 x_2^2 dots x_N^2\nendequation\n\nExamples\n\njulia> x_old = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n9-element Vector{Int64}:\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n\njulia> es = ExtendedSquare()\nExtendedSquare()\n\njulia> x_new = es(x_old)\n18-element Vector{Int64}:\n  1\n  2\n  3\n  4\n  5\n  6\n  7\n  8\n  9\n  1\n  4\n  9\n 16\n 25\n 36\n 49\n 64\n 81\n\n\n\n\n\n\n","category":"function"},{"location":"api/states/#References","page":"States","title":"References","text":"","category":"section"},{"location":"api/states/","page":"States","title":"States","text":"Barbosa, W. A.; Griffith, A.; Rowlands, G. E.; Govia, L. C.; Ribeill, G. J.; Nguyen, M.-H.; Ohki, T. A. and Gauthier, D. J. (2021). Symmetry-aware reservoir computing. Physical Review E 104.\n\n\n\nChattopadhyay, A.; Hassanzadeh, P. and Subramanian, D. (2020). Data-driven predictions of a multiscale Lorenz 96 chaotic system using machine-learning methods: reservoir computing,  artificial neural network,  and long short-term memory network. Nonlinear Processes in Geophysics 27, 373–389.\n\n\n\nHerteux, J. and Räth, C. (2020). Breaking symmetries of the reservoir equations in echo state networks. Chaos: An Interdisciplinary Journal of Nonlinear Science 30.\n\n\n\nPathak, J.; Lu, Z.; Hunt, B. R.; Girvan, M. and Ott, E. (2017). Using machine learning to replicate chaotic attractors and calculate Lyapunov exponents from data. Chaos: An Interdisciplinary Journal of Nonlinear Science 27.\n\n\n\n","category":"page"},{"location":"api/inits/delay_line/#delay_line","page":"delay_line","title":"delay_line","text":"","category":"section"},{"location":"api/inits/delay_line/#ReservoirComputing.delay_line","page":"delay_line","title":"ReservoirComputing.delay_line","text":"delay_line([rng], [T], dims...;\n    weight=0.1, return_sparse=false,\n    kwargs...)\n\nCreate and return a delay line reservoir matrix (Rodan and Tino, 2011).\n\nArguments\n\nrng: Random number generator. Default is Utils.default_rng() from WeightInitializers.\nT: Type of the elements in the reservoir matrix. Default is Float32.\ndims: Dimensions of the reservoir matrix.\n\nKeyword arguments\n\nweight: Determines the value of all connections in the reservoir. This can be provided as a single value or an array. In case it is provided as an array please make sure that the length of the array matches the length of the sub-diagonal you want to populate. Default is 0.1.\nshift: delay line shift. Default is 1.\nreturn_sparse: flag for returning a sparse matrix. Default is false.\nsampling_type: Sampling that decides the distribution of weight negative numbers. If set to :no_sample the sign is unchanged. If set to :bernoulli_sample! then each weight can be positive with a probability set by positive_prob. If set to :irrational_sample! the weight is negative if the decimal number of the irrational number chosen is odd. If set to :regular_sample!, each weight will be assigned a negative sign after the chosen strides. strides can be a single number or an array. Default is :no_sample.\npositive_prob: probability of the weight being positive when sampling_type is set to :bernoulli_sample!. Default is 0.5.\nirrational: Irrational number whose decimals decide the sign of weight. Default is pi.\nstart: Which place after the decimal point the counting starts for the irrational sign counting. Default is 1.\nstrides: number of strides for assigning negative value to a weight. It can be an integer or an array. Default is 2.\n\nExamples\n\njulia> res_matrix = delay_line(5, 5)\n5×5 Matrix{Float32}:\n 0.0  0.0  0.0  0.0  0.0\n 0.1  0.0  0.0  0.0  0.0\n 0.0  0.1  0.0  0.0  0.0\n 0.0  0.0  0.1  0.0  0.0\n 0.0  0.0  0.0  0.1  0.0\n\njulia> res_matrix = delay_line(5, 5; weight = 1)\n5×5 Matrix{Float32}:\n 0.0  0.0  0.0  0.0  0.0\n 1.0  0.0  0.0  0.0  0.0\n 0.0  1.0  0.0  0.0  0.0\n 0.0  0.0  1.0  0.0  0.0\n 0.0  0.0  0.0  1.0  0.0\n\n\n\n\n\n","category":"function"},{"location":"api/inits/delay_line/#References","page":"delay_line","title":"References","text":"","category":"section"},{"location":"api/inits/delay_line/","page":"delay_line","title":"delay_line","text":"Rodan, A. and Tino, P. (2011). Minimum Complexity Echo State Network. IEEE Transactions on Neural Networks 22, 131–144.\n\n\n\n","category":"page"},{"location":"api/inits/true_double_cycle/#true*double*cycle","page":"truedoublecycle","title":"truedoublecycle","text":"","category":"section"},{"location":"api/inits/true_double_cycle/#ReservoirComputing.true_double_cycle","page":"truedoublecycle","title":"ReservoirComputing.true_double_cycle","text":"true_double_cycle([rng], [T], dims...;\n    cycle_weight=0.1, second_cycle_weight=0.1,\n    return_sparse=false)\n\nCreates a true double cycle reservoir, ispired by (Fu et al., 2023), with cycles built on the definition by (Rodan and Tino, 2011).\n\nArguments\n\nrng: Random number generator. Default is Utils.default_rng() from WeightInitializers.\nT: Type of the elements in the reservoir matrix. Default is Float32.\ndims: Dimensions of the reservoir matrix.\n\nKeyword arguments\n\ncycle_weight: Weight of the upper cycle connections in the reservoir matrix. Default is 0.1.\nsecond_cycle_weight: Weight of the lower cycle connections in the reservoir matrix. Default is 0.1.\nreturn_sparse: flag for returning a sparse matrix. Default is false.\ncycle_kwargs, and second_cycle_kwargs: named tuples that control the kwargs for the weights generation. The kwargs are as follows:\nsampling_type: Sampling that decides the distribution of weight negative numbers. If set to :no_sample the sign is unchanged. If set to :bernoulli_sample! then each weight can be positive with a probability set by positive_prob. If set to :irrational_sample! the weight is negative if the decimal number of the irrational number chosen is odd. If set to :regular_sample!, each weight will be assigned a negative sign after the chosen strides. strides can be a single number or an array. Default is :no_sample.\npositive_prob: probability of the weight being positive when sampling_type is set to :bernoulli_sample!. Default is 0.5.\nirrational: Irrational number whose decimals decide the sign of weight. Default is pi.\nstart: Which place after the decimal point the counting starts for the irrational sign counting. Default is 1.\nstrides: number of strides for assigning negative value to a weight. It can be an integer or an array. Default is 2.\n\nExamples\n\njulia> true_double_cycle(5, 5; cycle_weight = 0.1, second_cycle_weight = 0.3)\n5×5 Matrix{Float32}:\n 0.0  0.3  0.0  0.0  0.1\n 0.1  0.0  0.3  0.0  0.0\n 0.0  0.1  0.0  0.3  0.0\n 0.0  0.0  0.1  0.0  0.3\n 0.3  0.0  0.0  0.1  0.0\n\n\n\n\n\n","category":"function"},{"location":"api/inits/true_double_cycle/#References","page":"truedoublecycle","title":"References","text":"","category":"section"},{"location":"api/inits/true_double_cycle/","page":"truedoublecycle","title":"truedoublecycle","text":"Fu, J.; Li, G.; Tang, J.; Xia, L.; Wang, L. and Duan, S. (2023). A double-cycle echo state network topology for time series prediction. Chaos: An Interdisciplinary Journal of Nonlinear Science 33.\n\n\n\nRodan, A. and Tino, P. (2011). Minimum Complexity Echo State Network. IEEE Transactions on Neural Networks 22, 131–144.\n\n\n\n","category":"page"},{"location":"api/inits/scale_radius!/#scale_radius!","page":"scale_radius!","title":"scale_radius!","text":"","category":"section"},{"location":"api/inits/scale_radius!/#ReservoirComputing.scale_radius!","page":"scale_radius!","title":"ReservoirComputing.scale_radius!","text":"scale_radius!(matrix, radius)\n\nScale the spectral radius of the given matrix to be equal to the given radius\n\nArguments\n\nmatrix: Matrix to be scaled.\nradius: desidered radius to scale the given matrix to\n\n\n\n\n\n","category":"function"},{"location":"api/inits/selfloop_feedback_cycle/#selfloop*feedback*cycle","page":"selfloopfeedbackcycle","title":"selfloopfeedbackcycle","text":"","category":"section"},{"location":"api/inits/selfloop_feedback_cycle/#ReservoirComputing.selfloop_feedback_cycle","page":"selfloopfeedbackcycle","title":"ReservoirComputing.selfloop_feedback_cycle","text":"selfloop_feedback_cycle([rng], [T], dims...;\n    cycle_weight=0.1, selfloop_weight=0.1,\n    return_sparse=false)\n\nCreates a cycle reservoir with feedback connections on even neurons and self loops on odd neurons (Elsarraj et al., 2019).\n\nThis architecture is referred to as TP2 in the original paper.\n\nEquations\n\nW_ij =\nbegincases\n    r  textif  j = i - 1 text for  i = 2 dots N \n    r  textif  i = 1 j = N \n    ll  textif  i = j text and  i text is odd \n    r  textif  j = i + 1 text and  i text is even i neq N \n    0  textotherwise\nendcases\n\nArguments\n\nrng: Random number generator. Default is Utils.default_rng() from WeightInitializers.\nT: Type of the elements in the reservoir matrix. Default is Float32.\ndims: Dimensions of the reservoir matrix.\n\nKeyword arguments\n\ncycle_weight: Weight of the cycle connections in the reservoir matrix. This can be provided as a single value or an array. In case it is provided as an array please make sure that the length of the array matches the length of the cycle you want to populate. Default is 0.1.\nselfloop_weight: Weight of the self loops in the reservoir matrix. Default is 0.1.\nreturn_sparse: flag for returning a sparse matrix. Default is false.\n\nExamples\n\njulia> reservoir_matrix = selfloop_feedback_cycle(5, 5)\n5×5 Matrix{Float32}:\n 0.1  0.1  0.0  0.0  0.1\n 0.1  0.0  0.0  0.0  0.0\n 0.0  0.1  0.1  0.1  0.0\n 0.0  0.0  0.1  0.0  0.0\n 0.0  0.0  0.0  0.1  0.1\n\njulia> reservoir_matrix = selfloop_feedback_cycle(5, 5; self_loop_weight=0.5)\n5×5 Matrix{Float32}:\n 0.5  0.1  0.0  0.0  0.1\n 0.1  0.0  0.0  0.0  0.0\n 0.0  0.1  0.5  0.1  0.0\n 0.0  0.0  0.1  0.0  0.0\n 0.0  0.0  0.0  0.1  0.5\n\n\n\n\n\n","category":"function"},{"location":"api/inits/selfloop_feedback_cycle/#References","page":"selfloopfeedbackcycle","title":"References","text":"","category":"section"},{"location":"api/inits/selfloop_feedback_cycle/","page":"selfloopfeedbackcycle","title":"selfloopfeedbackcycle","text":"Elsarraj, D.; Qisi, M. A.; Rodan, A.; Obeid, N.; Sharieh, A. and Faris, H. (2019). Demystifying echo state network with deterministic simple topologies. International Journal of Computational Science and Engineering 19, 407–417.\n\n\n\n","category":"page"}]
}
